{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a71ccf5a-bd72-4989-9050-481bf5e2014f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Test for random sequence of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0704217d-238c-45e0-a529-8e848078abb4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected weight:37\n",
      "train data size:20000\n",
      "test data size:1000\n",
      "Time:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce3fac6865b4ac7a45819666b98493a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\taccuracy:4.9\n",
      "\tloss: 2.361315\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 364>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    360\u001b[0m         grad_list\u001b[38;5;241m.\u001b[39mappend(train_model\u001b[38;5;241m.\u001b[39mgrads_norms)\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grad_list, hess_norm_list\n\u001b[0;32m--> 364\u001b[0m grad_list, hess_norm_list \u001b[38;5;241m=\u001b[39m \u001b[43mexp_get_lp_sm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m              \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mg_weight\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mg_times\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m              \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mg_epochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresult_root_dir\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m              \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresult_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq_reduce_by\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfreq_reduce_by\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m              \u001b[49m\u001b[43mfreq_reduce_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfreq_reduce_after\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36mexp_get_lp_sm\u001b[0;34m(train_data_all, test_data_all, op_features, weight, times, epochs, root_dir, path, clear_file, freq_reduce_by, freq_reduce_after)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    351\u001b[0m train_model \u001b[38;5;241m=\u001b[39m Train_nn(\u001b[38;5;241m784\u001b[39m, [weight], op_features, lr\u001b[38;5;241m=\u001b[39m details[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha_0\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 352\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_grads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_hessian\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbook_keep_freq\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mstore_gen_err\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_pt_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq_reduce_by\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfreq_reduce_by\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq_reduce_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreq_reduce_after\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(grad_file_path,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    355\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(grad) \u001b[38;5;28;01mfor\u001b[39;00m grad \u001b[38;5;129;01min\u001b[39;00m train_model\u001b[38;5;241m.\u001b[39mgrads_norms]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36mTrain_nn.fit\u001b[0;34m(self, train_loader, test_loader, epochs, store_grads, store_hessian, store_gen_err, store_weights, store_pt_loss, store_freq, freq_reduce_by, freq_reduce_after)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m store_hessian \u001b[38;5;129;01mand\u001b[39;00m (batch\u001b[38;5;241m%\u001b[39m store_freq\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;66;03m#assert False, \"Not implemented\"\u001b[39;00m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 290\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhess_norms\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_hessianv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    291\u001b[0m     store_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m store_count\u001b[38;5;241m%\u001b[39mfreq_reduce_after\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36mTrain_nn.get_hessianv2\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    221\u001b[0m     local_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(out, y)\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m local_loss\n\u001b[0;32m--> 223\u001b[0m hess_mat \u001b[38;5;241m=\u001b[39m \u001b[43mhessian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_fun_hess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;66;03m# print(f'len of hess mat:{len(hess_mat)}')\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# print(f'hess_mat[0] shape:{len(hess_mat[0])}')\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# print(f'hess_mat[0][0] shape:{hess_mat[0][0].shape}')\u001b[39;00m\n\u001b[1;32m    227\u001b[0m hess_norm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/functional.py:808\u001b[0m, in \u001b[0;36mhessian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, outer_jacobian_strategy)\u001b[0m\n\u001b[1;32m    805\u001b[0m     _check_requires_grad(jac, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacobian\u001b[39m\u001b[38;5;124m\"\u001b[39m, strict\u001b[38;5;241m=\u001b[39mstrict)\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jac\n\u001b[0;32m--> 808\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mjacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjac_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvectorize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m               \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mouter_jacobian_strategy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _tuple_postprocess(res, (is_inputs_tuple, is_inputs_tuple))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/functional.py:670\u001b[0m, in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[1;32m    668\u001b[0m jac_i: Tuple[List[torch\u001b[38;5;241m.\u001b[39mTensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m([] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs)))  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(out\u001b[38;5;241m.\u001b[39mnelement()):\n\u001b[0;32m--> 670\u001b[0m     vj \u001b[38;5;241m=\u001b[39m \u001b[43m_autograd_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m el_idx, (jac_i_el, vj_el, inp_el) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(jac_i, vj, inputs)):\n\u001b[1;32m    674\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m vj_el \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/functional.py:159\u001b[0m, in \u001b[0;36m_autograd_grad\u001b[0;34m(outputs, inputs, grad_outputs, create_graph, retain_graph, is_grads_batched)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m,) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_grad_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_grads_batched\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:276\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd.functional import hessian\n",
    "import torch.utils.data as data_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from tqdm.notebook import tqdm \n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import seaborn as sns\n",
    "import os\n",
    "import copy\n",
    "from torch.nn.utils import _stateless\n",
    "\n",
    "batch_size = 1\n",
    "num_workers = 1\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "# device\n",
    "\n",
    "details = {}\n",
    "details['use_db'] = 'mnist'\n",
    "details['result_root_dir']='results/t0/'\n",
    "details['result_path']='try1_t6_r1.5'\n",
    "details['ratio'] = 1.5\n",
    "details['book_keep_freq'] = 20\n",
    "details['g_times'] = 10\n",
    "details['g_epochs'] = 1\n",
    "details['alpha_0']= 0.001\n",
    "details['freq_reduce_by'] = 20\n",
    "details['freq_reduce_after'] = 100\n",
    "\n",
    "details['training_step_limit'] = 100000 ## this is to train for max updates per epochs\n",
    "details['stop_hess_computation'] = 20000 ## Stop computing hessian after calculated these many times\n",
    "\n",
    "details['g_weight'] = int(details['ratio']*20000/(784+10))\n",
    "print(f'selected weight:{details[\"g_weight\"]}')\n",
    "\n",
    "with open(details['result_root_dir']+'details_'+details['result_path']+'.txt', 'w+') as f:\n",
    "    for key, val in details.items():\n",
    "        content = key + ' : '+str(val) + '\\n'\n",
    "        f.write(content)\n",
    "        \n",
    "torch.manual_seed(3407)\n",
    "np.random.seed(3407)\n",
    "torch.cuda.manual_seed_all(3407)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "# os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":4096:2\"\n",
    "\n",
    "train_data_all = torchvision.datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "test_data_all = torchvision.datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "# print(f'train data:{train_data}')\n",
    "# print(f'test data:{test_data}')\n",
    "\n",
    "def get_random_subset(train_data_all, test_data_all):    \n",
    "    # train_indices = torch.arange(20000)\n",
    "    test_indices = torch.arange(1000)\n",
    "    train_indices = torch.randint(60000-1, (20000,))\n",
    "    # print(f'train indices:{train_indices[:10]}')\n",
    "    train_data = data_utils.Subset(train_data_all, train_indices)\n",
    "    test_data = data_utils.Subset(test_data_all, test_indices)\n",
    "    # print(f'train data:{train_data}')\n",
    "    # print(f'test data:{test_data}')\n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(0)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g,\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_data,\n",
    "        batch_size=256,\n",
    "        num_workers=num_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g,\n",
    "    )\n",
    "    print(f'train data size:{len(train_loader.dataset)}')\n",
    "    print(f'test data size:{len(test_loader.dataset)}')\n",
    "    # X_mat, y_mat = torch.Tensor(len(train_loader.dataset),784), torch.Tensor(len(train_loader.dataset)).long()\n",
    "    # for i, (data, label) in enumerate(train_loader):\n",
    "    #     X_mat[i] = data.flatten()\n",
    "    #     y_mat[i] = label.flatten()\n",
    "    # print(f'X_mat shape:{X_mat.shape}, y_mat shape:{y_mat.shape}')\n",
    "    return train_loader, test_loader #, X_mat, y_mat\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_features, hidden_layers, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = len(hidden_layers) + 1\n",
    "        self.total_params_len = 0\n",
    "        self.fc1 = nn.Linear(input_features, hidden_layers[0])\n",
    "        self.total_params_len += input_features*hidden_layers[0] + hidden_layers[0]\n",
    "        self.fc2 = nn.Linear(hidden_layers[0], output_size)\n",
    "        self.total_params_len += hidden_layers[0]*output_size + output_size\n",
    "        \n",
    "        ### Others required params\n",
    "        self.param_list = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        # print('x shape in forward',x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def fit(self, X, Y, X_val, Y_val, epochs, batch_size=1, **kwargs):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.SGD(self.parameters())\n",
    "        \n",
    "\n",
    "class Train_nn:\n",
    "    \n",
    "    def __init__(self, input_features, hidden_layers, output_size, lr):\n",
    "        self.model = Net(input_features, hidden_layers=hidden_layers, output_size=output_size)\n",
    "        self.model.to(device)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda= lambda it: 1/(it+1))\n",
    "        \n",
    "    def get_loss(self, X, y, params=None):\n",
    "        # if params is not None:\n",
    "        assert False, \"Model not initialized with given params\"\n",
    "        op = self.model(X)\n",
    "        loss = self.loss_fn(pred, y)\n",
    "        return loss\n",
    "    \n",
    "    def get_gradient(self):\n",
    "        params = (self.model.parameters())\n",
    "        grad_norm_sq = torch.tensor(0, dtype=float).to(device)\n",
    "        # print('grad Norm init:', grad_norm_sq)\n",
    "        for param in self.model.parameters():\n",
    "            temp = param.grad.data.pow(2).sum()\n",
    "            # print(f'param grad norm \\n\\tsum:{temp.data}')#,\\n\\tshape:{param.shape}')\n",
    "            grad_norm_sq += temp\n",
    "            \n",
    "        return grad_norm_sq.sqrt().cpu()\n",
    "    \n",
    "    def try_operator_norm(self, hess_mat):\n",
    "        for i in len(hess_mat):\n",
    "            for j in len(hess_mat[0]):\n",
    "                torch.unsqueeze(hess_mat[i][i],0)\n",
    "        hess_tensor_dim = list(hess_mat[0][0].shape)\n",
    "        hess_tensor_dim += [n*2,n*2]\n",
    "        hess_mat_np = np.zeros(shape=hess_tensor_dim)\n",
    "        hess_tensor = torch.tensor(hess_mat_np)\n",
    "        torch.cat(hess_mat, out=hess_tensor)\n",
    "        \n",
    "        hess_mat.reshpe(n*2,n*2)\n",
    "        hess_norm = torch.linalg.norm(hess_mat, 2)\n",
    "        assert False, \"Not working\"\n",
    "    \n",
    "    def get_hessian(self, X, y):\n",
    "        prev_params = copy.deepcopy(list(self.model.parameters()))\n",
    "        n = self.model.layers\n",
    "        def local_model(*params):\n",
    "            # print(f'len of params:{len(params)}')\n",
    "            # print(f'shape of params[0]:{params[0].shape}')\n",
    "            # with torch.no_grad():\n",
    "            #initialize model with given params\n",
    "            i = 0\n",
    "            for i, param in enumerate(self.model.parameters()):\n",
    "                param.data = params[i]\n",
    "            pred = self.model(X)\n",
    "            loss = self.loss_fn(pred, y)\n",
    "            # print(f'loss type:{type(loss)}')\n",
    "            return loss\n",
    "        p =list(self.model.parameters())\n",
    "        hess_mat = hessian(local_model, tuple(p))\n",
    "        hess_norm = torch.tensor(0.).to(device)\n",
    "        for i in range(len(hess_mat)):\n",
    "            for j in range(len(hess_mat[0])):\n",
    "                hess_norm+= hess_mat[i][j].pow(2).sum()\n",
    "        \n",
    "        # print(f'Hess mat len:{len(hess_mat)}')\n",
    "        # print(f'Hess mat[0] len:{len(hess_mat[0])}')\n",
    "        # print(f'Hess mat[0][0] shape:{hess_mat[0][0].shape}')\n",
    "        \n",
    "        hess_norm = hess_norm.sqrt()\n",
    "        # print(f'hess norm:{hess_norm}')\n",
    "        \n",
    "        # Reinitialize the original params to model\n",
    "        for i, param in enumerate(self.model.parameters()):\n",
    "                param.data = prev_params[i]\n",
    "        \n",
    "        return hess_norm\n",
    "    \n",
    "    def get_hessianv2(self, X,y):\n",
    "        names = list(n for n, _ in self.model.named_parameters())\n",
    "        def loss_fun_hess(*params):\n",
    "            out: torch.Tensor = _stateless.functional_call(self.model, {n: p for n, p in zip(names, params)}, X)\n",
    "            local_loss = self.loss_fn(out, y)\n",
    "            return local_loss\n",
    "        hess_mat = hessian(loss_fun_hess, tuple(self.model.parameters()))\n",
    "        # print(f'len of hess mat:{len(hess_mat)}')\n",
    "        # print(f'hess_mat[0] shape:{len(hess_mat[0])}')\n",
    "        # print(f'hess_mat[0][0] shape:{hess_mat[0][0].shape}')\n",
    "        hess_norm = torch.tensor(0.).to(device)\n",
    "        for i in range(len(hess_mat)):\n",
    "            for j in range(len(hess_mat[0])):\n",
    "                hess_norm+= hess_mat[i][j].pow(2).sum()\n",
    "        hess_norm = hess_norm.sqrt()\n",
    "        # print(f'v2 hess norm{hess_norm}')\n",
    "        return hess_norm.cpu()\n",
    "        \n",
    "    def fit(self, train_loader, test_loader, epochs, store_grads=False, store_hessian=False, store_gen_err=False, store_weights=False, store_pt_loss=True, store_freq = 20, freq_reduce_by=None, freq_reduce_after=None):\n",
    "        \n",
    "        ## For Book keeping results ##\n",
    "        self.grads_norms = []\n",
    "        self.param_list = []\n",
    "        self.hess_norms = []\n",
    "        self.gen_err = []\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        self.point_loss = []\n",
    "        ## Initializing values ##\n",
    "        terminate_training = False\n",
    "        store_count = 0\n",
    "        \n",
    "        for epoch in tqdm(range(epochs), total=epochs, unit=\"epoch\", disable=True):\n",
    "            if terminate_training == True:\n",
    "                break\n",
    "            for batch, (X, y) in tqdm(enumerate(train_loader), total=len(train_loader), unit='batch'):\n",
    "                # if batch>300:\n",
    "                #     terminate_training = True\n",
    "                #     break\n",
    "                \n",
    "                X, y =X.to(device), y.to(device)\n",
    "                pred = self.model(X)\n",
    "                loss = self.loss_fn(pred, y)\n",
    "\n",
    "                # Backpropagation\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                ## Saving point loss\n",
    "                if store_pt_loss and (batch%store_freq==0):\n",
    "                    self.point_loss.append(loss.item())\n",
    "                    \n",
    "                ## Saving the weights\n",
    "                if store_weights and (batch%store_freq==0):\n",
    "                    current_params = tuple(self.model.parameters())\n",
    "                    self.param_list.append(current_params)\n",
    "                \n",
    "                ## computing and saving the gradient\n",
    "                if store_grads and (batch% store_freq == 0):\n",
    "                    # store_count += 1\n",
    "                    # # print(f'\\tstore_freq:{store_freq}, batch:{batch}')\n",
    "                    # if store_count%freq_reduce_after==0:\n",
    "                    #     store_freq += freq_reduce_by\n",
    "                    #     # print(f'store freq:{store_freq}, batch:{batch}')\n",
    "                    grad_norm_per_update = self.get_gradient()\n",
    "                    # print('\\tgrad norm:', grad_norm_per_update)\n",
    "                    self.grads_norms.append(grad_norm_per_update)\n",
    "                    \n",
    "                ## computing and saving hessian\n",
    "                if store_hessian and (batch% store_freq==0):\n",
    "                    #assert False, \"Not implemented\"\n",
    "                    self.optimizer.zero_grad()\n",
    "                    self.hess_norms.append(self.get_hessianv2(X,y))\n",
    "                    store_count += 1\n",
    "                    if store_count%freq_reduce_after==0:\n",
    "                        store_freq += freq_reduce_by\n",
    "                \n",
    "                ## computing and storing the generalization error\n",
    "                if store_gen_err and (batch% store_freq == 0):\n",
    "                    assert False, \"fix reducing freq to get it working\"\n",
    "                    train_loss, test_loss, point_loss=0, 0, 0\n",
    "                    with torch.no_grad():\n",
    "                        for sub_batch, (X_local,y_local) in enumerate(train_loader):\n",
    "                            # if sub_batch> batch: # only taking the encountered points to calculate train loss\n",
    "                            #     break\n",
    "                            X_local, y_local = X_local.to(device), y_local.to(device)\n",
    "                            pred_local = self.model(X_local)\n",
    "                            train_loss += self.loss_fn(pred_local, y_local).item()\n",
    "                    train_loss = train_loss/(batch+1)\n",
    "                    with torch.no_grad():\n",
    "                        for sub_batch, (X_local,y_local) in enumerate(test_loader):\n",
    "                            X_local, y_local = X_local.to(device), y_local.to(device)\n",
    "                            pred_local = self.model(X_local)\n",
    "                            test_loss += self.loss_fn(pred_local, y_local).item()\n",
    "                    test_batch_size = len(test_loader)\n",
    "                    # print(f\"Number of batches in test:{len(test_loader)}\")\n",
    "                    test_loss = test_loss/ len(test_loader)\n",
    "                    self.train_loss.append(train_loss)\n",
    "                    self.val_loss.append(test_loss)\n",
    "                \n",
    "                if batch % 10000 == 0:\n",
    "                    loss, current = loss.item(), batch * len(X)\n",
    "                    correct = 0\n",
    "                    test_loss = 0\n",
    "                    with torch.no_grad():\n",
    "                        for X, y in test_loader:\n",
    "                            X, y = X.to(device), y.to(device)\n",
    "                            pred = self.model(X)\n",
    "                            test_loss += self.loss_fn(pred, y).item()\n",
    "                            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "                    acc = 100*correct/len(test_loader.dataset)\n",
    "                    print(f\"\\taccuracy:{acc}\")#, at batch:{batch}\")\n",
    "                    print(f\"\\tloss: {loss:>7f}\")\n",
    "                \n",
    "                    # print(f'Learning rate:{self.scheduler.get_last_lr()}')\n",
    "                self.scheduler.step()\n",
    "            \n",
    "def exp_get_lp_sm(train_data_all, test_data_all, op_features, weight = 10, times = 8, epochs = 1, root_dir='', path=None, clear_file = True, freq_reduce_by=10, freq_reduce_after=100):\n",
    "    grad_list        = []\n",
    "    hess_norm_list   = []\n",
    "    if path is not None:\n",
    "        grad_file_path = root_dir+'grad_'+path\n",
    "        hess_file_path = root_dir+'hess_'+path\n",
    "        # gen_file_path = root_dir+'gen_'+path\n",
    "        if clear_file:\n",
    "            with open(grad_file_path, 'w+') as f:\n",
    "                f.write('')\n",
    "            with open(hess_file_path, 'w+') as f:\n",
    "                f.write('')\n",
    "            \n",
    "    for t in range(times):\n",
    "        train_loader, test_loader = get_random_subset(train_data_all, test_data_all)\n",
    "        print(f'Time:{t}')\n",
    "        train_model = Train_nn(784, [weight], op_features, lr= details['alpha_0'])\n",
    "        train_model.fit(train_loader, test_loader, epochs=epochs, store_grads=True, store_hessian=True, store_freq=details['book_keep_freq'],  store_gen_err=False, store_pt_loss=False, store_weights=False, freq_reduce_by = freq_reduce_by, freq_reduce_after=freq_reduce_after)\n",
    "        \n",
    "        with open(grad_file_path,'a+') as f:\n",
    "            f.write(' '.join([str(grad) for grad in train_model.grads_norms]) + '\\n')\n",
    "        with open(hess_file_path,'a+') as f:\n",
    "            f.write(' '.join([str(hess) for hess in train_model.hess_norms]) + '\\n')\n",
    "        \n",
    "        hess_norm_list.append(train_model.hess_norms)\n",
    "        grad_list.append(train_model.grads_norms)\n",
    "         \n",
    "    return grad_list, hess_norm_list\n",
    "\n",
    "grad_list, hess_norm_list = exp_get_lp_sm(train_data_all, test_data_all, op_features=10, \n",
    "              weight=details['g_weight'], times=details['g_times'], \n",
    "              epochs=details['g_epochs'], root_dir=details['result_root_dir'], \n",
    "              path=details['result_path'], freq_reduce_by=details['freq_reduce_by'], \n",
    "              freq_reduce_after=details['freq_reduce_after'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ccfb24-6798-426d-98f1-e6a90bc3bdd9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## EXP3 plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6b9646c0-4dbf-4aac-831b-afc3f8c50dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "def get_exp_details(root_dir, path):\n",
    "    result_details={}\n",
    "    details_path = root_dir+ 'details_'+ path + '.txt'\n",
    "    with open(details_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        key, val = line[:-1].split(':')\n",
    "        key, val = key.strip(' '), val.strip(' ')\n",
    "        if key in ['ratio', 'alpha_0']:\n",
    "            val = float(val)\n",
    "        if key in ['Times', 'Weights', 'Epochs', 'book_keep_freq', 'g_times', 'g_epochs','g_weight', 'freq_reduce_by', 'freq_reduce_after']:\n",
    "            val = int(val)\n",
    "        result_details[key] = val\n",
    "    return result_details\n",
    "\n",
    "def get_exp_results(r_det, train_size=20000):\n",
    "    root_dir = r_det['result_root_dir']\n",
    "    path = r_det['result_path']\n",
    "    grad_file_path = root_dir+'grad_'+path\n",
    "    hess_file_path = root_dir+'hess_'+path\n",
    "    gen_file_path  = root_dir+'gen_'+path\n",
    "    \n",
    "    # print(result_details)\n",
    "    grad_list    = []\n",
    "    hess_list    = []\n",
    "    gen_err_list = []\n",
    "    \n",
    "    with open(grad_file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        tensors = line[:-1].split(') ')\n",
    "        # print('tensors',tensors)\n",
    "        t_list = [float(t[7:].split(' ')[0][:-1]) for t in tensors]\n",
    "        # print('grad_list_i length:',len(t_list))\n",
    "        grad_list.append(t_list)\n",
    "    \n",
    "    with open(hess_file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        tensors = line[:-1].split(' ')\n",
    "        # print('tensors',tensors)\n",
    "        t_list = [float(t[7:-1]) for t in tensors]\n",
    "        # print('hess_list_i length:',len(t_list))\n",
    "        hess_list.append(t_list)\n",
    "    hess_list = np.array(hess_list)\n",
    "    grad_list = np.array(grad_list)\n",
    "    print('hess list shape:',hess_list.shape)\n",
    "    print('grad list shape:',grad_list.shape)\n",
    "    K_g = np.max(np.mean(np.array(grad_list), 0))\n",
    "    L_g = np.max(np.mean(np.array(hess_list), 0))\n",
    "    \n",
    "    book_keep_freq = r_det['book_keep_freq']\n",
    "    freq_reduce_after = r_det['freq_reduce_after']\n",
    "    freq_reduce_by = r_det['freq_reduce_by']\n",
    "    x_values = []\n",
    "    count_keep = 0\n",
    "    for i in range(train_size):\n",
    "        if i%book_keep_freq==0:\n",
    "            x_values.append(i+1)\n",
    "            count_keep+=1\n",
    "            if count_keep%freq_reduce_after==0:\n",
    "                book_keep_freq+=freq_reduce_by\n",
    "    \n",
    "    return grad_list, hess_list, K_g, L_g, x_values\n",
    "\n",
    "def plot_exp_data(root_dir_list, path_list, ratios, save_fig=False, train_size=20000):\n",
    "    grad_mean_list = []\n",
    "    grad_df_list = []\n",
    "    grad_err_list  = []\n",
    "    hess_mean_list = []\n",
    "    hess_df_list = []\n",
    "    hess_err_list  = []\n",
    "    data = {}\n",
    "    for root_dir, path, ratio in zip(root_dir_list, path_list, ratios):\n",
    "\n",
    "        res_details = get_exp_details(root_dir, path)\n",
    "        grad_list, hess_list, K_g, L_g, x_values = get_exp_results(res_details, train_size)\n",
    "        grad_list_m = np.max(grad_list,1)\n",
    "        hess_list_m = np.max(hess_list,1)\n",
    "        # K_g = np.percentile(np.mean(grad_list, 0),90)\n",
    "        # L_g = np.percentile(np.mean(hess_list, 0),90)\n",
    "        \n",
    "        # K_g = np.max(np.mean(grad_list, 1))\n",
    "        # L_g = np.max(np.mean(hess_list, 1))\n",
    "        # print('grad_list shape',grad_list_m.shape)\n",
    "        # print('grad_list ',grad_list_m)\n",
    "        # print(f'K_g:{K_g}, L_g:{L_g}')\n",
    "        grad_mean, grad_err = np.mean(grad_list_m), np.std(grad_list_m)\n",
    "        hess_mean, hess_err = np.mean(hess_list_m), np.std(hess_list_m)\n",
    "        grad_mean_list.append(grad_mean)\n",
    "        grad_err_list.append(grad_err)\n",
    "        hess_mean_list.append(hess_mean)\n",
    "        hess_err_list.append(hess_err)\n",
    "        \n",
    "        for run_id in range(len(grad_list)):\n",
    "            for epoch in range(len(grad_list[run_id])):\n",
    "                grad_df_list.append({'key':int(train_size*ratio), 'run_id': run_id, 'epoch':epoch, 'val': grad_list[run_id][epoch]})\n",
    "                                      \n",
    "        for run_id in range(len(hess_list)):\n",
    "            for epoch in range(len(hess_list[run_id])):\n",
    "                hess_df_list.append({'key':int(train_size*ratio), 'run_id': run_id, 'epoch':epoch, 'val': hess_list[run_id][epoch]})\n",
    "        \n",
    "        # plt.errorbar(x_values, np.mean(grad_list,0),np.std(grad_list,0), label='ratio:'+str(ratio))\n",
    "    df1 = pd.DataFrame(grad_df_list)\n",
    "    df2 = pd.DataFrame(hess_df_list)\n",
    "    \n",
    "    temp_df1 = df1.groupby(['key','run_id'])['val'].max().reset_index()\n",
    "    temp_df2 = df2.groupby(['key','run_id'])['val'].max().reset_index()\n",
    "    # temp_df1 = df1\n",
    "    # temp_df2 = df2\n",
    "    grad_mean_list = np.array(grad_mean_list)\n",
    "    grad_err_list = np.array(grad_err_list)\n",
    "    hess_mean_list = np.array(hess_mean_list)\n",
    "    hess_err_list = np.array(hess_err_list)\n",
    "    x_values = [str(int(train_size*t)) for t in ratios]\n",
    "    fig, ax = plt.subplots(figsize=(12,7))\n",
    "    sns.pointplot(data = temp_df1, x='key',y='val', estimator=np.mean, ax=ax, label='Lipschitz const.', color='orange', errwidth=0)\n",
    "    ax.fill_between(x_values, grad_mean_list-grad_err_list, grad_mean_list+ grad_err_list, color='orange', alpha=.3)\n",
    "    plt.xlabel('Number of parameters', fontsize = 24)\n",
    "    plt.ylabel('Lipschitz constant', fontsize = 24)    \n",
    "    sns.set_context(\"talk\")\n",
    "    sns.set(font='sans-serif', style='whitegrid', font_scale=2, rc={\"lines.linewidth\": 2, \"lines.markersize\":10})\n",
    "    plt.title('Mnist dataset', fontsize=24)\n",
    "    if save_fig:\n",
    "        plt.savefig('e3_Lipschitz_constant_mnist.png')\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12,7))\n",
    "    sns.pointplot(data = temp_df2, x='key',y='val', ax=ax, label='Smoothness const.', color='green', errwidth=0)\n",
    "    ax.fill_between(x_values, hess_mean_list-hess_err_list, hess_mean_list+ hess_err_list, color='green', alpha=.3)\n",
    "    plt.xlabel('Number of parameters', fontsize = 24)\n",
    "    plt.ylabel('Smoothness constant', fontsize = 24)    \n",
    "    sns.set_context(\"talk\")\n",
    "    sns.set(font='sans-serif', style='whitegrid', font_scale=2, rc={\"lines.linewidth\": 2, \"lines.markersize\":10})\n",
    "    # plt.show()\n",
    "    plt.title('Mnist dataset', fontsize=24)\n",
    "    if save_fig:\n",
    "        plt.savefig('e3_Smoothness_constant_mnist.png')\n",
    "    return df1, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3863788d-37ce-4b01-8d20-c6ea38a9707c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hess list shape: (6, 396)\n",
      "grad list shape: (6, 400)\n",
      "hess list shape: (6, 396)\n",
      "grad list shape: (6, 400)\n",
      "hess list shape: (6, 396)\n",
      "grad list shape: (6, 400)\n",
      "hess list shape: (6, 396)\n",
      "grad list shape: (6, 400)\n",
      "hess list shape: (6, 396)\n",
      "grad list shape: (6, 400)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "root_dir_list = ['results/t2/','results/t2/','results/t2/','results/t2/','results/t2/','results/t2/']\n",
    "path_list = ['try1_t6_r.1', 'try1_t6_r.25', 'try1_t6_r.5','try1_t6_r.75', 'try1_t6_r1.0', 'try1_t6_r1.5']\n",
    "ratios = [0.1, 0.25, 0.5, 1, 1.5]\n",
    "grad_df, hess_f= plot_exp_data(root_dir_list[:], path_list[:], ratios[:], save_fig=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed38903-dfcb-4f7f-9b62-f3be6a0f8efa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "384c46be-8ca7-40d6-b46e-4edd290e9639",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hess list shape: (8, 100)\n",
      "grad list shape: (8, 100)\n",
      "hess list shape: (8, 100)\n",
      "grad list shape: (8, 100)\n",
      "hess list shape: (8, 100)\n",
      "grad list shape: (8, 100)\n",
      "hess list shape: (8, 100)\n",
      "grad list shape: (8, 100)\n",
      "hess list shape: (8, 100)\n",
      "grad list shape: (8, 100)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvIAAAHdCAYAAAB/iZ0yAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAACMDUlEQVR4nOzdd3xUVf7/8dedyWTSCSEQeicg1QbYK2vbVVFWsWFva1l33XUV/doLqKxrwbaubS0oq2L5qSBgARUp0puA0iGEJKS3Seb+/jiZSULqpE0meT8fj3mQnDl37ifDJXzumXM+x7Jt20ZEREREREKKI9gBiIiIiIhI4JTIi4iIiIiEICXyIiIiIiIhSIm8iIiIiEgIUiIvIiIiIhKClMiLiIiIiIQgJfIiIiHilFNOYfDgwSxevDjYodRbKMYsIhIqlMiLiNTgrrvuYvDgwQwePJhhw4aRnp5ea/958+b5+w8ePJiPPvqohSIN3EcffcRzzz3Hhg0bgh1KwObNm8dzzz3XJm4OsrOzee6553juueeCHYqIhCAl8iIi9VBSUsJnn31Wa5+PP/64WWPo1asX/fr1IzIystGvNWvWLKZPnx6yifz06dNZsmRJsENptOzsbKZPn8706dODHYqIhCAl8iIidejevTsAn3zySY19MjMz+fbbb4mKiiI+Pr5Z4njzzTeZPXs2I0eObJbXFxGR0KJEXkSkDoceeii9e/dm/fr1bN68udo+n3/+OR6Ph9NPPx23293CEYqISHsUFuwARERCwbnnnstzzz3Hxx9/zB133FHled9o/bnnnsuPP/5Y7Wvs2rWLU089FYBffvmFTZs28eKLL7JkyRKys7Pp0aMHZ599Ntdddx3h4eFVjj/llFPYvXs3//3vfxk7dmyl5zZu3Mhrr73GsmXLSE1NxeVykZCQQN++fTn++OOZOHEikZGRfPTRR0yePNl/3OTJkyt936NHD77++uuA3ptPP/2Ut99+m82bN+NyuRgyZAhXX301J510Uo3HlJaWsnTpUubNm8fy5ctJSUkhOzub+Ph4Ro0axWWXXcbRRx9d6ZjFixdz+eWX+7+vbkrKL7/84v9606ZNzJ07lx9++IE9e/aQlpZGdHQ0ycnJnHPOOZx//vk4nc5q41uyZAlvv/02K1euJCMjA7fbTadOnRg4cCAnnHACF154IQ5H1bGwZcuW8c477/Dzzz+TkZFBdHQ0hxxyCH/84x/5/e9/j2VZ/r6TJk2qND1o8ODBlV7rlltu4dZbb63xPRQRUSIvIlIPvkT+s88+429/+1ulJG7r1q2sWrWKbt26VUmwa/L9999z8803U1hYSGxsLCUlJWzdupVnn32WdevW8cILL9Q7tu+++46bb74Zj8cDQHh4OA6Hg127drFr1y6+//57jj/+eAYMGEBERASJiYlkZWXh8XiIiYkhIiLC/1odO3as93kBHnroId555x0AHA4HYWFhLFmyhMWLF3PPPffUeNyvv/7KFVdc4f8+PDwcl8vF/v37mTdvHvPmzeOvf/0rN954o7+Py+UiMTGRnJwcioqKiIqKIioqqsZzTJo0iczMTACcTidRUVFkZmayZMkSlixZwty5c3nhhRcIC6v8X+H777/Pfffd5/8+MjISr9fL9u3b2b59O/Pnz+e8886r8snLk08+yX/+8x//99HR0WRnZ7No0SIWLVrE119/zbRp0/zXTocOHejYsSMHDhwAIDExsdLr1faziYiAEnkRkXrp1asXhx9+OMuXL+enn37imGOO8T/nW+R69tlnVztKW52//vWvnHzyyfz973+nZ8+e5Ofn8/bbb/PUU08xf/58vvvuO0488cR6vdbDDz+Mx+Ph5JNP5s4776Rfv34A5ObmsnHjRj755BN/0nnWWWdx1lln+UeD77nnHs4///wA3olyn376qT+Jv/rqq/nTn/5EXFwcaWlpPPnkkzzxxBNVkmQfl8vFGWecwfjx4xkxYgSdOnXCsizS09N5//33mT59Ok8//TRHH300o0aNAuDwww/nhx9+4K677mLWrFlcffXVtY5Yjx49mhNPPJFjjz2WLl26EBYWRn5+PnPnzuXJJ5/ku+++44033uDaa6/1H1NQUMDUqVMBmDBhArfeeivdunUDzDqI1atX88knn1QaWQezfuE///kPCQkJ3HrrrfzhD38gLi6OoqIivv76ax599FE+//xzBg8ezA033ACYTxQqfkrzww8/NOSvQUTaMc2RFxGpp/HjxwOVq9PYtu2vZuN7vj5GjBjBv/71L3r27AmY0dfrr7/ePx1l9uzZ9Xqd9PR0du7cCcAjjzziT+IBYmJiOPLII3n44Yf952kqtm37p7Wcd9553HnnncTFxQFmZHnq1KmMHj2agoKCao/v168fzzzzDCeffDKJiYn+xLhTp07cdNNN3Hzzzdi2zXvvvdfgGKdPn84FF1xA9+7d/TcUUVFRnHvuuTz99NMAvPvuu5WO2bx5M/n5+URFRfHwww/7k3iA+Ph4TjjhBP75z39WmvqUnZ3N008/TVhYGC+//DKXXHKJ/71wu92ceeaZTJ8+HcuyePXVVykuLm7wzyQiUpESeRGRejrzzDNxu93MnTuX/Px8wMyl3r17N8OHD2fAgAH1fq3rrruuyqgu4B+drWlR7cGio6P9nwLs37+/3udvrA0bNrB9+3YArr/++irPW5blH3luiFNOOQWA5cuXN/g1anPkkUcSFxfH7t272bdvn789OjoaAI/H45+WU5c5c+aQn5/P4YcfXmNFoUMPPZRevXqRlZXFunXrGh2/iAhoao2ISL3FxcVx8sknM3v2bL766ivGjx/vH50PZDQezIh8dZKSkgAzylsfERERjB49msWLF3PNNddw2WWXcfLJJ5OcnFzjQs6m4EtGO3XqRP/+/avtc/jhhxMWFkZJSUm1zxcWFvLee+8xf/58tmzZQnZ2dpW+qampjYpz9uzZfPrpp6xfv56MjAyKioqq9ElNTfW/73379qVv375s27aNiRMnctlll3H88cfTv3//am+8AFasWAHA6tWrOfbYY2uMJSsrC4C9e/dy2GGHNernEhEBJfIiIgEZP348s2fP5pNPPuGMM87gq6++wuVy8fvf/z6g14mJiam23TeXvabktzqPPvooN9xwA7/++ivPPPMMzzzzDFFRUYwePZrf//73/P73v69xrnpD+RZo+hLg6oSHh9OxY8dqPylITU1l0qRJbNu2zd8WFRVFXFwcDoeD0tJSDhw44P/kI1AlJSX85S9/Ye7cuVXi8d3gZGRk4PV6K03/cTqdTJs2jZtvvpmdO3cyZcoUpkyZQnx8PGPHjuXcc8/llFNOqZTU+36+wsJCCgsL64ytPn1EROpDibyISACOP/54OnXqxE8//cTbb79Nbm4up5xyCgkJCUGLqVevXnz66ad8++23LFiwgGXLlvHrr7/y3Xff8d133/Hmm2/y1ltv+aeNtCTbtqttf+yxx9i2bRu9evXiH//4B2PHjqVDhw7+53fs2MHvfve7Bp935syZzJ07l8jISG6//XZOO+00unbtWqnPiSeeSEpKSpUYR4wYwVdffcVXX33FDz/8wM8//8zOnTuZM2cOc+bM4YQTTuCll17y3xB4vV4ArrzyykqlPEVEmpvmyIuIBCAsLIyzzjoLr9frXzB57rnnBjcoTFzjxo3joYce4osvvuD777/nH//4B263m3Xr1lWpt95YvjKVtU19KS4urnaeeXFxMfPnzwdg2rRpnHbaaZWSeIC0tLRGxedbLHzTTTdx+eWXV0nifSP+NYmIiOCcc87h8ccf95fDvOGGG7AsiwULFlRahOsrG7lly5ZGxSwiEigl8iIiAfLNh/d4PHTo0MG/MLM16dy5M9dcc42/VvvSpUsrPe+bGlLTiHldhg0bBpiEe+vWrdX2WbFiRbVThA4cOOCv3DJ06NBqj61pUy2oX+y+BayHHHJItc8vX7682vnyNenVqxe33347Z511FkCljZwOPfRQwLzHtd0cVKdiudKG/l2ISPulRF5EJEDDhw/n1ltv5eqrr+buu++udhfWluLxeGpNAH1z7g8ueeibo5+Tk9Og8x5yyCH06dMHgFdeeaXK87Zt8+9//7vaY2NiYvzJeMWdWH1SU1N5++23azy3L/baFgT7+mzatKnKcyUlJf5PUw5WV2nI6t7PM844g6ioKIqKinjiiSdqPd634PXgOKH+C5xFRHyUyIuINMAtt9zCnXfeGXC1mqa2ZcsW/vCHP/DGG2+wdetWf1Lv8XiYM2cOb7zxBgDHHXdcpeMGDRoEwFdffdWgZN6yLG655RYAPvzwQ5588kl/IpqWlsbdd9/N4sWLiYyMrHJsdHS0fxT77rvvZsOGDYCZa75o0SImTZpU682JL/aFCxfWOLXHVz3mhRdeYN68eZSWlgJmR9kbb7yR1atXV7tz6oIFC5g4cSIzZ85k9+7d/vaCggJmzpzp3zOg4vvZsWNHbr/9dgA++ugjbrvttko3EEVFRSxbtowHH3yQiy++uNL54uLi6NKli/9YEZFAaLGriEiI27Jli7+6Snh4OFFRUWRnZ/sXYQ4fPpybbrqp0jHnnHMOr776Kj///DNHHXUUCQkJuFwukpKSmDFjRr3Oe84557By5Ureeecd/vOf//D6668TExNDdnY2tm1zzz338MYbb1RKiH0mT57M5ZdfzqZNmxg/fjxRUVF4vV4KCwuJj4/n0Ucf5eabb672vOPGjeOf//wn27Zt48QTT6RTp07+T0W+/vprwOw0++WXX7Jjxw5uvvlmXC4Xbreb3NxcnE4njzzyCNOnT6+2Ks7KlStZuXIlYObKu91u/88EZpHsxIkTKx0zadIkcnJyePbZZ5k9ezazZ88mMjKS8PBwcnJy/H8XPXr0qHK+Cy64gOeff56pU6fy7LPP+tcfXH755Vx55ZX1+JsQkfZKI/IiIiFswIABPPvss1x00UUMHTqUuLg4cnNziYmJ4YgjjuDee+9lxowZVcpdDhgwgNdff53jjz+emJgY0tLSqmyOVB/33XcfTz75JKNGjSI8PBzbthk9ejQvv/wyl19+eY3HjRo1ivfff59x48bRoUMHPB4PnTp1YuLEiXz88ccMGTKkxmMTEhJ48803Oe2000hISCAjI4Pdu3dXumGIj4/n/fff5+KLL/YvdI2IiGDcuHG89dZbnH/++dW+9lFHHcUTTzzBeeedR3JyMhEREeTl5REfH88xxxzD448/zksvvVRtOc+bbrqJTz75hIkTJ9K3b19s2yY/P5/OnTtzwgkn8MADD/C///2vynE333wzf//73xk8eDC2bft/loZOexKR9sOytbpGRERERCTkaEReRERERCQEKZEXEREREQlBSuRFREREREKQEnkRERERkRCk8pN18Hq95OXl4XK5/BuYiIiIiIg0Jdu28Xg8REdHV9r1uTZK5OuQl5dX7c6AIiIiIiJNLTk5mdjY2Hr1VSJfB5fLBZg3NZjbsIuIiIhI21VcXMymTZv8uWd9KJGvg286TXh4OG63O8jRiIiIiEhbFshUbi12FREREREJQUrkRURERERCkBJ5EREREZEQpEReRERERCQEKZEXEREREQlBSuRFREREREKQEnkRERERkRCkRF5EREREJAQpkRcRERERCUFK5EVEREREQpASeRERERGREKREXkREREQkBCmRFxEREREJQUrkRURERERCkBL51ixtCRRnBjsKEREREWmFlMi3Znm/QcrX4C0NdiQiIiIi0sookW/t8nZA5qpgRyEiIiIirYwS+dYusiuk/QSFqcGORERERERaESXyrZ3lBFcHSJkHpcXBjkZEREREWgkl8qHAFQeeHEhfGuxIRERERKSVUCIfKiK6woGVkL872JGIiIiISCugRD5UWA5wd4KUuVBSEOxoRERERCTIlMiHkrBo8Hog7Qew7WBHIyIiIiJBpEQ+1LiTIGsj5P4a7EhEREREJIiUyIcayzLz5fd9YxbAioiIiEi7pEQ+FDndpizlvm/B9gY7GhEREREJAiXyocrdGfJ3QNb6YEciIiIiIkGgRD6URXSD1IVQlBHsSERERESkhSmRD2UOl6lkkzIfvCXBjkZEREREWpAS+VAX3hGK0yBjRbAjEREREZEWpES+LYjoBhlLoCAl2JGIiIiISAtRIt8WWE5wdYSUeVBaFOxoRERERKQFKJFvK1yxUJIP6UuCHYmIiIiItAAl8m1JRBIcWAW524MdiYiIiIg0MyXybYnlMPXlU+ab0XkRERERabOUyLc1YVGADfu/B9sOdjQiIiIi0kyUyLdF7s6Qs9k8RERERKRNUiLfFlkWuJNg37dQnBXsaERERESkGSiRb6ucbrPz675vwfYGOxoRERERaWJK5NsydyIU7IbM1cGORERERESamBL5ti6iK+z/EQrTgh2JiIiIiDQhJfJtncMFYXFm11evJ9jRiIiIiEgTUSLfHoR3gOIDkPFzsCMRERERkSaiRL69iOxmEvn8PcGORERERESaQFiwA6jJb7/9xsKFC1mzZg1r165l27Zt2LbNM888wxlnnNFkx7QblhPCE8wUmz4XgjMi2BGJiIiISCO02kR+xowZ/Pe//232Y9qVsBgoSDGLX7ueEuxoRERERKQRWu3UmuTkZK655hr+9a9/MXfuXMaMGdMsx7Q7EV0gaz3kbA12JCIiIiLSCK12RP6CCy5okWPaHcsBEUmwbz5EXASumGBHJCIiIiIN0GpH5KUZOSPAsiB1gXZ9FREREQlRSuTbK3cXyN0KWRuDHYmIiIiINIAS+fYssivsXwjFmcGOREREREQCpES+PXOEgyMCUuaDtzTY0YiIiIhIAFrtYtfWZu3atS1+zo7Zv2HjwHa4m/U8Ls8GcncWUhgxpFnPIyIiIiJNR4l8PQ0fPhy3u3kT6iq2bwaczb95k7cvFO6B3r1MeUoRERERaVFFRUUBDxxrao2AIwxc8WbX19LiYEcjIiIiIvWgRF4MVxx4ciB9SbAjEREREZF6UCIv5SK6woFVkL8r2JGIiIiISB2UyEs5ywHuRDPFpqQg2NGIiIiISC0s27btYAdRnXXr1vHggw/6v9+yZQt5eXn07duXDh06+NtnzpzZqGPq4lt4EJzFru/RIotdD1aYAtH9oOupZgdYEREREWlWDck5W23VmtzcXFatWlWlfdu2bU16jFTDnQTZGyGmL8QODHY0IiIiIlKNVpvIjx07ll9++aXZj5FqWJaZL7/vG4hIAldssCMSERERkYNojrxUz+kGywn7vgXbG+xoREREROQgSuSlZu7OkL8DMtcFOxIREREROYgSealdRDdI+wGKMoIdiYiIiIhUoEReaudwgTMKUuaDtyTY0YiIiIhIGSXyUrfwjlCUBhkrgh2JiIiIiJRRIi/1E9kN0pdAQUqwIxERERERlMhLfVlOMzKfMg9Ki4IdjYiIiEi7p0Re6s8VCyX5kLY42JGIiIiItHtK5CUwEUmQtQZytwc7EhEREZF2TYm8BMZyQHiiqWJTkhfsaERERETaLSXyEriwKMCG1O/BtoMdjYiIiEi7pEReGsbdGXK3QM6mYEciIiIi0i4pkZeGsSxwJ8G+76A4K9jRiIiIiLQ7SuSl4ZxucITDvm/BWxrsaERERETaFSXy0jjuTlCw21SyEREREZEWo0ReGi+iG+z/EQrTgh2JiIiISLuhRF4azxEGYXFm11evJ9jRiIiIiLQLSuSlaYR3gOJMSF8W7EhERERE2gUl8tJ0IrvCgeWQvyfYkYiIiIi0eQEl8kuXLmXlypX17r969WqWLl0aaEwSqiwnhCeYKTalhcGORkRERKRNCwuk86RJk+jcuTMLFy6sV/+//OUvpKSksH79+gYFJyEoLAYKUszi16STTb15EREREWlyAU+tsW27WftLGxCRBFnrIXdrsCMRERERabOadY58Xl4eLperOU8hrZFlmWR+39fgyQ12NCIiIiJtUrMl8qtXryYrK4ukpKTmOoW0Zs4IsByQugBsb7CjEREREWlzap0jP2vWLGbNmlWpLSsri8svv7zGY2zbJicnhy1btmBZFieccELTRCqhx93ZTK/J2gjxQ4MdjYiIiEibUmsiv3v3bpYsWVKpzePxVGmryejRo/nzn//c8Ogk9EV2NaPyUd0gvGOwoxERERFpM2pN5MeNG0ePHj0AM9J+9913Exsby913313jMZZlERMTw6BBg+jTp0/TRiuhxxEOzkhI+Rp6jgeHM9gRiYiIiLQJtSbyQ4YMYciQIf7v7777btxuN+edd16zByZtiDsB8nfBgVXQ6fBgRyMiIiLSJgRUR37jxo3NFYe0dRFdIX0RRPeEiC7BjkZEREQk5DVr+UkRP0cYuOLLdn0tDnY0IiIiIiEvoBH5ivbu3cvmzZvJzs6mpKSk1r7jx49v6GmkLXHFQcEeSF8CXY4LdjQiIiIiIS3gRH7VqlU8+uijrFmzpt7HKJEXv4iucGAlRPeB6F7BjkZEREQkZAWUyK9du5YrrriCoqIibNuma9euJCUlER4e3lzxSVtjOUx9+ZR50OciCIsMdkQiIiIiISmgRH769OkUFhaSnJzMlClTGDZsWHPFJW1ZWBSUZMP+H6DrqWBZwY5IREREJOQEtNh1xYoVWJbFtGnTlMRL47iTIHsj5GwJdiQiIiIiISmgRL6oqIioqCiSk5ObKx5pLyzLzJdP/RY82cGORkRERCTkBJTI9+7dm+Li4jqr1IjUi9MNVhjs+w5sb7CjEREREQkpASXy559/Ph6Ph/nz5zdXPNLeuBMhfwdkrgt2JCIiIiIhJaBE/vLLL+e4447j/vvvZ8WKFc0Vk7Q3Ed0g7Qcoygh2JCIiIiIhI6CqNS+88AIjRoxg9erVXHLJJRx55JEMHz6c6OjoWo+75ZZbGhWktHEOFzijIGU+9DrP7AIrIiIiIrUKuPykZVnYtg3A0qVLWbZsWY39bdvGsiwl8lK38I6QvwsylkPimGBHIyIiItLqBZTIjx8/HquFan7/9ttvLFy4kDVr1rB27Vq2bduGbds888wznHHGGbUe+9lnnzFjxgx++eUXvF4v/fr1Y8KECVx88cU4HAHNJpKWFNkN0pdBdG+I7BrsaERERERatYAS+alTpzZXHFXMmDGD//73vwEf9+CDD/Luu+/idrs5+uijCQsLY9GiRTz00EMsWrSIZ555BqfT2QwRS6NZTgiPN7u+9r7AVLURERERkWq12uHp5ORkrrnmGv71r38xd+5cxoype7rFnDlzePfdd+ncuTOffvopL7/8Ms8//zxfffUVAwYMYO7cubz99tstEL00mCsWSvIh7adgRyIiIiLSqrXaVYUXXHBBwMe8/PLLAPz973+nb9++/vbExEQeeOABJk2axCuvvMKkSZM0xaY1i0iCrLUQ3Rdi+gQ7GhEREZFWqVGJvG3bZGVlUVBQ4F8AW53u3bs35jT1kpKSwrp163C5XNXOoR8zZgxJSUns27ePlStXcvjhhzd7TNJAlgPCE00Vm74TIaz2qkgiIiIi7VGDEnnfFJZVq1ZRVFRUa1/Lsli/fn2DgguE7xyDBg0iIiKi2j4jRoxg3759bNiwQYl8axcWBSW5kLoQup0OLbTIWkRERCRUBJzI33///cycObPWEfiK6tuvsXbt2gXUPvrfrVu3Sn2llXN3htwtkNMP4gYHOxoRERGRViWgieJz5szh/fffJzIykieeeIIlS5YAZg76+vXrWbBgAVOnTqVv377Ex8fz2muvsXHjxmYJ/GD5+fkAREZG1tjHt3FVXl5ei8TUYEXppnJLxkooSAl2NMFjWWbX133fQXFWsKMRERERaVUCGpH/3//+h2VZ/O1vf+Occ86p9JzD4aBLly6MHz+e0047jSuuuIKbb76ZDz74gAEDBjRp0NXxjfw3V537tWvXNsvrVhTu2Uv3tBfomDMPh+3xt+eH9ycj7gwK3QObPYbWyFmaSenu/5AVc7wpUSkiIiIigSXyvnno5557bqX2g6fPREVFce+993LhhRfy73//m8cff7yRYdbNN9ruG5mvjm8k3tc3EMOHD8ftbsa65lkbYP41UJha5amo4t+ISnsRBv8Vkk5qvhhas/wd0DkcOh4a7EhEREREmlxRUVHAA8cBTa3Jzs4mOjqamJgYf5vL5ao2eR45ciSRkZEsXrw4oIAaqkePHgDs2bOnxj4pKSmV+rYa3lJYeF61SXw5GzY9AwW7WyysViWiO+z/EQr3BzsSERERkVYhoEQ+ISGhSpWaDh06UFhYSEZGRpX+Xq+X9PT0xkVYT0OHDgVg8+bNFBYWVttnzZo1ABxyyCEtElO97f0Ssn+pu59dCrs/b/54WiNHGITFmZKUXk/d/UVERETauIAS+W7dulFSUsL+/eWjokOGDAHg+++/r9R36dKlFBUV0aFDhyYIs36xDRs2DI/Hw+zZs6s8v2TJElJSUujcuTOHHXZYi8RUb9tn1r/v/u/r7tNWhXeA4kxIXxbsSERERESCLqBEfsyYMQAsW1aeSJ1++unYts3UqVP58ssv2bZtG1999RV33nknlmVx7LHHNm3Etbj++usBmDZtGtu3b/e3p6en8+CDDwJw3XXXtb5dXYsCmC7iaefVWyK7woHlkN9OpxiJiIiIlLHsAAq9r169mgsvvJCTTjqJl156CYDS0lIuvfRSVq5cWalijG3bJCQk8L///a9Bc9LXrVvnT74BtmzZQl5eHn379q00yj9zZuXR7AceeIAZM2bgdrs55phjCAsLY9GiReTm5jJu3DieffZZnM76Vz7xLTxo1sWuP1wM29+rX9+wWDjmneaJI1SU5IK3GPpMBGf1m3+JiIiIhJKG5JwBVa0ZOXIky5cvrzSi7XQ6ee2113j++eeZM2cOKSkpxMbGcswxx/CXv/ylwQtLc3NzWbVqVZX2bdu21XrcAw88wBFHHME777zDkiVL8Hq99O/fnwkTJnDxxRe3vtF4gB7n1j+R76gdaQmLMfX19/8ISSdr11cRERFplwIakW+PWmREvrQYPu1fv4o0kT1g1BQIj2+eWEKFbUPBDuh2JsQ2/z4FIiIiIs2pITlnKxyeboec4XDcTAirR337gt2wajIUpTV/XK2ZZYE7CfZ9A57cYEcjIiIi0uICSuRPPfVULrzwwnr3v+SSSxg3blzAQbVLnY+BcQuhy0lVn3N1BKvCLKiC3bDyLiiouWZ+u+CMAMsBqd+B7Q12NCIiIiItKqA58rt3765SR742KSkp7N27N+Cg2q2Ew2DcN2aX17SfIG0RRHY37VkbYO1DUFpg+halmpH5EQ9CdN+ghh1U7s6Quw2yNkL80GBHIyIiItJimnVqTUlJSetcXNradTgEBlwFXU+BDsMAh/lz5KOmao1P8QFYdTdkbwpaqK1CZFdIXWDeDxEREZF2otmy7NzcXDIyMoiLi2uuU7Q/sQPLFromlLeV5MKaeyFzdfDiCjZHODgjIeVr8JYEOxoRERGRFlHr1JqNGzeycePGSm1FRUV8/PHHNR5j2zbZ2dnMnTuX0tJSRowY0SSBSpno3jBqKqy5DwpTTFtpAax5EIbeCZ3GBDe+YHEnQP4uOLAKOh0R7GhEREREml2tify8efN4/vnnK7Xl5uYyefLkOl/Ytm1cLpd/t1VpQpFdzcj8mvsgf6dpsz2wfgoM/gt0OTGo4QVNRFdI/wmie0FEl2BHIyIiItKsak3ke/TowZFHHun/funSpYSFhXHooYfWeIzD4SAmJoaBAwdy7rnn0r9//yYLVipwdypL5h+A3C2mzS6FjU+ZEfpuZwQ1vKBwhIErHvbOhd4XmLKeIiIiIm1UQBtCDRkyhMTERL7//vvmjKlVaZENoWqy/T3Aacos1qQkH9Y9DFnrKrf3uwp6ndes4bVaBXugw1DocnywIxERERGpl4bknAGVn5wyZUrLJ7NSu7AoGP4AbHgcMpaVt299HUrzoM+lZvOk9iSiKxxYDVF9IKZ3sKMRERERaRYBVa0577zzOOuss5orFmkopxuGTobOx1Vu3zETfn2l/W2WZDnAnQj75ptPLERERETaoIBG5IuKitizZw9ut5vu3btXeq6goIDp06ezdOlSiouLOf7447nxxhuJjo5u0oClBg4XDPkbOKMg5avy9j3/z8yZT74FLGfw4mtpYVFQkgP7f4Cu49rfpxIiIiLS5gU0Iv/ee+9x1lln8dJLL1VqLy0t5dJLL+W1115j9erVbNy4kf/85z9cddVVlJSorneLsZww6GbocW7l9n3zYcOT4PUEJ65gcXeB7F8gZ0uwIxERERFpcgEl8gsXLgTg3HMrJ4qff/4569evx+12c/3113PrrbcSExPDmjVrmDlzZtNFK3WzLOh/NfS5uHJ72o+w7hEoLQxOXMFgWWa+fOo34MkOdjQiIiIiTSqgRH7btm2AqV5T0eeff45lWdx2223cfvvt3HzzzTzyyCPYts0XX3zRZMFKPVmWSeT7X1O5/cAKWHM/lOQFJ65gcLrBcsG+b9vfWgERERFp0wJK5DMyMoiJiak0793r9bJ06VIAzjnnHH/7qaeeisPhYPPmzU0UqgSs57mQfCuV/pqzN8Dqe6A4K2hhtTh3otk4K3NtsCMRERERaTIBJfIej4fi4uJKbZs2bSI/P58BAwbQqVMnf3tYWBixsbHk5bWj0d/WqOvv4JC/g1VhXXPub7BqMhSlBS+ulhbRzUwvKkoPdiQiIiIiTSKgRL5z584UFxezY8cOf9s333wDwBFHHFGlf0FBAfHx8Y2LUBqv83Ew7B5wVNjptGAXrLzLbJ7UHjhcZRV9vgavFmCLiIhI6AsokT/yyCMBmDp1KhkZGWzcuJG3334by7I44YQTKvXdtm0bxcXFdOnSpemilYZLOMJsHOWMLG8rSjUj83nbgxZWiwrvaD6FyFge7EhEREREGi2gRP7qq6/G6XTyzTffcOyxx3LeeeeRnp7OgAEDOPnkkyv1/e677wAYNWpU00UrjRM/HEY+AmGx5W3FB2DV3ZC9KXhxtaTIbpC+DAr2BjsSERERkUYJKJEfMmQIzz//PD169MC2bSzLYuzYsbz44os4HJVf6n//+x8AxxxzTNNFK40XOwhGTYHwhPK2khxYcy9krgleXC3FcpqR+ZR5UFoU7GhEREREGsyybdtuyIEZGRlER0fjdrurPFdSUsKWLWYTnv79+xMeHl6lT6goKipi7dq1DB8+vNqftVltfw9wgjOi6V+7IMUk74X7ytsc4XDIndBpdNOfr7Up2ANxgyHppGBHIiIiItKgnDOgEfmKEhISajxJWFgYQ4YMYciQISGdxLdpkV1h1FSI6lXe5i2G9Y9B6sLgxdVSIrqacpS524IdiYiIiEiDNDiRlzbA3clMs4kZWN5ml8LGabB3TvDiagmWAyK6mCo27WmDLBEREWkzwuruUpXX62X58uVs3ryZ7OxsPB5Prf1vueWWBgUnLcAVZxbArn0IsteXNdqw+XkozYee5wU1vGbljARPjvkEotvpZkdcERERkRARcCI/d+5cHn74Yfbv319nX9+CWCXyrVxYFIx4ENZPgQMVSjP+9jqU5EOfS9pukuvuDLm/QvYv0GFIsKMRERERqbeAEvkff/yR2267Da/Xi8vlYuTIkSQlJbX8IlBpek632TRq41OQ9kN5+473TTI/4BozHaWtsSwzXz71O1OaMrxDsCMSERERqZeAEvmXXnoJr9fL6NGjeeqpp+jcuXNzxSXB4HDBIX+HTZGwb155+57PzDSb5FtM+ca2xhEODjfs+xp6nAOONvgzioiISJsT0BDrunXrsCyLqVOnKolvqyynSdh7nFO5fd982PAkeGtfDxGy3J3MJlGZq4MdiYiIiEi9BJTI27ZNTEwMPXr0aK54pDWwHND/Guh9UeX2tB9h3SNtdyOliG6QtggK617/ISIiIhJsASXyAwYMoKCggKKiNprISTnLgr6XmIS+ogMrYM39bbNkoyMMwuLMrq9t9ZMHERERaTMCSuQvueQSSkpK+OSTT5orHmltep4Lg24BKlStyV4Pq/8PirOCFlazCe9gfq70ZcGORERERKRWASXy5513Hn/84x957LHH+Pzzz5srJmltup0Gh9xReaFr7q+wajIUpQcvruYS2dWU4czfHexIRERERGoUUNWayZMnAxAeHs7f//53/vnPfzJ8+HCio6NrPMayLB577LHGRSnB1/k4cEbA+qngLTZtBbtg5Z0w8mFTurGtsJwQ3slMsekz0fzcIiIiIq2MZdu2Xd/OQ4YMwbIsAjgEy7LYsGFDg4JrDYqKili7di3Dhw9v+Xr5298DnK0rkcxcC+sehtKC8rbwBBjxEET3Dl5czaEgBWL7Q9IpbXdDLBEREWkVGpJzBjQirx1ahfjhMPIRWPMAlOSYtuIMM81mxAMQOyiY0TWtiCTI3gDRfSF2QLCjEREREalEibwELnYQjJoCa+4zSTyYpH71/8Gwe02y3xZYFriTYN83ENEFXLHBjkhERETEL6DFriJ+0b1h1FRwdylvKy2AtQ9ARhuq+OKMMHX1UxeA7Q12NCIiIiJ+SuSl4SK7wqGPQ1Sv8jZvMax7FFIXBi+upubuDHnbISt013qIiIhI2xPQ1Bofj8fDZ599xpdffsn69evJzMwEID4+nqFDh3LWWWfxhz/8AZfL1ZSxSmvk7gQjHzMj8bm/mja7FDZOMyP03U4LanhNJqKruTmJ6g7hHYMdjYiIiEhgVWsAduzYwc0338yWLVtqrF5jWRaDBg1i+vTp9O4d2pVMVLWmnkryYO3DZrOoivpfDT3HByWkJleUAa4Ys0mWo0H3wCIiIiLVavaqNbm5uVx55ZXs2bOHsLAwTj/9dI466ii6du0KQEpKCj/99BNz5sxh06ZNXHXVVXzyySfExMQE/tM0wp49e/j3v//NwoUL2bdvHzExMYwYMYIrr7ySY489tkVjaTfComHEg7B+itlMyee316AkH/pcHPolHN0JkL8LDqyCTkcEOxoRERFp5wJK5F9//XX27NlD9+7d+fe//83AgQOr9Lngggu48cYbueGGG9izZw9vvPFGi1a7WbVqFddddx1ZWVn06NGDk046idTUVL7//nsWLFjA3//+d6677roWi6ddcbph2D2w8Z+Q9mN5+473oDQP+l9jFo6GsshukP4TRPWEyKRgRyMiIiLtWEBZ1dy5c/07tVaXxPsMGjSIRx99FNu2+eqrrxodZH0VFRXx5z//maysLCZNmsTcuXOZPn06M2fO5PXXXycqKopp06axYsWKFoup3XG44JA7IOnUyu27P4NNz5n586HMcoIr3uz6Wloc7GhERESkHQsokd+5cycREREcddRRdfY9+uijiYyMZOfOnQ0OLlBz584lJSWFXr16ceedd+J0Ov3PHXXUUVx55ZUAvPjiiy0WU7tkOSH5VuhxduX2ffNhw5Pg9QQnrqbiioOSXEhfHOxIREREpB0L8XkOla1ZswaAMWPGVFsx55hjjgHgxx9/JDc3t0Vja3csB/S/FnpfVLk97UdTnrK0KDhxNZWIrnBgNeTuCHYkIiIi0k4FlMj37t2bwsJCFi1aVGffRYsWUVBQQK9evers21Ty8/MB6Nix+vKAvnaPx8OmTZtaLK52y7Kg7yWmck1FB5bDmvtNpZtQZTnAnWg+ZSjJD3Y0IiIi0g4FlMiPGzcO27a55557+PXXX2vst3HjRu655x4sy+K001qujnhCQgJAjdN5Krbv2rWrRWISTPnJQbcAFarWZK+H1feCJztYUTVeWJSZ87//ewisiquIiIhIowVUteaqq65i1qxZ7Nmzh3PPPZdx48YxduxYkpKSKC4uZs+ePSxevJgFCxZg2zY9evTwz0tvCUcddRQvvfQS3333HSkpKf6ymD7vvfee/+tAp9asXbu2SWIMRMfs37BxYDtauH59s+hPTMfLSDrwDhZe05S7haKlf2NP4g2UOuODGl2D2TaukvnkbM2lyN0n2NGIiIhIOxJQIh8TE8Prr7/OrbfeyqZNm5gzZw5z5syp1Me3SdTgwYN57rnnWrSG/NFHH83o0aNZunQpV199Nffeey8jRoxg//79vPrqq3z77beEhYVRUlKCwxHY8oDgbAi1mZDZEKpeBkJ6X9jwOHhNxRd3yT76Zb4MIx6GyK61H95alfYGTxr0GWcWwoqIiIgEyLchVCAC3tkVzBzzL774gjlz5rB+/XoyMjIAM7Vl6NChnH766Zx11lnVLjhtbunp6dx66638/PPPVZ6bNGkSS5cuZePGjTz99NOceeaZdb6ednZtBplrYN0jUFpQ3haeACMegugQ3Qm4KA3cnaDHH0K/Vr6IiIi0uIbknA1K5Fs727b58ccfWbx4MQcOHCAhIYFTTz2VYcOGceSRR5Kfn88XX3zBgAED6nwtJfLNJHsTrH0QSnLK28JiYcQDEDsoaGE1Sv4O6Hw8dBwZ7EhEREQkxDQk5wxoak2osCyLY489lmOPPbZS+9KlS8nPz6d79+70798/SNEJAHHJMOoxWHMfFB8wbSU5sPr/YNi9ED88uPE1REQ3U14zqocZnRcRERFpRgHNASgtLWXPnj3s27evzr779u1jz549eL3eBgfX1P79738DcMkll2BZVh29pdlF94FRU8HdpbyttADWPgAZy4IWVoM5XOCMNru+hvqmVyIiItLqBZTIf/HFF5x66qk8++yzdfZ9/PHHOfXUU6sshm1uv/zyCwUFBZXaCgsLefjhh1mwYAFDhgzhiiuuaNGYpBaR3eDQqRDZs7zNW2w2jdr/ffDiaqjweCjKgIwVwY5ERERE2riAptZ88cUXAEyYMKHOvhMnTuSLL77giy++qNei0qby+uuvM2fOHIYNG0aXLl3Iz89n+fLlZGVlkZyczCuvvEJ4eHiLxSP14E6EUVNg7f2Q+5tps0thwzSz2VK3ltuLoElEdoP0pRDdy3wtIiIi0gwCSuQ3b94MwJAhQ+rsO2LECMCMkLekcePGkZGRwcaNG1m5ciWRkZEMGDCAs846i4suukhJfGsV3gFGPgprHzabRQHghc3TzXSbnucGNbyAWE5ThSdlHvS+oG0uVhYREZGgCyiRT01NJS4ujqioqDr7RkVFERcXR2pqaoODa4hx48Yxbty4Fj2nNJGwaBjxIKx/DA5UmJry26tQkgd9LoZQWdvgioGCvZD2EySdFOxoREREpA0KaI58ZGQkeXl5lJSU1NnX4/GQn58flFryEsKcbhj2f5B4TOX2He/Bb69BKFVLjUiCzLWQuy3YkYiIiEgbFFAi369fP0pLS1m4cGGdfRcuXEhJSQl9+/ZtaGzSXjlccMgdkHRq5fbdn5ipNnZpcOIKlOWAiC6QMt98oiAiIiLShAJK5H/3u99h2zZTpkxh//79NfZLTU3lsccew7IsTXORhrGckHwrdD+7cnvKXNj4z9Ap7+iMBGxIXRBanyaIiIhIqxdQIn/JJZfQvXt3du7cybnnnsurr77K5s2byc3NJTc3l02bNvHKK68wfvx4du3aRdeuXbnsssuaK3Zp6ywHDLgWek+s3L7/e1j3GJQWBSeuQLm7mGo8WRuDHYmIiIi0IZZtBzZMuGXLFq699lpSUlJq3FTJtm2SkpJ45ZVXSE5ObpJAg6Uh2+U2me3vAU5VPQHYOQu2vl65rcMwswtsWN2Lr4POWwxF6dDnQlNrXkRERKSChuScAY3IAwwcOJBPPvmEq6++mk6dOmHbdqVHp06duOaaa/jkk09CPomXVqTXeTDoJqDCzWPWOlj9f+DJDlpY9eYIN49934A3ROb4i4iISKsW8Ij8wXbv3k16ejq2bZOYmEiPHj2aKrZWQSPyrUzqd/DL05UXvEb1ghEPgbtT0MKqt/ydkHg0JBwW7EhERESkFWlIzhlQHfnq9OjRo80l79KKdTkRnFGwfirYZQte83fCqrtgxMMQ2TW48dUlohukLYKonhDROdjRiIiISAgLeGqNSNB1Gg0j7i+rCFOmcJ9J5vN2BC+u+nCEQVic2fU1VCrviIiISKukRF5CU/xIMwIfFlPeVpwBqyZDzpbgxVUf4R3MvP70pcGOREREREKYEnkJXXHJMGoKhHcsbyvJgdX3mB1VW7OIJDiwAvJ3BzsSERERCVFK5CW0Rfcxyby7S3lbaQGsfQAyfg5aWHWynBDeyWxwVVIQ7GhEREQkBCmRl9AX2R0OnQqRPcvbvMWw7lGzeVRrFRYNpcWQ9qN2fRUREZGAKZGXtsGdaEbmY/qXt9klsGGaGfVurSK6QvYGyP012JGIiIhIiFEiL21HeAcY+QjEHVKh0QubnoPdnwYtrFpZFriTYN+34MkJdjQiIiISQpTIS9sSFgMjHoSOB2249Ot/zAZbrXEKizMCLAekLgDbG+xoREREJEQElMhffvnlXH755bz66qv16n/rrbdyxRVXNCgwkQZzRsCw/zM7qFa0/V347bXWmcy7O0PedsjaEOxIREREJEQEtLPrkiVLsCyLpUuXsn79eh577LFat5BdsWIF6enpjQ5SJGAOFxzyDzOtZt/X5e27PzFVbQb9yVSOaU0iuppR+chu4E4IdjQiIiLSygU8tcblcuF2u/niiy+45JJL2LdvX3PEJdJ4lhOS/wzdf1+5PeUr2PhU69tZ1eEylWxSvgZvSbCjERERkVYu4EQ+Li6OGTNm0LVrV9atW8eECRNYsWJFc8Qm0niWAwZcD70urNy+fyGsnwKlRcGJqybhHaFoP2SsDHYkIiIi0so1aLHrIYccwgcffMARRxxBWloaV1xxBR9++GFTxybSNCwL+l0G/a6s3J6xDNY+CCX5QQmrRpHdIGMxFOjTLhEREalZg6vWdOrUiTfffJMLL7yQ4uJi/u///o9HH30Ur1dVN6SV6nU+DLwJsMrbstbC6v8DT3bQwqrCcoKrI6TMMxtGiYiIiFSjUeUnw8LCeOihh7j33ntxOp28/fbbXHPNNWRnt6KkSKSi7mfAkNupdOnnboFVk6GoFS3MdsVCSS6kLw52JCIiItJKNUkd+UsvvZTXXnuN+Ph4Fi1axB//+Ee2bNnSFC8t0vS6nAjD7gbLVd6Wv9Mk8wUpwYvrYBFd4cAqyN0R7EhERESkFWqyDaHGjBnDBx98wODBg9mxYwcTJ07UyLy0Xp3GwIj7wRlZ3laYAqvugrxWkjhbDlNfft/81jePX0RERIKuSXd27dGjB++//z5nnHEGeXl5FBdrfq+0YvEjYcRDZjdYn+IMWH035PwavLgqCosCuxT2f986N7ISERGRoAloQ6ibb76Z6OjoWvtERETw9NNP8+9//5uFCxc2KjiRZhc3GEY9BqvvA0+mafNkw+p7YPh90GFoUMMDwN0FsjdDdF+ISw52NCIiItJKWLatYb7aFBUVsXbtWoYPH17rLrbNYvt7gBOcES173vaoYI9J5otSy9sc4TD0bkg4PHhx+ZQWgScD+lwErrhgRyMiIiJNrCE5Z0BTayZPnsyUKVPq3f+JJ57g7rvvDuQUIsER2R1GTYHIHuVt3mJY9wjs/zF4cfk43ebGYt83YKvEq4iIiASYyM+aNYvPP/+83v1nz57NrFmzAg5KJCgiOptkPrpfeZtdAhuegJT5wYvLJ7wT5O+CzLXBjkRERERagSZd7Fody7Lq7iTSWoTHw6hHIW5IhUYvbHoGdn8WrKjKRXSD/T9AYVqwIxEREZEga7ZE3uv1kp6eTmRkZN2dRVqTsBhTzSb+0Mrtv75i1i0Ec1mJw2Xi2zcfvJ7gxSEiIiJBV2vVmtzc3Cq14L1eL3v37qWmNbK2bZOTk8PHH39MUVERQ4YMqbafSKvmjIDh98KGJyH9p/L27e9CaT70uwqC9WlTeLyZYpOxHBLHBicGERERCbpaE/k33niD559/vlLbgQMHOOWUU+p9ggsuuKBhkYkEm8MFQ++EX56F1G/K23d9bDZoGvQnsJzBiS2yG6Qvg+je5msRERFpd2pN5G3brjTybllWjSPxFfvExMQwcOBALrjgAs4///ymiVQkGCwnDL4NwiJhzxfl7SlfQWkBDP4rOALajqHp4gpPgL1zoc+FKlEqIiLSDtWagdx6663ceuut/u+HDBlCYmIi33//fbMHJtJqWA4YcAM4o2Hn/8rb9y80yfwhd5rykC3NFQMFeyHtJ0g6qeXPLyIiIkEV0GLX8ePHc+aZZzZXLCKtl2VBv0nQ74rK7RnLYO2DZqpNMEQkQeY6yNkanPOLiIhI0AQ0J2Dq1KnNFYdIaOg1AZxRsOUloGyaWdZaWHMvDL+/5XddtRym/v2+ryHyIgiLbtnzi4iISNA0ex15kTan+5kw5HYq/fPJ2Qyr7oaijJaPxxkJ2LDvu+CWxhQREZEWVeOI/PTp0wHo2LEjl156aaW2QN1yyy0NOk6k1epyokmg1z8Odlk99/wdsOouGPmwmfLSktxdIG8rZG2E+ENa9twiIiISFJZdQxmaIUOGYFkW/fr144svvqjUVl+2bWNZFhs2bGiaaOspJSWFV155he+//95f875bt24cddRRXHfddfTq1aver1VUVMTatWsZPnw4bncLL2jc/TnkbYPwRAiLatlzS/0cWAXrHgVvYXlbeCeTzEf1bNlYvMVQlAZ9Jppa8yIiIhIyGpJz1jgiP378eCzLonPnzlXaWrP169dzxRVXkJ2dTdeuXTnuuOMAWLt2Le+//z6fffYZr776KocffniQI62HbqdB7lazoDJ/p9nR0xUfvI2IpKqOo0zSvvZBKMk1bcXpZmR++IMQO6DlYnGEg8MNKV9Dz3PBEaQa9yIiItIiahyRD1UXXXQRK1as4MILL+S+++7D5XIB4PF4uP/++/nwww8ZPHgwn376ab1eL6gj8j6215QZzFhuEnqn24z6BmszIqkqdyusuR88meVtzigYfh90GNqyseTvhMSjICEEblZFREQEaFjO2aYWuxYVFbFixQoA/vznP/uTeACXy8Vtt90GwC+//EJBQUFQYmwQywFRPaDn2dDnIogdBIX7IH+3mU4hwRfTDw6dCu7yT7AozYc195kbsJYU0c3Uli/c37LnFRERkRbVphJ5h8NBWJiZLVTdBw2+aUFRUVFERIToTpjuBOhyAvS7DBLHgifLjMCW5AU7MonsDqOmQmSP8jZvMax7BPb/2HJxOMLA1QFS5kGpbvRERETaqkYl8oWFhaSmprJnz55aHy3F5XJx1FFHAfDcc8/h8Xj8z3k8Hp5++mkAJkyY0Orn+tcpLBoSDjObFHU9zbTl74TiAypBGEwRnWHUYxDdr7zNLoENT0DK/JaLwxUHnmxIX9py5xQREZEWFfAc+ZycHF5++WXmzJnDrl276j6BZbF+/foGBxionTt3cu2117Jt2za6du3K8OHDAVizZg3Z2dlccMEF/OMf/6g07aY2rWKOfH3YXihIgQMrIG8HOFzgTtQ8+mDx5MK6hyB7Y+X2AddDjz+0TAx2qZl+1Wu8mZolIiIirVaTVq2pzv79+7n44ovZvXt3tVNXqtPSa2l79erFjBkzuPPOO1mwYAEpKSn+54YPH87o0aPrncSHFMsBUd3NoygDstabB7ZZGOtsxTchbZErBkY8COseg8xV5e2//ttMg+p9YfNXH7Kc4O4EKXOh90QIi2ze84mIiEiLCmhE/t577+V///sfcXFx/OlPf2LcuHEkJSURHh7enDEGZPny5dx6663ExMTwj3/8g8MPPxzbtlm+fDmPP/44O3bs4NZbb633JlW+u6NQZHkLcRfvIKpwIw67iFJnHF6HkrmWZNkekjLeIqaw8jV0IOYk0uPObpFSomEl6RS5upMbNUalS0VERFq5QEbkA0rkTzjhBPbv388LL7zAySef3OAAm0t2djann346BQUFfPbZZ1U2ftq+fTvnnHMOJSUlfP755/Tt27fO1wyZqTW18ZZA3nYzX7o4A5zREN5RSV1LsUvhl2ch9ZvK7V1Ph0E3Nv/0J9uGgp3Q7XSIHdi85xIREZEGafbykwcOHCA8PJwTTzyxQQE2t2+//ZaMjAxGjRpV7e6tffr0YeTIkZSUlLBkyZIgRBgkjjCzMVGfidDrPIjsAoW7TQlLuzTY0bV9lhMG3wbdzqrcnjIHNj5lbrSa9fwWuLvAvm/Bk9O85xIREZEWE1Ai36VLFxwOBw5H66xauXfvXgBiY2Nr7BMXFwdAZmZmS4TUulgWRHaD7meaevRxh5hkvmA3lBYFO7q2zXLAwBug1x8rt+9fCOunNP/774wwMez71iyMFhERkZAXUEY+btw4CgsLWb16dXPF0yhdunQBYN26dZVKT/p4PB7WrVsHQM+ePVs0tlYnvCN0ORb6XQ6Jx0BpDuTvMtVWpHlYlnm/+11RuT1jKax9CErym/f87s6mRGlWy1WREhERkeYTUCJ/00030a1bNx544AGys7ObK6YGO+GEE4iMjGTPnj1MmTKF4uLyzXCKi4t55JFH2Lt3Lx06dOD4448PYqStSFgkdBwJfSeZOdQOp0n2itJVj7659JoAA28EKqxRyFpjdoFt7qkvEV0hdaGpbCQiIiIhrcbFrkuXVr+RzN69e3nkkUcIDw/noosuYvjw4URHR9d6ktGjRzc+0nqaNWsW99xzD6WlpXTp0oVhw4YBsHbtWvbv3094eDj/+te/GDduXL1er00sdg2EbZvpNgdWQu5WM78+PNH8KU1r37fwy9NAhakuUX1M2Up3QvOdt/gAOKNMfXn9vYqIiLQKDck5a0zkhwwZUuvup7Zt12t31JbeEArM1Jo333yTZcuWsX//fgCSkpIYO3YsV111FQMH1r9yR7tL5CsqzjTTMDLXmO9Vj77ppf1kdn21Kyx4jegKIx+GiKTmO2/BLug4GhKPbL5ziIiISL01aSJ/yimnNFlgX3/9dZO9Vktr14m8T2khZG+GA8vNPG5XB3DVvKBYAnRgFax7FLyF5W3hiTDyIYhqprUcdqlZE9HxMIjpa+bPO1vPfhAiIiLtTZMm8mIoka/AWwJ5O+HAz1CYaqZnhHc01VCkcbI3wtoHza6vPq4OMOIBiBnQPOf0esynLnYR4IDIHqbOfGRXc27tMyAiItJiGpJzaoKs1J8jDGL7mRHcwn1wYDXkbgGHS/PoGytuCIx8rGzBa5Zp82TBqv+D4fdCh6FNf06HCyI6m69trzlf6rdmnYQrFmIHQVQv08fhavrzi4iISKMo85LAWZYZtY3sCsVjIXuDSeptL7g7mZrlEriYfjBqKqy5F4rSTFtpHqy5H4beDQmHNd+5LQe44swDTF37zDWQscL8fUf1NpuKRXYt7yMiIiJBFVAin5KSwgcffEBSUhIXXHBBrX3fe+890tLSuPDCC/313aUNCu8AiUdBx0Mh51fIWAZF+8vm0SvhC1hUDxj1uEnmC/aYNm8RrHsYDvm7qfnfEpxucJYttrVLzY1F3nZTMTMsFmKTIboXuPVJjIiISLAENLl51qxZPP/88+Tm1r1pUFpaGs8//zwff/xxQ2OTUOKMgPhh0Pcy6PF7833+TpMAaifRwER0hlFTILpveZtdAuufgJT5LR+P5YTweHOTEdkDHOGmPOmuWfDb67B3DmRvaf4a+CIiIlJJQIn8t99+C8Cpp55aZ9+zzz4b27ZDumKNNIDDCdF9oNf5ZuOj6F5mZLlgr1ksK/UT3tHMmY8dXKHRC5uegd3/L2hhAeYmLbIrRPY01W4K9kHKXNj6Nmx7D9KXmzZvaXDjFBERaeMC+kx89+7dOJ1OevasuyRer169CAsLY/fu3Q0OTkKYZUFkknl0GgNZvnn0pZpHX1+uGFOCct1jkLmqvP3Xf0NpPvS6IPiVZSynuekI72i+L8k3VY3Sl5Td1PU1c+sjkiCs9o3jREREJDABJfJZWVlERUXhcNQ9kO9wOIiKiiIzM7OhsUlb4YqDxLHl8+gPaB59vTkjTdWaDU9C+uLy9m1vm6S53xXBT+YrCosyDyirVb/HVDbCgvCEsko4PczmYg5nUEMVEREJdQEl8gkJCaSmppKRkUFCQu1byGdkZJCdnU1iYmKjApQ2xOmG+KEQNxgKdpuFsXk7zei8u5Pq0dfEEQ6H3AmbnjXlIX12fWRG5gfeYEbGWxvLCe4EoOx3RUk+pC+F9J/AcpmR+uh+ENGlPPkXERGRegsocxo1ahQAM2bMqLPvu+++C8DIkSMbEJa0aQ4nRPeGnudBnz9CTJ8K8+g9wY6udXKEweC/QLczK7fvnQ0bnw6N9QdhURDZzcytD+8Eudthz2z47U3Y8aGZelWoxdEiIiL1FdCI/IUXXshXX33Fiy++SJcuXWosQTlz5kxefPFFLMuqs0yltGOWZUZju55aNo9+o5kL7vWUzaOPDHaErYvlgIE3moR454fl7fu/g9ICGPoPM3ofChxh5u8YzAZUpXmQ9pNJ4h3hENsfYvqb60PrKURERKpl2bZtB3LAHXfcwWeffYZlWfTv358TTjiB7t27A7Bnzx4WLFjAb7/9hm3bnHXWWTz11FPNEnhLach2udIIpUWQu9VMu/FkQ1icqVUvle34ALb9t3JbhxEw7J7Qn6bi9Zi/e2+h+T4iCWIGQVR3s6i2Na0JEBERaSINyTkDTuQ9Hg+PPfYY7733HrZtYx30n6qvbeLEidx9992Eh4fICGENlMgHie2F/F2QsdxMu3FoHn0Ve76ALS9VbotNhuH3gys2ODE1NduGkjwoyTJfh0WYpD66T9lovf5NiohI29AiibzPli1b+OSTT1i5ciXp6enYtk1iYiKHHnoo5557LgMHDmzIy7Y6SuRbgcL9kLkGsn8pW0CZCA5XsKNqHfZ9A788A1SYVx7dB0Y8VF4Ssi3xeqA4E7zFZpfZyO6mEk5kV3DFa7ReRERCVosm8u2FEvlWxJMD2ZvgwAqwPeBKCP1pJE0h7SfY8ITZ/dUnohuMfNiMWrdVthdKcst3lA2LNEl9dB+zUZUztD8NFBGR9qUhOafmKUjocMVCpyOg3+WQdIpJ5vN3mhHa9izxKBh+Hzgq/KMv3Asr7zLTk9oqy2H2IYjqYR5hMWbB9O5P4bfXYdf/M98XZ5ppOSIiIm1MQFVr6vLdd9+xdOlSiouLOe644zjhhBOa8uVFDGe4qUUfOwjyd5t59Pk7y+bRJ7TOmurNreOhZgR+7YNmTjlAcRqsmgwjHjQVYNo6RzhEdDZf217wZJm6+7ZtbgJjB0FUL9NHU7NERKQNCGhqzRdffMFjjz3GSSedxCOPPFLpufvuu4///e9/ldomTpzIAw880CSBBoum1oSIwjTIWmtGYC1H+51Hn7sV1txnklgfZ7QZse9wSPDiCrbSIvBkmnr7lgVRvc2GVJFdtbuwiIi0Cs0+tWb+/Pmkp6dz4oknVmpfunQpM2fOxLZtRo0axZgxYwB4//33+fbbbwM5hUjDRCRC0knQ7zJIOBKKD5hpJSX5wY6sZcX0g1FTzY2MT2meSe4PrKjQ0a48p76tc7pNGcuoHubPojRI+Rq2vQtb34G0pVCQEhoba4mIiJQJaGrNunXrADjiiCMqtX/4odmc5sILL+Shhx4C4KWXXuLpp5/mgw8+4KSTTmqCUEXqwRUDnQ6H+OGQtxUyfjbTbsJi2k9Vk6geMOpxWHOvKd0J4C2CtQ9DrwmQ+yscWGkS+bA4cwPU42yT4LYHlhPC480DoLTQvB8HloEVZnYdjh4AkUltp4yniIi0SQGNyB84cAC3201CQkKl9u+//x7Lsrjiiiv8bZdeeikAq1evboIwRQLkm0ff5yLoeY5J2gp2QWEq2KXBjq75RXSGUVMgum95m10CO943m235RuNLss3i0GU3Q/qyoIQadM4IM8UmsqepdlOwD1Lmwta3Ydt7kL7ctHnbwXUjIiIhJaAR+by8PCIiKm+XvmvXLtLS0khKSmLAgAH+9tjYWOLi4sjIyGiaSEUawnJAVE/zKEqHrHWQtR6wyubRt+ESheEdYeRjZgFszi+19/UWw4YpcNhTpnxje2U5zfvmq8Ffkg8Hfob0JeBwmhuj2AHm04uw6KCGKiIiEtCIfIcOHcjLyyMzM9Pf9uOPPwJVp9uA2QU2Olr/2Ukr4e4EXU6AfpOg0xhTljB/V3mVl7bIFQPDJlOvf+peD+z8sNlDCilhURDR1UxXcneG/D2wdw5sfQu2z4SMFWbDMttb92uJiIg0sYAS+aFDhwLwxhtvAFBYWMg777yDZVkcffTRlfru37+fgoICOnfu3DSRijSVsGhIOAz6Xw5dTwW8ZfXoD7TNeuMZy6i082tt9n8PpQXNGk7IspymvGlkT4jsYa6V9KWw8wP49TWzy27utva3wFpERIImoKk1EydOZOHChbz88svMnTuXnJwcUlNT6dChA2eeeWalvosXLwZg8ODBTRetSFNyuCAuGWIHQsFeU9Ulb4epcBLeqe3Uoy9IqX9fu8RUdInq1XzxtBVhUeU7C3tLIHc7ZP0C2GZH3dhBENm9bG8D7b0nIiJNL6BEfty4cdxwww288sor/Prrr4CZbvPEE08QExNTqe+sWbMAqozUi7Q6lqN8d9CijLJ69BvMiGtE59CfR28FuO/bL89C97Mg8RhzUyN1c4SZqVtgrpvSPEj7yUy5cYabDbli+psE3xlR+2uJiIjUU0AbQvns3r2b1atXExMTw6hRo4iLq7yhisfjYcaMGdi2zdlnn12lyk0o0YZQ7VRJHmRvMqP0pYVm8WNYTN3HtUYHVsCa+wM/zhkJnY+HruMgdnD7KN3ZHLwe8GSDt9B8H5EEMYMgqru5rvS+iogIDcs5G5TItydK5Ns5r8dMmchYBsUZIVqP3gtL/2SmDzVUZE+T0CedXF7RRQJn2+YmsSTLfB0WYZL66D5lo/X6HSMi0l41JOcM8DN3kXbG4YK4gabkYMFes3FQ3jYz3cadGCLz6B0w6CZYfT+1LnoN72TmdGetBQ66vy/YBVvfgK3/NTvndh1n/nToV0hALMtUEnKVfbrj9ZhPfjLXgoV5/2MHmbr2IXfDKCIiLa3B/wsvX76cOXPmsH79en+t+ISEBIYOHcoZZ5zBYYcd1mRBigSdZZmpEFHdTXWbzHWmJj2YBLi1j6TGj4Lh98IvT4Mnq5rnR8Ahd5jksTAV9n0NKfOgKPWgjl7IWGIerg7Q5SST1Lfn2vON4XCZdRhg5tN7cmDfd+b7sEiT1Ef3MaUvnSG+VkNERJpcwFNr0tLSuPPOO/314w8+3CobQTr22GOZOnUqiYmJTRRqcGhqjdSoJB9yNkHGcjOP3tWxfKS1tfJ6TInJAyvAW2RuQpJONgnjwWyvGZ1PmQdpP5pNo2oSOwiSTjV1+kN1LUFr4y2G4iywiwCHKXkZO7BstL6DRutFRNqYZp8jn5uby4QJE9ixYwe2bXPYYYcxZswYunTpAkBqaipLly5l+fLlWJZFnz59+OCDD6pUtAklSuSlTt4SyCubR1+UDs7otreIsSQP9i80SX3Oppr7OcKh01FmlD5+pMouNhXbCyW5UJJj5ta7Ys3NU1SvsspKrmBHKCIijdTsc+Sff/55tm/fTkJCAv/6178YO3Zstf2WLl3Kbbfdxvbt23nxxRe54447AjmNSGhxhJk59DH9oTDFzKPP3Rpi8+jrEBYN3c4wj7wdsG++2QDJk1m5n7cY9i8wD3dnM0qfdIoZRZaGsxzgijMPgNIiyFxjdpa1LJPQxw6CyKTyPiIi0uYFNCJ/6qmnsmfPHp5//nlOOeWUWvt+/fXX3HTTTfTs2ZN58+Y1OtBg0Yi8NEjxAchcX75wNDyx9c+jD5S3BA78DCnzIWMp2KU19+0wwozSqzZ907NLzdz6kjyzYDYsFmKTIbqXuZHUgmQRkZDQ7FNrRo4cicPhYOXKlXX29U298Xq9rF69ur6naHWUyEujlBRAzmaT8JYUmMWkrthgR9X0ijMh9Vsz9SZ/R839nFFltelPVW365lJaaP4+KDGbgUX3hugBZaP1bfDaExFpI5p9ak1CQgI5OTn16mtZFg6Hg/j4+EBOIdK2hEVCx5HQYaiZkpKxDPJ3moQ2PKHtJLLh8dBzPPQ4F3K3QMpcSF0ApfmV+5XmQ8oc84jqVTb1RrXpm5Qzonwqk10KBfsgZyvmk6GOZrQ+qkfZaH0bmPYlItKOBZTIH3vssXz00UesWLGizvKSK1asID8/n7POOqtRAYq0CY4wiO0PMf2gcF+FefRhZtpNW5n+YFlmrnbsIOh/DaT/ZEbpM1dV7Zu/s7w2facjIUm16Zuc5TTJu+9GqSTffDqUvsQk8dF9zfqOiCSzDkJEREJKQFNr9u7dy/nnn09cXBz/+c9/6NWrV7X9du3axbXXXktOTg4ffvghXbuG7kI3Ta2RZlOcCVnrzaJF2wZ3JzOa2hYV7iurTT+/mtr0Ffhr0//OTAmR5mOXgicbSvMAy3xCFDsIonqaa1EVh0REWlSzz5FfunQp27Zt44knnsDj8XDmmWcyZswYkpKSAFN+csmSJXz55Ze4XC7+8Y9/0KdP9RvFjB49ur6nDSol8tLsSgshezMcWG4WLLbVefRgyihmroF98yBtUR216ZPLatMfr9r0LaEkv2yzsFKwXGakProfRHSBsKhgRyci0uY1eyI/ZMgQ/4ZPtm37vz5Ybc+BmT+/fv36+p42qJTIS4vxlkDeTjP1oTC1bB59x7Y7MlqSB6kLTVJfZ236o8tq049ou+9Ha+ItMUl9aSFgm2Q+dhBEdgd3gv4ORESaQbMvdu3evXuDAhORenCEQWw/iOlrEvkDq8zCUYerbc2j9wmLhu5nmEfeDpPQ7/umbFS4Am8x7P/OPNxdTF36rqeaed3SPBxhZnoNmGlfpXmQ9pP5RMUZbvZMiOlvEvy2Oh1MRCQEBDQi39otXryYyy+/vF59v/nmm3rdmGhEXoKqOAuyN8CB1WZOszuxbSdO3hJT2WfffEhfCnhr7hs/0iyQTTxatelbktdj5tZ7C833EUkQMwiiure9HY1FRFpQs4/It3aJiYmcd955NT6/evVqfv31V3r37k23bt1aMDKRBgrvAIlHQcdDIedXk+QW7TeLQtviDp6OMPPzJh5lNtXy16bfWbVv5mrz2BIFXU4w8+ljk5VINjeHq/JofUkepH1vvg6LMEl9dJ+y0XrdYImINKc2NSJfl9///vds2bKFv/71r9x44431OkYj8tKqeEuhYBekLzOVYJyRZfXo2/CcZds2m2rtm1d9bfqKonqZUfqkk1SbPhi8HlONyVtsdpl1RppNqbDKpoZZZbXrHeaatRymRCZlf1rO8jbLYdodTsAJDkfZn2X9sMr61PanVfbaFf+0ys+NVXf/tvxvS0RalXY/Il+bFStWsGXLFpxOZ62j9iKtmsNpRjujepuR+czVpuKN5Szb4McV7AibnmVBXLJ59L/GVLvZN8/87AfL3wlbX4etb6o2fTA4XBDR2Xxt22CXALb5mrJHqadqm23jn0ZVpd02c/PNk+Xtlg22ZXJxG/NnJfX9ZMau/GXF17PtspdxliX1FW4wfDcTVW5QaroZcZT39d20VHquQptV4eaivjcqNfWptb+IhLoa/3fzzTXv0aMHU6ZMqdQWCMuyePPNNxsYXtP58MMPATj++OP95TJFQpZlmakLXcdBpzGQ5ZtHX1JWjz4y2BE2D6fbjLYnnQQFKaY2/b6vq6lN7zWbHqUvMdOQkk42Sb1q07ccyzJlLNuCSjcaFW4qKt5olHpNH7vsuSo3KNUd7/u6wnMWDb9BqXIDcnB7hYbqblAOvrmo9gYlrOpNjf8GJeygmxjfJyi+m5+6bjSq+dOqpY8Vppt0afdq/BewZMkSAPr371+lLRC1laFsKQUFBXzxxRcA/PGPfwxyNCJNzBUHiWPL59Ef+BmK0truPHqfyK7Q9xLoc1Httek9WbDrY/OITTYJfZfjtZOp1J9lAc6yr51BDaVJ2V4qfzpC5ZuLSjcoB/c5qH+l4yp8quJ7riluUPzfVBDe0Sy4jugK4fHmd54zEq2VkfaixkTeNwofGxtbpS3UzJ49m7y8PDp16sRJJ50U7HBEmofTDfFDIW4wFOw2C2Pzd4Ijom3v1Gk5oOMo8yjJhdTva65Nn7PJPH77j6l2k6Ta9NKO+a77UM15bdtUT8rdBtkb8d8lOMLLkvtuENHJJPdhsRq9lzapxqu6unnkoTq33Det5txzz8XlaiMf9YrUxOE0U0iie5t69JlrIHtT255H7xMWU//a9KnfmYdq04uEJssyo+8HTyX0LbrO32PK9oLJ8cMTKozel31iqV2LJcQ1W9WakpISVqxYAcDo0aOb4xT1sn37dk477TQAvvjiCwYMGBDQ8b4VxCKhzOHNI6JoG5FFm7DsEkqcHbAdbbgefUV2KdGFG4jNX0x04QasWmrT57sHkR01hrzIEdhWeAsGKSLNyrax7CIcdiGWt7jsQwgLrxVGSVgnisMSKXXG43VEUeqIbltTqCTktIqqNTk5OUyaNAmHw8H69eub6zR18o3GH3bYYQEn8RWp/KSEvhOgtAhyt5ppN57stj+P3m8wMN7Upt/3rRmpr6Y2fVTRZqKKNkN2tJlHnzQOYgdpvq1IW+X1QGkBlOQDOeVT8N0JENnNjOD7fk+GtdEiAtJqNGTwuNknjAWzTH1paSkff/wxABMmTAhaHCKthtMNHYaYUo75u8zuqfk7zZQUV3zbT1jDO0Kv86DneDNXPmUe7F9YtTZ9aR7snW0eUb1MdaAuJ6k2vUhb43CZR8UBDdsLpYWmeEDmuvJKQGGR5VNz3BXn3mv0XoKnTa/8+P7779m3bx9RUVGcddZZwQ5HpPWwHGYOfVQvKEwxG0zl72wfG0xBWW36weYx4FpI+xH2za+5Nv1vr8PW/5qa9F3HQccjtHBOpK2yHGbu/MHz570eKEyHvJ2UlxoFIhJNch/RxYzeh3cAZzuZuihB16b/J/rggw8AOPPMM4mOVqk5kSosy3x83PNsKNwPGSsgd7P5Tyi8U/uYJ+p0l9WZP7lCbfr5ZsOtiuxSSF9sHq4OZoFs0qmqTS/SXjhcJkmnQ3mbf/R+M2SuNb9TK47eR3YzgyOuOHDFtv1BEmlxbTaRz8jI4JtvvgFUO16kXiI6Q/fToHg0HFhlNplyhEF4YvsZfa5Um341pMyH9Jpq088yD9WmF2m/ah29TzPVs/wL7C1TOSyya9nc+7iyuvcavZeGa7P/O3/66ad4PB769+/P4YcfHuxwREJHeEezc2rC4WZ+qG+6ibtz2y5dWZHlMBtsdTy0rDb9wrLa9Jur9q1Um/4YM0qv2vQi7Vtto/dZv5iywL5dgF3RFcpiJpjjwmL0O0Tqpc0m8r5qNVrkKtJArjjofLTZaClrAxxYAd4SM6LkbEcVnMJioPuZ5pG33SyQTf22htr035qHu4upS590qpk3KyJS4+h9MRSkmo2t/NvY+kbvyxJ8/+h9O/rdK/XSbHXkDxw4wNFHH41lWWzYsKE5TtEifKWAVH5S2r3SQsjeDBlLTRlLd6eqG7G0F14PZPxskvqMZVBjbXoL4keaBbKdjtJ/wiJSP7a3vCym7ZvaZ5uBBd+ute6OZZVzNHrfVjQk56x1RP7yyy9vcDAlJSUNPlZEWiFnBHQcYcpX5v4G6UvMgtDwBPMfSXvicEHiUeZRfMDsHrtvfjW16W3IXGUezrLa9F1/BzED236pTxFpOMth1twcvO7GW2wW5eduM8m+r29EoknuI7po9L6dqTWRX7JkCZZlBbUWvIi0Mg6XKdsYMxDytpXXom83m0sdJLwj9Dofep4HOb+YBbL7F5jRtIoq1abvXaE2fXwwohaRUOQIh/CDdp22S83vm6wNplCBZVHz6H20Ru/bmFoT+fHjx2Np1EhEquNwQuwAiOkH+bvNCH3eDlNirT1sLnUwy4K4Iebhq02fMg+y1lTtm78DfnsNtr4JCaPNfHrVpheRhrCcJmk/+JPR0iIo2Gt28/ZP/3OaCmURXSGyC4T5Ru/DD35VCRHNNke+rdAceZF6su32ublUXWqrTV+RK76snr1q04tIM7FLoaTA7GbtLS6ve++KK0vuu5pPCf1z79vZgEyQNSTnVCJfByXyIg3g31xqi5mn2V42l6qN7S2rTT8P0haB7am5b+xgM0rfWbXpRaQFlBaZ5L60AP+WtVZY+eh9ROfy6ZPtpQxxEDT5YlcRkQbR5lJVVaxN78k18+hT5pmbnYPl/GIev5bVpu86DjoM16cbItI8nO6yxbEdy9vsUvDkQ+HastF7R9nofazZsTayq/kk0T/3XqP3waAR+TpoRF6kCXiyzfblmWsBu31tLlWXvG1mgWx1tekrikgy026STlFtehEJDtsGb5EZuS/NL2+3wso3tYpILK+co9/zAdHUmmagRF6kCZXkm9H5jOVmtKe9bS5VG6/H1KRPma/a9CISWuxS8/u9NB/sEsymVraZjhPZ1ST54WWVc5xRGr2vgabWiEjrFhYFnY6A+GGQvckkrEXtfHMpH4cLEo82j6IMM0KfMg8Kdh3U8eDa9CeYpF616UUkWCxnWcWy2PI23+h97g7z+97X5nCZTxUju5nBHP/ovVLShtCIfB00Ii/SjLye8s2lPDlmxKa9bS5VG9suq00/D/YvrFqbvqKoPhVq03dosRBFRALiLSnbtTYP8I3eY37/+6bnhHdol6P3mlrTDJTIi7QAb2nZ5lJLzE6p7XVzqdqUFppqNzXVpvexnGW16cdBwhGqFiQirZ9tg7fQTM/xFprCORZmAyx32eh9RCKExbbp0XtNrRGR0FTd5lL5O83ofHvcXKo6zoiyOvMnl9Wmn19Wmz6tcj+7FNJ/Mo/wjmaEvus4iOoVlLBFROpkWWZ65cFTLL0lpghAwV6gtDzBD0+oOnofFhWEwINPI/J10Ii8SBBoc6n6sUsr1Kb/qR616ceV1aZvn//hiUgbUHH0vrTQPzMHR7hJ7iO7mXVXrjizc60jdD6V1Ii8iLQNlmV+Gfc8W5tL1cZyQsfDzKPetelfgcRjy2rTD9PNkYiElhpH7z1QnAn5e8wgh69veMfyXWtdcWbqZljbKa6gEfk6aERepJUoPlB5cyl3ZyX0NcndaqbdpH5ravjXxF+b/lSziZeISFtie82ofWlB5dF7Z0T53Hv/6H1s0Efvtdi1GSiRF2lltLlU/Xk9kLG0rDb9z9Rem34UdD1VtelFpO3zesrr3uM1c+/BJPWR3Ux5zIjOZjS/BWlqjYi0fa446HwMdDxUm0vVxeGCxGPMoygDUr8pq02/+6CONmSuNI+waOis2vQi0oY5XGVleiuU6vWN3udsNp/+RiZB7z8GLcT6UiIvIqFJm0sFxp0AvSZAz/Nrr01fkgd7vzQP1aYXkfbCcpj/V8KiIKywbIfa1k+JvIiENmcEdBwJHQ4p31yqKE2bS9XEsiBuiHkMuBbSfiyrTb+2at/87fDbq7D1jbLa9L+DhMPrsTbBNmXjHGGUT0oVEZGmpkReRNoGhwviBpvpIL7NpfJ3anOp2jgjIOkU8yjYW1ab/ut61KY/uaw2fc/K/XI2w+7PzMZV3iJwuCHxaOhxNsQOarmfS0SkndBi1zposatIiLK95ZtLFe7T5lL1ZZfCgdWwrx616eOGmIo3nY+HlLlm9L4m/a+Bnuc2fbwiIk2ttBAohT4XtehptdhVRMTHckB0LzNqXGVzqU5K6GtiOSHhMPOoqzZ99kbz2PJy3fNJf3vVVBjqfEzzxC3tgNfsKbHvGyhON58oxR9qqi1pGp20U0rkRaRt0+ZSDeeKge5nmYevNv2+b6Akp3K/+i4K2zEDOh+N5s1LwIpSYe0jZtpcRRk/w7a3IflW6HJCUEITCSYl8iLSfkR0hu6nQfFobS4VqJh+EHMt9LuirDb9PFP6s8ba9NXI2w6LLjdz5y2Hec/9fzrLv+fgNl+/erYTYH/LWXbOamKpKUb/MdX1P+hYaRxPNqy8B4r2Vf+8twg2TgNnuNkHQaQdUSIvIu1PeEdIOslUYNHmUoGpVJs+HX57zZSxrC9PVvPF1lrVeRNQx9c13jDUo70hNzUNuZmqsX8T3NTsmlVzEl/R5n+b6kq6KZd2RIm8iLRf/s2lRkHWRm0uFSh3J+h8bGCJfHtkl5pHu2fVcsNR082JBbnb6vfyxWmw9yvocqJZC6N1MNIOKJEXEQmL1uZSDRU7GHBQ7yk2HUaYBM2f3HrLv7a9B/1Zy9e+4ySE2GY9RXPWytvyonk43OaTN/8jocKf8eXfu+I0/UlCmhJ5ERGfiptL5fwKGdpcqk7uTpA41tSOr0vi0TB0ctOd27YxCX11NwAVvqeG9pr6V2kvPeg8dfQngBuS6m5OAjmu4s8m5bxFplpVYUrt/SynKUtbbdLfsXLyr2l30gopkRcROZjDBR2GmE2MtLlU3fpdadYZHFzNpqKwWNOvKVkWZg64E2jnSVZL3dRUe7NRQ/9qP22p6esazuMtKatU00zD+HapKWVZnF5337DYqiP61SX+YVHNE6tINZTIi4jUxOGE2AGmYotvc6n8ndpc6mCR3WDUFFg/FQp2VfN8Txh6l+knzaMt39Rses5sOFYXRzj0+iN4csBzAIoyoPiAeXgLGx9HSY555G+vI46IWkb4K9wIuGI1rUcaTYm8iEhdtLlU3aJ7w+jpkLES0n40CU9YrKluk3AoZh69SAP0mgD7v4fSgtr79b0Meo6v/rmS/PKkvvgAFGcc9H1ZW22fKtWXtxAK95pHbSwnuDqCu6P5s2Ky7y770zftx6F0TaqnK0NEpL60uVQdHKakZ8LhwQ5E2pLI7jD8flj3MJTkVd+n1wToeW7NrxEWZR5RPWo/l9cDxZnVJPrVJP6B7KFQHbvUVNopTqu7ryuuPMl3xYM7ofppPVqc3+4okRcRaQjf5lJFR0Lmam0uJdKcOgyF0S+Z8pL7vjFz2p0R0PFQ6P4Hs56lKThc5t92ROfa+9les1GVP8nPrJDsH5T0e4saH5cn2zzy6pjW44w8aFpPdVN7OppPy/RJYpugRF5EpDHcCQdtLrWmrF2bS4k0KVcH6H2BeQSb5Sib6x4P9Ku5n22bKUGVkvzMg5L9srammNZTWgAFBVCwp474w6pZtFtNqU5XB03raeX0tyMi0hS0uZSIHMyyKkzr6Vl7X6+n9vn7/q8zafy0nhJTWreormk9VoVpPdXU4a94I+CMaFxM0iBK5EVEmpI2lxKRhnC4IKKLedTGLq0wrae6xL9itZ7iRgZlgyfLPPK21d7VGVlzhZ5K03piQmBaj11W0rX1UyIvItIcatxcKsEk+yIiDWE5y5Pl2tg2lOYflNzXMLWnpkXEgSgtgILd5lFr/GF1bL7l+zq+ZdcbleTC3jmQMq/sZ3DAlpdg4I3Q+0JTjrgVUiIvItKctLmUiASDZZlBg7Douqf1lBaBJ7Ny0l9U9rUns0Lin0XTTOvZbx61/wBl03pq2XzL931jpy/m74TV9x20MZgXUheYx2+vwwmzWuUgjBJ5EZGWUGlzqV2QvlSbS4lI6+B0gzMJIpJq72eXmmTeUzaaX3TQyL4ns7zN9jQyqIrTerbWEX9U7Ztv+b4Pi676u7YkF1bfX/vuvilzYdGVcPz/GvkzNT0l8iIiLclymM2TonpBwV7I+Llsc6ko8x+NEnoRaa0sp6nU5U6ovZ9tm+k69anHX9oU03ryoSC/HtN6XFVH9Av31q+W/84PTFWy+BGNj7cJKZEXEQkGy4Ko7uZRZXOpRG3dLiKhy7LAFWMe0b1r71taVL96/J4soJELUG0PFKWaR0P8+hoc8a/GxdDE2mwiX1hYyFtvvcXs2bPZvn07Ho+HTp06MXz4cK644gqOOOKIYIcoImJocykRaa+cbojsah61sUvLEv2aRvgrfG2XNE+sOZua53UboU0m8jt37uSaa65h+/btdOrUidGjRxMeHs7u3bv5+uuvGTJkiBJ5EWl9tLmUiEj1LKcp4+vuVHs/2zbz3usqzVl8wEzJCYQjvOHxN5M2l8jn5+dz9dVXs2PHDm666SZuuukmXK7y/wAPHDhAZmZm8AIUEamLNpcSEWkYywJXrHnUOa2n0FSrydlYv9dOPKbx8TWxNjcJ88UXX2THjh2MHz+e2267rVISD9CxY0f69atlO2URkdbCt7lU/0kmsS/JNhVvSguCHZmISOhzRkDP8fXr63BD/6uaNZyGaFOJfHFxMTNnzgTg+uuvD3I0IiJNxLe5VL9JkHQyeAtNpZum2MRFRKQ963wUJBxZd79Dp0JEYvPHE6A2NbVm3bp1ZGZm0q1bNwYMGMDy5cv59ttvyczMJDExkeOPP57DDjss2GGKiDSMNpcSEWliDhh6J2yaDqnfVfO0Gw59HIbc1vKh1UObSuQ3bTKrifv06cNdd93FrFmzKj3//PPPc/rpp/PEE08QERERjBBFRBqvyuZSS7S5lIhIQzncMORv0Hui2fwpfydgQZ+LoP8VdS+wDaI2lchnZWUBsGzZMkpLS7n66qu5+OKLiY+PZ+nSpTz44IPMmTOH6OhopkyZEtBrr127tjlCFhFpPLsvrtIYIgs34Pasp9ThptTRQQm9iEjAjsdyF2Hh5UD+ibB2G7AtyDHVrE0l8l6vF4CSkhIuuOAC7rzzTv9zp556Kl26dOGCCy7g448/5qabbqJXr171fu3hw4fjdqtahIi0ZmdocykRkcYqLQRKoU/LliovKioKeOC4Tf12j46O9n994YUXVnl+xIgRDBs2DK/Xy+LFi1syNBGRluHbXKrPRRAzwGw/XphiyleKiEib0qYS+R49evi/7tmzZ7V9fO1paWktEpOISFD4Npfqewl0GAaF+6BgD3g9wY5MRESaSJtK5IcNG+b/+sCBA9X28bVHRUW1SEwiIkHl21yq32WmxFpROuTvhtKiYEcmIiKN1KYS+aSkJEaNGgXATz/9VOX5rKws1q9fD5g57yIi7YY2lxIRaXPaVCIPcOONNwKm1OSGDRv87UVFRTzwwAPk5OQwbNgw1ZMXkfbp4M2lSgu0uZSISIhqU1VrAE455RSuvvpqXnvtNS644AJGjRpFfHw8q1evJjU1laSkJJ566ikslWUTkfZMm0uJiIS8NpfIA9x5550cfvjhvPXWW2zYsIGCggK6d+/OVVddxfXXX09CQkKwQxQRaR20uZSISMhqk4k8wO9+9zt+97vfBTsMEZHQYDkgujdE9YKCvZDxs0nonVEQnqCEXkSkFWqzibyIiDSAZUFUd/OotLlUBIR30uZSIiKtiBJ5ERGpnm9zqaIjIXM1ZG8EywnuzuZPEREJKiXyIiJSO9/mUgmHQ+ZayFxT1t7ZLJoVEZGgUCIvIiL149tcquMoyNoIGcvBLgV3IjjdwY5ORKTdUSIvIiKB8W0uFT8MsjdBxjIoKgJ3J3BGBjs6EZF2Q4m8iIg0jG9zqQ6HQM6vkL4YitJMlZuw6GBHJyLS5imRFxGRxqlpcynLVVa20vcoY1X83jqotKV1UFvFfhWer+51am2r7rVFREKbEnkREWkalTaX2g2FaYANeMEue+A1bbbvT9/zdvnzdoVj/H0Oet73nO/1vd7K5/I9Zx90Dv/5oTzRL+vuy+8rfl3JQY019quNXXuzRVmcvtgqxFrdDQ9UvVmq9ibIquG4Cm0H3yg16LWri1VEmosSeRERaVqWA6J7mUdrZVdI9P03BFT+vsrX1PJcfY6v8Bo19asptoo3Nf4bmoNveqh6s+T/nkbeLJU24mbJNol9oDdLDbpR8h1YS7N10NcVY63uhgdquKE56MaqSW+W9KmS1I8SeRERaX+siomVNLn63Nw05Ll63RDV9lwdN0v+m6MKNywVb3oq3ihB5bYqN0ocdFyF530x+G+IKt4oVTjOH5dd9XUr3YCUHWpVuEtp0M2SXbXPwWy7+qf9h1oVYqNhnypVaqvl2Gabghc6lMiLiIhI06qYKIVWXhQ6Arq5qc8nRrUd38DnqnySddANke0te4kabpaa9FOlAKfgRfdt4F9My1IiLyIiIhJq9KmSAI5gByAiIiIiIoFTIi8iIiIiEoKUyIuIiIiIhCAl8iIiIiIiIUiJvIiIiIhICFIiLyIiIiISgpTIi4iIiIiEICXyIiIiIiIhSIm8iIiIiEgIUiIvIiIiIhKClMiLiIiIiIQgJfIiIiIiIiFIibyIiIiISAhSIi8iIiIiEoLCgh1Aa2fbNgDFxcVBjkRERERE2ipfrunLPetDiXwdPB4PAJs2bQpyJCIiIiLS1nk8HiIiIurV17IDSfvbIa/XS15eHi6XC8uygh2OiIiIiLRBtm3j8XiIjo7G4ajf7Hcl8iIiIiIiIUiLXUVEREREQpASeRERERGREKREXkREREQkBCmRFxEREREJQUrkRURERERCkBJ5EREREZEQpEReRERERCQEaWfXBvB4PCxbtozvvvuO5cuXs2fPHjIzM+nYsSOHHXYYl156KWPHjq3x+M8++4wZM2bwyy+/4PV66devHxMmTODiiy+udQOAlj5Ogueuu+5i1qxZNT7fr18/Zs+eXe1zur4E4LfffmPhwoWsWbOGtWvXsm3bNmzb5plnnuGMM86o9dhQuYZ07QVPQ66vxvxeA11f7UV7ybGa6vrShlAN8OOPP3LVVVcB0LlzZ4YNG0ZkZCS//vormzZtAuCmm27itttuq3Lsgw8+yLvvvovb7eboo48mLCyMRYsWkZeXx+9+9zueeeYZnE5n0I+T4PL9h3f44YfTp0+fKs937tyZv/3tb1XadX2Jz6OPPsp///vfKu11JfKhcg3p2guuhlxfDf29Brq+2pP2kGM16fVlS8B+/PFH+9Zbb7WXLl1a5bnPP//cPuSQQ+zk5GR70aJFlZ6bPXu2nZycbB977LH21q1b/e379++3zzzzTDs5Odl+4403qrxmSx8nwXfnnXfaycnJ9ocffljvY3R9SUUzZ860H3/8cfvzzz+3t2/fbl922WV2cnKy/eWXX9Z4TKhcQ7r2gq8h11dDfq/Ztq6v9qat51hNfX0pkW8Gd999t52cnGxPnjy5Uvt5551nJycn27NmzapyzOLFi/1/saWlpUE9ToKvIf/h6fqS2tQn0QqVa0jXXuvTnIm8ri+pKNRzrKa+vjTJqxkMHToUgH379vnbUlJSWLduHS6Xq9qPHceMGUNSUhL79+9n5cqVQTtOQpOuL2msULmGdO21L7q+5GChnGM1x/WlRL4ZbNu2DTBzu3zWr18PwKBBg4iIiKj2uBEjRgCwYcOGoB0nrcvixYuZMmUK9957L08//TQLFy7E6/VW6afrSxorVK4hXXuhr76/10DXl1QVyjlWc1xfqlrTxPbv3+9flX/aaaf523ft2gVA9+7dazy2W7dulfoG4zhpXT7++OMqbQMHDuSpp55i8ODB/jZdX9JYoXIN6doLffX9vQa6vqSyUM+xmuP60oh8EyopKeGOO+4gJyeHo48+mlNOOcX/XH5+PgCRkZE1Hh8dHQ1AXl5e0I6T1mHIkCH83//9H59//jkrVqxg4cKFvPzyywwZMoQtW7Zw1VVXVfpYUdeXNFaoXEO69kJXoL/XQNeXlGsLOVZzXF8akW9C999/P4sWLaJbt248+eSTlZ6zy6p8WpYV0Gu29HHSOlx55ZWVvo+KiqJLly4cc8wxTJo0iZUrV/Lyyy9z3333Abq+pPFC5RrStRe6Av29Brq+pFxbyLGa4/rSiHwTeeSRR/jggw/o3Lkzb7zxRqW5W1B+h+W7G6uO7+7L1zcYx0nrFh4ezvXXXw/Ad99952/X9SWNFSrXkK69tqem32ug60uMtpJjNcf1pUS+CUydOpW33nqLhIQE3njjDfr27VulT48ePQDYs2dPja+TkpJSqW8wjpPWr3///kDlFfu6vqSxQuUa0rXXNlX3ew10fUnbyrGa4/pSIt9ITzzxBK+//jrx8fG8/vrrDBw4sNp+vnJJmzdvprCwsNo+a9asAeCQQw4J2nHS+mVmZgKV79Z1fUljhco1pGuvbaru9xro+mrv2lqO1RzXlxL5Rpg2bRqvvvoqHTp04PXXX2fIkCE19u3WrRvDhg3D4/Ewe/bsKs8vWbKElJQUOnfuzGGHHRa046T1+/LLLwEYPny4v03XlzRWqFxDuvbapup+r4Gur/asLeZYzXJ91WvbKKniX//6l52cnGwfeeSR9po1a+p1zJdffunfsWvbtm3+9rS0NPuss86qcVvelj5Ogmv9+vX2119/bZeUlFRq93g89muvvWYPGTLETk5OthcsWFDpeV1fUpv67LwZKteQrr3Wp67rq6G/12xb11d71JZzrKa+vizbLltCK/U2f/58brrpJsCMHgwaNKjafv379/cv4PF54IEHmDFjBm63m2OOOYawsDAWLVpEbm4u48aN49lnn8XpdFZ5rZY+ToJn3rx53HzzzcTHx9O3b1+SkpLIy8tj06ZNpKam4nA4uP3227nuuuuqHKvrS3zWrVvHgw8+6P9+y5Yt5OXl0bdvXzp06OBvnzlzZqXjQuUa0rUXXIFeX435vQa6vtqT9pBjNeX1pUS+AT766CMmT55cZ78xY8bw1ltvVWn/7LPPeOedd9i0aRNer5f+/fszYcIELr74YhyOmmc7tfRxEhw7d+7kv//9L2vWrGH37t1kZmZiWRZdu3bliCOO4NJLL63y8XNFur4EzO6Zl19+eZ39fvnllyptoXIN6doLnkCvr8b+XgNdX+1Fe8mxmur6UiIvIiIiIhKCdEspIiIiIhKClMiLiIiIiIQgJfIiIiIiIiFIibyIiIiISAhSIi8iIiIiEoKUyIuIiIiIhCAl8iIiIiIiIUiJvIhILZ577jkGDx7MXXfdFexQgmL16tXceOONjB07liFDhjB48GCee+65YIclIiIokReRRrrrrrsYPHgwgwcP5vzzz6e2Peb+/ve/t+ukONRs27aNyy+/nG+++Ybs7Gw6duxIYmIiUVFRwQ5NqrF48WKee+455s2bF+xQRKSFKJEXkSazbt065s6dG+wwpIm8//77FBQUcOSRR7J48WIWLVrEDz/8wDXXXBPs0KQaS5YsYfr06UrkRdoRJfIi0qSeffZZvF5vsMOQJrBlyxYAzjzzTOLi4oIcjYiIHEyJvIg0iTFjxhAZGcnmzZv57LPPgh2ONIHCwkIATaUREWmllMiLSJNITEzk0ksvBWD69OmUlJQEdLxvnv2uXbuqfX7Xrl3+PgebNGkSgwcP5qOPPiI3N5cnnniCcePGMXLkSE499VSeeeYZioqK/P0XLVrENddcw9ixYzn00EO59NJLWbZsWZ0xer1e3njjDc455xwOPfRQxo4dy4033sjq1avrPO7jjz/mqquu4qijjmL48OEcd9xx/OUvf2HVqlXVHlNxka3X6+Xtt9/mj3/8I0ceeSSDBw9mw4YNdcZb8fz/+9//uOyyyxgzZgwjRozglFNO4d5772X79u1V+p9yyikMHjyYJUuWADB58mT/e3/KKafU65yLFy+u1P/rr79m0qRJjB49msMOO4yJEyfWesOXmprKu+++y/XXX89pp53GqFGjOPzwwxk/fjzPPvss2dnZ9Trvd999x7XXXsvRRx/NkCFDeOONN/x9V61axT//+U8uvPBCjj/+eIYPH87RRx/NNddcw+zZs2uMzbcu5LnnnqO4uJgXXniBM888k1GjRnHSSSfxyCOPkJWV5e+/du1abrnlFo499lhGjhzJhAkT6pz+UlxczNtvv80ll1zCmDFjGD58OCeffDKTJ0/m119/rdTX929j+vTpAMyaNcv/91Xbv6uvv/6aP/3pTxx77LH+n/3GG29k4cKF1cb00UcfMXjwYCZNmgTAp59+ymWXXcbYsWMZPHhwpZ9pyZIl/PnPf+aEE05g+PDhHHHEEZx22mncdNNNvPfee/rUTqSJhAU7ABFpO6677jree+89duzYwUcffcSFF17YoufPzs7mggsu4LfffiMqKgqv18uuXbt44YUX2LBhAy+99BLvvPMODz/8MJZlERUVRUFBAcuWLePKK6/kzTff5Igjjqj2tW3b5rbbbuOrr74iLCyMyMhIMjMz+eabb1iwYAHTpk3jrLPOqnJcbm4ut956Kz/++CMAlmURHR3N/v37+fLLL5kzZw733HMPl112WY3nveWWW5g/fz5Op5Po6OiA3pOCggJuueUWvv/+ewBcLhcRERHs3r2bmTNn8sknn/DUU08xbtw4/zEdO3akqKiIrKwsPB4PMTExRERE+J8L1Jtvvsljjz2GZVnExsZSWFjIypUr/Y977723yjGPPPIIc+bM8X8fFxdHbm4uGzZsYMOGDXz22We89dZbdO3atcbzvvbaazz++OP+8zoc5WNXeXl5la5Pl8tFeHg4GRkZfP/993z//fdMnDiRhx56qMbX93g8XHXVVSxbtgy32w3A3r17eeutt1ixYgXvvvsuCxcu5K9//av/fSwqKvIn9k899VS110xqairXXXcdGzduBMDhcBAZGcmePXv46KOP+Pzzz5k2bRqnnXYaAE6nk8TERPLz88nPz8ftdhMbG1vpNZ1OZ6W4J0+eXOlGKiYmhoyMDL755hu++eYbrrnmGv7xj3/U+LM/8sgjvPXWWzgcjirv7fvvv899993n/z4yMhKv18v27dvZvn078+fP57zzzvO/ZyLSCLaISCPceeeddnJysv2Xv/zFtm3bfvbZZ+3k5GT7xBNPtIuKiir1/dvf/mYnJyfbd955Z5XXSU5OtpOTk+2dO3dWe56dO3f6+xzssssus5OTk+0jjjjCPv300+2lS5fatm3bRUVF9syZM+2hQ4faycnJ9vTp0+1hw4bZ//znP+2srCzbtm17165d9sSJE+3k5GR7woQJVV7b9/McccQR9iGHHGK//vrrdkFBgW3btr19+3b7qquuspOTk+2RI0fa27dvr3L8TTfdZCcnJ9tnn322/e233/qPzcrKsl966SV72LBh9pAhQ+xly5ZVe95DDz3UHj58uP3OO+/Y+fn5tm3bdlpamp2Tk1Pt+3Swe++9105OTraHDx9uz5gxw/938ttvv/nft1GjRtm//fZbje/rhx9+WK9zVfTTTz/5X3vYsGH2P/7xD3v//v22bdt2ZmamPXXqVP/f56efflrl+GnTptkvvPCCvXnzZruwsNC2bdsuLi62Fy9ebE+YMMFOTk62r7vuuhrPO2LECPuQQw6xH3jgAf95CwsL7b1799q2bdv5+fn2ddddZ/+///f/7JSUFLu0tNS2bfP38tZbb9mHHnqonZycbH/xxRdVzuG75o844gj72GOPtb/55hu7tLTULikpsefOnWsfdthhdnJysj1t2jT7iCOOsCdPnmynpqbatm3b6enp9p/+9Cc7OTnZPvbYY22Px1PptYuLi/0/36WXXmovXbrU/3e2f/9+//s2atSoKteb75qp7t9XRY8++qidnJxsn3zyyfZnn31m5+bm2rZt27m5ufZ7771nH3744XZycrL92WefVTruww8/9F+TgwcPtp977jn/v6OcnBw7LS3Nzs/P9793kydPtvfs2eM//sCBA/Z3331n33777VV+N4hIwyiRF5FGOTiRz8nJsceMGWMnJyfbb7zxRqW+zZ3IDx061N62bVuV5ydPnuw/9q677qry/K5du+zBgwfbycnJ9u7duys950uOkpOT7RdeeKHKsYWFhfbpp59uJycn23fffXel53744Qd/wnTgwIFqf65///vfdnJysn399dfXeN733nuv2mPrsmvXLnvIkCF2cnKyPWPGjCrP5+fn2+PGjbOTk5PtO+64o8rzTZHIJycn21dddZXt9Xqr9PFdO7/73e+qfb4mBw4csI866ig7OTnZ3rFjR43nvf322wOO22fWrFl2cnKyfdlll9UYd3Jysr148eIqz0+fPt3//KRJk6o8n5eX50/2lyxZUum5mTNn+m8qa0p277//fjs5Odl+8MEHK7XXJ5HfunWrPWTIEPvII4+s8t75fP7553ZycrL9+9//vlK7L5FPTk62//nPf1Z77KpVq/zJfklJSY1xiEjT0Bx5EWlSMTEx/vKEL7/8Mvn5+S127jPOOIM+ffpUaT/mmGP8X99www1Vnu/Ro4f/uM2bN1f72pGRkVxxxRVV2t1uN1dffTUAX331VaU6+rNmzQLg/PPPJz4+vtrXPfvsswEzt7u0tLTK8/Hx8UyYMKHaY+syd+5cvF4vnTt35oILLqjyfGRkJNdee62/b3XnbwrXX389lmVVab/xxhsB2L59u38aSX3Ex8dz2GGHAbBy5coa+zWmTKZvjv2qVatqfF8OO+wwxowZU6W9rustKiqKQw89FIBNmzZVes53zVx66aWEh4dXe94//OEPAPzwww91/BRVffzxx3i9XsaNG0evXr2q7XPaaacRHh7O5s2bSU1NrfK80+nkyiuvrPZY39Qvj8dDZmZmwPGJSGA0R15EmtykSZN48803SUtL46233qo2mWkOycnJ1bZ36tQJMEl3dYm+r8+2bdsqLVKsaPjw4TVWbxk9ejRg5ujv2rXLnyCtWLECgDfeeIMZM2bUGntBQQGZmZn+WCueNyysYb+q161bB8ARRxxRaY50RUcddRQA+fn5bN26lYEDBzboXDVxuVwcfvjh1T7Xt29fOnfuzP79+1m3bh2HHHJIpedXr17NjBkzWLFiBfv27av2prC6RBMgIiKCIUOG1BpbSUkJs2bNYvbs2fzyyy9kZmbi8Xgq9fGtFUhISKhyfF3XG8CgQYNq7VNx0W5JSYl/4fTUqVOZNm1atcf6bixSUlJq+tFq5LsmZ8+ezYIFC2rs51usnpKSQpcuXSo917t372rfDzB/p3379mXbtm1MnDiRyy67jOOPP57+/ftXezMnIo2jRF5EmlxkZCQ33HADjz76KK+++iqXXHJJlcV3zaFz587VtvsW4iUmJtaYTPgS3Zqq7SQlJdV43orPZWRk+BP5/fv3A5CTk0NOTk4d0Ztk/mA1JUz1kZGRUSW+g1VcLOrr35Ti4+NrHFkGE9v+/furnPvVV1/lySef9H/C4XQ66dChAy6XCzDvaVFRUbXvme+8FRdgHiwvL49rrrnGn9iCSf4rLtxMS0sDqv97gbqvN6BKEuxT3fXmW1wM1Gs021ceNBC+a9K3MLYugV6TTqeTadOmcfPNN7Nz506mTJnClClTiI+PZ+zYsZx77rmccsopSupFmogSeRFpFhdddBGvvfYae/fu5bXXXuO2224LdkjNpuJ0mop8JfZeeOEFTj311Aa9dk0j6YEoLi6u8blgJ1TVvXebN29m2rRp2LbNZZddxsUXX0y/fv0qvRd33HEHn376aY3vfV3v2wsvvMCKFSvo2LEjd911F8cff3ylkfTS0lKGDh1aY4zNoWJJxk8++aTOTxQac4577rmHyy+/vEGvUdd7O2LECL766iu++uorfvjhB37++Wd27tzJnDlzmDNnDieccAIvvfRSk1zbIu2d5siLSLMIDw/npptuAkz5wbpGe33/qVes915Rbm5u0wYYoJqmcED5KCdUHq1MTEwEqFL3u6X4YtmzZ0+Nffbu3Vulf1PKzMys9UbC995VPPecOXPwer0cd9xx3HvvvQwcOLBK0peent6ouHx14u+9917Gjx9fZUqTbzS+JcXHx/t/zua6ZnzXpG/X3uYSERHBOeecw+OPP868efOYN28eN9xwA5ZlsWDBAt57771mPb9Ie6FEXkSazfnnn0/v3r3Jy8vjlVdeqbWvb+rNvn37qn1+zZo1TR5fINasWVPjFIulS5cCptZ5z549/e2+BY0V66G3pGHDhgFmwWZNsf/000+AWYDZr1+/Jo/B4/HUuCB1+/bt/hskX6xQfg34RsQPlp+fX+si1/rwnePgefk+ixYtatTrN4TL5WL48OGAWTgdKN+nK7V9guC7Jr/55psq6wGaU69evbj99tv9dfN9m42JSOMokReRZhMWFsYtt9wCwLvvvlvrqLZv4eD8+fOrPFdcXMybb77ZPEHWU0FBAf/973+rtBcXF/P6668DcPrpp1eaqnLeeecBZmfPjz/+uNbXr2mRbWP87ne/w+FwkJmZyfvvv1/l+YKCAl599VV/3+aa6vDyyy9Xm1y+/PLLgFkgWXEaSUxMDFC1oovPSy+9RF5eXqNiqu0ceXl5vPjii416/YbyXTNfffWV/yarJgdfM76fqaZdb32v73A4SE1N9b//9X39+qjt0xfAvwlUXf1EpH6UyItIszr77LMZOHAghYWFLF68uMZ+Z555JgAzZ87kww8/9P9Hv3nzZq677rpabwJaQmxsLM888wxvvvmmf5Hhzp07+dOf/sSvv/6K2+3m+uuvr3TMCSec4N998+677+bZZ5+t9HNkZWUxb948/vSnPzF16tQmj7lHjx7+3Uv/+c9/8v777/vf161bt3L99dezfft2IiMj+dOf/tTk5wez8Hnx4sXcfffd/ukw2dnZPPnkk3z44YcA3HLLLZVugI499lgAvv32W1566SX/pwkZGRk8/vjjvPzyyzWW86wv3zmmTp3KkiVL/Dcaq1ev5sorrwxa6cQ//vGPHHrooXi9Xm688UbefPPNSrGkp6fz//7f/2PSpElVbix9FXKWL1/Otm3bqn39AQMG+MuoPvfcczz44IPs3LnT/3xeXh4//PADd9xxR4PWtSxYsICJEycyc+ZMdu/e7W8vKChg5syZ/t1kjzvuuIBfW0Sq0mJXEWlWDoeDP//5z/z5z3+utd8FF1zAxx9/zKpVq7j77ru57777iIiIIDc3l/j4eB577DFuvvnmFoq6qlNPPZW8vDwee+wxnnzySSIjI/0jn06nkylTptC7d+8qxz3++ON4vV7mzZvH888/z/PPP09sbCy2bVea93/++ec3S9x33XUXO3fu5IcffuC+++7j4YcfrhR7eHg406ZNa5ZpNWDmvl9++eVMmTKFWbNmERcXR05Ojn/R5aWXXuqvpe9z3HHHcdppp/HVV1/xr3/9i6effpq4uDiys7OxbZsJEybg9Xr9Ndcb4i9/+Qs//PADe/fuZdKkSbjdbpxOJ/n5+URERPD88883qg59Q7lcLl544QVuueUWli9fzmOPPcaUKVOIi4vD4/FUqjQzduzYSseOGTOG3r17s2PHDs444ww6duxIZGQkYD4R81UouuOOOygsLGTGjBm8++67vPvuu0RHR+N0OsnJyfHf1FRXI78+Vq5c6Z/6FBERgdvt9v/dAZx44olMnDixQa8tIpUpkReRZnfaaacxbNgwf13z6rhcLl577TVeeOEFZs+eTWpqKpGRkZx22mlBTeB9LMvimWee4a233uKjjz5ix44ddOjQgcMPP5ybbrqJkSNHVntcVFQUzz//PN9++y0ffvghq1atIiMjA4fDQZ8+fRgxYgSnnXYaJ554YrPEHRkZySuvvMJHH33Exx9/zC+//EJBQQE9evTgmGOO4dprr6Vv377Ncm6fK6+8kt69e/P666+zYcMG3G43gwcP5tJLL+Wcc/5/e3fMmjAQhnH8KeIuuCkubsmYyV3JlujkmDVfIaOzS4gfQBwchCyCkNFNJINjvoCjusTVIR0K0kJbSguWa/+//eCWg+cO3uf8d9fEcaz5fK71eq3j8aiqquQ4jsbjsUajkaIo+tGeOp2O0jTVbDbTbrfT9XpVo9FQv99XGIYf9r8/QrPZ1HK5VJZl2mw2KopCZVmqXq+r2+3KcRy5rvvm4ynp5QwtFgslSaI8z3W5XO5D5q9rLmu1miaTiTzP02q10uFw0Pl81u12U6vVkm3bGgwG32pa6vV6mk6n2u/3KopCp9Ppfhm3LEvD4VC+739aDQrg656qR/VqAQD+jTzPFQSB2u22ttvtb28HAP4krsQAAACAgQjyAAAAgIEI8gAAAICBCPIAAACAgRh2BQAAAAzEizwAAABgIII8AAAAYCCCPAAAAGAggjwAAABgIII8AAAAYCCCPAAAAGCgZ7MEel9TZnZQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "root_dir_list = ['results/t4/','results/t4/','results/t4/','results/t4/','results/t4/']\n",
    "path_list = ['try1_t6_r1.0', 'try1_t6_r2.5', 'try1_t6_r5','try1_t6_r7.5', 'try1_t6_r10']\n",
    "ratios = [1.0, 2.5, 5.0, 7.5, 10.0]\n",
    "grad_df, hess_f= plot_exp_data(root_dir_list[:], path_list[:], ratios[:], save_fig=False, train_size=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "2e305d1b-4df7-4073-b0cf-c11467f9fa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir='results/t2/'\n",
    "path='try1_t6_r1.5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6892c9-76d6-43fa-b5ec-a62b23fbc255",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Reading the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dd8191cb-6495-4a73-9b92-dafcefe5c978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "aa98df0d-6dca-425d-b065-efaa275a033c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exp_details(root_dir, path):\n",
    "    result_details={}\n",
    "    details_path = root_dir+ 'details_'+ path + '.txt'\n",
    "    with open(details_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        key, val = line[:-1].split(':')\n",
    "        key, val = key.strip(' '), val.strip(' ')\n",
    "        if key in ['ratio', 'alpha_0']:\n",
    "            val = float(val)\n",
    "        if key in ['Times', 'Weights', 'Epochs', 'book_keep_freq', 'g_times', 'g_epochs','g_weight', 'freq_reduce_by', 'freq_reduce_after']:\n",
    "            val = int(val)\n",
    "        result_details[key] = val\n",
    "    return result_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "100a5238-e315-4d61-92f8-8b50e5621df4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def our_bound_computatio(K_g, L_g, m, t, alpha_0, way='normal'):\n",
    "    beta = 1\n",
    "    alpha_lg = alpha_0*L_g\n",
    "    print('alpha_0 L_g (<=0.25?):',alpha_lg)\n",
    "    if way == 'normal':\n",
    "        F = np.power(t/m, 2*alpha_lg)\n",
    "        F = F*np.power(2, t/m)\n",
    "        # print('F', F)\n",
    "        beta = (1+alpha_lg)*np.power(K_g,2)/L_g\n",
    "        beta = beta* F/(np.power(m, 1-2*alpha_lg))\n",
    "    est_gen_err = beta + 2*beta*np.sqrt(m)\n",
    "    return est_gen_err\n",
    "\n",
    "def get_bound(r_det, K_g, L_g):\n",
    "    alpha_0 = r_det['alpha_0']\n",
    "    book_keep_freq = r_det['book_keep_freq']\n",
    "    freq_reduce_after = r_det['freq_reduce_after']\n",
    "    freq_reduce_by = r_det['freq_reduce_by']\n",
    "    x_values = []\n",
    "    count_keep = 0\n",
    "    for i in range(20000):\n",
    "        if i%book_keep_freq==0:\n",
    "            x_values.append(i+1)\n",
    "            count_keep+=1\n",
    "            if count_keep%freq_reduce_after==0:\n",
    "                book_keep_freq+=freq_reduce_by\n",
    "    x_values = np.array(x_values)\n",
    "    print('x value shape:', x_values.shape)        \n",
    "    m = x_values\n",
    "    t = m\n",
    "    # print('shape of m',m.shape)\n",
    "    # print('m:',m)\n",
    "    bound = our_bound_computatio(K_g=K_g, L_g=L_g, m=m, t=t, alpha_0=alpha_0)\n",
    "    return bound\n",
    "\n",
    "def get_exp_results(r_det):\n",
    "    root_dir = r_det['result_root_dir']\n",
    "    path = r_det['result_path']\n",
    "    grad_file_path = root_dir+'grad_'+path\n",
    "    hess_file_path = root_dir+'hess_'+path\n",
    "    gen_file_path  = root_dir+'gen_'+path\n",
    "    \n",
    "    # print(result_details)\n",
    "    grad_list    = []\n",
    "    hess_list    = []\n",
    "    gen_err_list = []\n",
    "    \n",
    "    with open(grad_file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        tensors = line[:-1].split(') ')\n",
    "        # print('tensors',tensors)\n",
    "        t_list = [float(t[7:].split(' ')[0][:-1]) for t in tensors]\n",
    "        # print('grad_list_i length:',len(t_list))\n",
    "        grad_list.append(t_list)\n",
    "    \n",
    "    with open(hess_file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        tensors = line[:-1].split(' ')\n",
    "        # print('tensors',tensors)\n",
    "        t_list = [float(t[7:-1]) for t in tensors]\n",
    "        # print('hess_list_i length:',len(t_list))\n",
    "        hess_list.append(t_list)\n",
    "    hess_list = np.array(hess_list)\n",
    "    grad_list = np.array(grad_list)\n",
    "    print('hess list shape:',hess_list.shape)\n",
    "    print('grad list shape:',grad_list.shape)\n",
    "    K_g = np.max(np.mean(np.array(grad_list), 0))\n",
    "    L_g = np.max(np.mean(np.array(hess_list), 0))\n",
    "    \n",
    "    # alpha_0 = r_det['alpha_0']\n",
    "    # book_keep_freq = r_det['book_keep_freq']\n",
    "    # freq_reduce_after = r_det['freq_reduce_after']\n",
    "    # freq_reduce_by = r_det['freq_reduce_by']\n",
    "    # x_values = []\n",
    "    # count_keep = 0\n",
    "    # for i in range(20000):\n",
    "    #     if i%book_keep_freq==0:\n",
    "    #         x_values.append(i+1)\n",
    "    #         count_keep+=1\n",
    "    #         if count_keep%freq_reduce_after==0:\n",
    "    #             book_keep_freq+=freq_reduce_by\n",
    "    # x_values = np.array(x_values)\n",
    "    # print('x value shape:', x_values.shape)        \n",
    "    # m = x_values\n",
    "    # t = m\n",
    "    # # print('shape of m',m.shape)\n",
    "    # # print('m:',m)\n",
    "    # bound = our_bound_computatio(K_g=K_g, L_g=L_g, m=m, t=t, alpha_0=alpha_0)\n",
    "    \n",
    "    return grad_list, hess_list, K_g, L_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3aae3183-9dce-4a66-9a45-5de4ec92b27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hess list shape: (2, 400)\n",
      "grad list shape: (2, 400)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6.15095, 64.3463)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir='results/t3/'\n",
    "path='try1_t6_r1.0'\n",
    "details = get_exp_details(root_dir, path)\n",
    "# details\n",
    "grad_list, hess_list, K_g, L_g = get_exp_results(details)\n",
    "K_g, L_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68ed7c9-0976-4721-854d-76f6ebd3ed65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c1070be2-0de7-4330-bbfb-88f7b77a1b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f5afc09fd90>]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.max(hess_list, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c0d08230-5810-4b2b-b705-3fd162ab24b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.568433333333332"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fd9dc6-e039-42bd-971a-baf1a1922c3b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## For non decreasing lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5e8a228-934b-4a09-b471-b9d8d164097c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected weight:[12]\n",
      "train data size:2000\n",
      "test data size:256\n",
      "Time:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94fdc882fc8b40e6aeb25e96de4ccd97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(4.9090, dtype=torch.float64)\n",
      "hess: tensor(35.3408)\n",
      "\taccuracy:10.15625\n",
      "\tloss: 2.077993\n",
      "grad: tensor(3.1533, dtype=torch.float64)\n",
      "hess: tensor(23.4449)\n",
      "grad: tensor(2.7884, dtype=torch.float64)\n",
      "hess: tensor(18.4208)\n",
      "grad: tensor(3.8249, dtype=torch.float64)\n",
      "hess: tensor(30.1776)\n",
      "grad: tensor(2.9495, dtype=torch.float64)\n",
      "hess: tensor(19.5573)\n",
      "grad: tensor(3.3696, dtype=torch.float64)\n",
      "hess: tensor(21.0472)\n",
      "grad: tensor(3.9576, dtype=torch.float64)\n",
      "hess: tensor(41.5925)\n",
      "grad: tensor(2.8509, dtype=torch.float64)\n",
      "hess: tensor(27.9009)\n",
      "grad: tensor(3.7645, dtype=torch.float64)\n",
      "hess: tensor(41.7642)\n",
      "grad: tensor(2.9160, dtype=torch.float64)\n",
      "hess: tensor(23.0151)\n",
      "grad: tensor(5.0757, dtype=torch.float64)\n",
      "hess: tensor(37.1711)\n",
      "grad: tensor(4.1145, dtype=torch.float64)\n",
      "hess: tensor(44.3680)\n",
      "grad: tensor(3.6553, dtype=torch.float64)\n",
      "hess: tensor(30.5565)\n",
      "grad: tensor(3.2693, dtype=torch.float64)\n",
      "hess: tensor(36.4351)\n",
      "grad: tensor(3.7443, dtype=torch.float64)\n",
      "hess: tensor(26.5937)\n",
      "grad: tensor(6.4074, dtype=torch.float64)\n",
      "hess: tensor(52.3759)\n",
      "grad: tensor(6.0343, dtype=torch.float64)\n",
      "hess: tensor(48.3401)\n",
      "grad: tensor(5.9828, dtype=torch.float64)\n",
      "hess: tensor(43.6040)\n",
      "grad: tensor(3.9270, dtype=torch.float64)\n",
      "hess: tensor(34.2989)\n",
      "grad: tensor(4.7289, dtype=torch.float64)\n",
      "hess: tensor(49.6219)\n",
      "grad: tensor(4.2319, dtype=torch.float64)\n",
      "hess: tensor(37.1062)\n",
      "grad: tensor(2.8755, dtype=torch.float64)\n",
      "hess: tensor(22.5591)\n",
      "grad: tensor(6.1508, dtype=torch.float64)\n",
      "hess: tensor(44.3183)\n",
      "grad: tensor(3.5870, dtype=torch.float64)\n",
      "hess: tensor(32.1345)\n",
      "grad: tensor(5.9689, dtype=torch.float64)\n",
      "hess: tensor(41.8048)\n",
      "grad: tensor(6.2377, dtype=torch.float64)\n",
      "hess: tensor(48.0429)\n",
      "grad: tensor(3.9863, dtype=torch.float64)\n",
      "hess: tensor(27.5788)\n",
      "grad: tensor(5.8723, dtype=torch.float64)\n",
      "hess: tensor(37.7594)\n",
      "grad: tensor(4.3738, dtype=torch.float64)\n",
      "hess: tensor(39.5553)\n",
      "grad: tensor(3.8992, dtype=torch.float64)\n",
      "hess: tensor(28.7223)\n",
      "grad: tensor(5.0619, dtype=torch.float64)\n",
      "hess: tensor(47.6522)\n",
      "grad: tensor(3.0570, dtype=torch.float64)\n",
      "hess: tensor(30.2895)\n",
      "grad: tensor(6.0969, dtype=torch.float64)\n",
      "hess: tensor(51.2158)\n",
      "grad: tensor(4.2346, dtype=torch.float64)\n",
      "hess: tensor(32.0216)\n",
      "grad: tensor(4.2579, dtype=torch.float64)\n",
      "hess: tensor(28.8578)\n",
      "grad: tensor(4.7450, dtype=torch.float64)\n",
      "hess: tensor(33.6350)\n",
      "grad: tensor(4.4438, dtype=torch.float64)\n",
      "hess: tensor(26.6078)\n",
      "grad: tensor(4.8560, dtype=torch.float64)\n",
      "hess: tensor(37.6322)\n",
      "grad: tensor(4.7986, dtype=torch.float64)\n",
      "hess: tensor(38.5857)\n",
      "grad: tensor(2.9459, dtype=torch.float64)\n",
      "hess: tensor(28.9520)\n",
      "grad: tensor(2.7016, dtype=torch.float64)\n",
      "hess: tensor(18.1241)\n",
      "grad: tensor(4.8019, dtype=torch.float64)\n",
      "hess: tensor(38.4251)\n",
      "grad: tensor(4.3024, dtype=torch.float64)\n",
      "hess: tensor(31.4527)\n",
      "grad: tensor(6.1896, dtype=torch.float64)\n",
      "hess: tensor(40.3147)\n",
      "grad: tensor(5.4642, dtype=torch.float64)\n",
      "hess: tensor(40.1855)\n",
      "grad: tensor(3.8067, dtype=torch.float64)\n",
      "hess: tensor(26.7563)\n",
      "grad: tensor(4.2798, dtype=torch.float64)\n",
      "hess: tensor(26.7896)\n",
      "grad: tensor(5.3425, dtype=torch.float64)\n",
      "hess: tensor(36.8674)\n",
      "grad: tensor(5.4335, dtype=torch.float64)\n",
      "hess: tensor(34.0951)\n",
      "grad: tensor(5.4619, dtype=torch.float64)\n",
      "hess: tensor(40.7434)\n",
      "grad: tensor(4.0653, dtype=torch.float64)\n",
      "hess: tensor(32.0296)\n",
      "\taccuracy:60.546875\n",
      "\tloss: 1.610808\n",
      "grad: tensor(9.4661, dtype=torch.float64)\n",
      "hess: tensor(56.3633)\n",
      "grad: tensor(4.7321, dtype=torch.float64)\n",
      "hess: tensor(38.0325)\n",
      "grad: tensor(4.9950, dtype=torch.float64)\n",
      "hess: tensor(37.5948)\n",
      "grad: tensor(3.0786, dtype=torch.float64)\n",
      "hess: tensor(26.1241)\n",
      "grad: tensor(3.3914, dtype=torch.float64)\n",
      "hess: tensor(28.9493)\n",
      "grad: tensor(5.0570, dtype=torch.float64)\n",
      "hess: tensor(41.5690)\n",
      "grad: tensor(5.8425, dtype=torch.float64)\n",
      "hess: tensor(35.0623)\n",
      "grad: tensor(5.4336, dtype=torch.float64)\n",
      "hess: tensor(28.3594)\n",
      "grad: tensor(4.9477, dtype=torch.float64)\n",
      "hess: tensor(39.1939)\n",
      "grad: tensor(5.6739, dtype=torch.float64)\n",
      "hess: tensor(40.1674)\n",
      "grad: tensor(5.5040, dtype=torch.float64)\n",
      "hess: tensor(32.5508)\n",
      "grad: tensor(11.0769, dtype=torch.float64)\n",
      "hess: tensor(89.2552)\n",
      "grad: tensor(7.6669, dtype=torch.float64)\n",
      "hess: tensor(54.7557)\n",
      "grad: tensor(5.2677, dtype=torch.float64)\n",
      "hess: tensor(37.6786)\n",
      "grad: tensor(4.6781, dtype=torch.float64)\n",
      "hess: tensor(31.8345)\n",
      "grad: tensor(5.9638, dtype=torch.float64)\n",
      "hess: tensor(47.9999)\n",
      "grad: tensor(5.8678, dtype=torch.float64)\n",
      "hess: tensor(35.4265)\n",
      "grad: tensor(8.8745, dtype=torch.float64)\n",
      "hess: tensor(52.2049)\n",
      "grad: tensor(5.5223, dtype=torch.float64)\n",
      "hess: tensor(40.8739)\n",
      "grad: tensor(5.7112, dtype=torch.float64)\n",
      "hess: tensor(41.9332)\n",
      "grad: tensor(3.8189, dtype=torch.float64)\n",
      "hess: tensor(22.6363)\n",
      "grad: tensor(5.8887, dtype=torch.float64)\n",
      "hess: tensor(40.6360)\n",
      "grad: tensor(5.3494, dtype=torch.float64)\n",
      "hess: tensor(38.5157)\n",
      "grad: tensor(4.2164, dtype=torch.float64)\n",
      "hess: tensor(23.0810)\n",
      "grad: tensor(4.1688, dtype=torch.float64)\n",
      "hess: tensor(25.2288)\n",
      "grad: tensor(5.4544, dtype=torch.float64)\n",
      "hess: tensor(32.0309)\n",
      "grad: tensor(7.5617, dtype=torch.float64)\n",
      "hess: tensor(62.6172)\n",
      "grad: tensor(13.2327, dtype=torch.float64)\n",
      "hess: tensor(79.9723)\n",
      "grad: tensor(3.5410, dtype=torch.float64)\n",
      "hess: tensor(26.5260)\n",
      "grad: tensor(4.7710, dtype=torch.float64)\n",
      "hess: tensor(28.8068)\n",
      "grad: tensor(6.6105, dtype=torch.float64)\n",
      "hess: tensor(42.3067)\n",
      "grad: tensor(3.9900, dtype=torch.float64)\n",
      "hess: tensor(19.3685)\n",
      "grad: tensor(6.1766, dtype=torch.float64)\n",
      "hess: tensor(36.7097)\n",
      "grad: tensor(6.5307, dtype=torch.float64)\n",
      "hess: tensor(47.0763)\n",
      "grad: tensor(6.0827, dtype=torch.float64)\n",
      "hess: tensor(48.6441)\n",
      "grad: tensor(5.9745, dtype=torch.float64)\n",
      "hess: tensor(39.0077)\n",
      "grad: tensor(4.5577, dtype=torch.float64)\n",
      "hess: tensor(36.1707)\n",
      "grad: tensor(5.3369, dtype=torch.float64)\n",
      "hess: tensor(35.5863)\n",
      "grad: tensor(4.8627, dtype=torch.float64)\n",
      "hess: tensor(32.7086)\n",
      "grad: tensor(1.6848, dtype=torch.float64)\n",
      "hess: tensor(18.0634)\n",
      "grad: tensor(1.1396, dtype=torch.float64)\n",
      "hess: tensor(14.6856)\n",
      "grad: tensor(12.3348, dtype=torch.float64)\n",
      "hess: tensor(92.7479)\n",
      "grad: tensor(3.5014, dtype=torch.float64)\n",
      "hess: tensor(43.2091)\n",
      "grad: tensor(4.2122, dtype=torch.float64)\n",
      "hess: tensor(33.3337)\n",
      "grad: tensor(5.8797, dtype=torch.float64)\n",
      "hess: tensor(40.7212)\n",
      "grad: tensor(6.2242, dtype=torch.float64)\n",
      "hess: tensor(41.7900)\n",
      "grad: tensor(7.1195, dtype=torch.float64)\n",
      "hess: tensor(48.3965)\n",
      "grad: tensor(6.7516, dtype=torch.float64)\n",
      "hess: tensor(53.4580)\n",
      "grad: tensor(3.6861, dtype=torch.float64)\n",
      "hess: tensor(25.6963)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb959b32c614351954bffd1a6ec52d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(0.5954, dtype=torch.float64)\n",
      "hess: tensor(11.3269)\n",
      "\taccuracy:77.34375\n",
      "\tloss: 0.032371\n",
      "grad: tensor(2.9844, dtype=torch.float64)\n",
      "hess: tensor(15.7794)\n",
      "grad: tensor(3.4841, dtype=torch.float64)\n",
      "hess: tensor(20.8347)\n",
      "grad: tensor(5.0536, dtype=torch.float64)\n",
      "hess: tensor(55.4513)\n",
      "grad: tensor(11.4155, dtype=torch.float64)\n",
      "hess: tensor(73.1358)\n",
      "grad: tensor(10.3055, dtype=torch.float64)\n",
      "hess: tensor(57.4586)\n",
      "grad: tensor(4.8877, dtype=torch.float64)\n",
      "hess: tensor(33.9880)\n",
      "grad: tensor(7.2319, dtype=torch.float64)\n",
      "hess: tensor(31.9405)\n",
      "grad: tensor(4.8639, dtype=torch.float64)\n",
      "hess: tensor(47.8478)\n",
      "grad: tensor(4.7219, dtype=torch.float64)\n",
      "hess: tensor(36.4633)\n",
      "grad: tensor(0.9104, dtype=torch.float64)\n",
      "hess: tensor(12.7152)\n",
      "grad: tensor(13.2961, dtype=torch.float64)\n",
      "hess: tensor(70.3237)\n",
      "grad: tensor(0.9106, dtype=torch.float64)\n",
      "hess: tensor(16.1274)\n",
      "grad: tensor(5.5423, dtype=torch.float64)\n",
      "hess: tensor(35.4139)\n",
      "grad: tensor(4.3628, dtype=torch.float64)\n",
      "hess: tensor(42.1757)\n",
      "grad: tensor(3.1523, dtype=torch.float64)\n",
      "hess: tensor(35.5400)\n",
      "grad: tensor(5.4658, dtype=torch.float64)\n",
      "hess: tensor(50.9567)\n",
      "grad: tensor(4.8963, dtype=torch.float64)\n",
      "hess: tensor(34.5207)\n",
      "grad: tensor(1.4601, dtype=torch.float64)\n",
      "hess: tensor(14.3679)\n",
      "grad: tensor(8.6136, dtype=torch.float64)\n",
      "hess: tensor(55.9435)\n",
      "grad: tensor(2.0238, dtype=torch.float64)\n",
      "hess: tensor(11.1304)\n",
      "grad: tensor(1.2119, dtype=torch.float64)\n",
      "hess: tensor(15.6014)\n",
      "grad: tensor(1.8759, dtype=torch.float64)\n",
      "hess: tensor(25.3249)\n",
      "grad: tensor(5.1484, dtype=torch.float64)\n",
      "hess: tensor(35.7426)\n",
      "grad: tensor(8.3087, dtype=torch.float64)\n",
      "hess: tensor(55.9764)\n",
      "grad: tensor(2.7588, dtype=torch.float64)\n",
      "hess: tensor(23.7113)\n",
      "\taccuracy:75.390625\n",
      "\tloss: 0.338962\n",
      "grad: tensor(2.7284, dtype=torch.float64)\n",
      "hess: tensor(25.2317)\n",
      "grad: tensor(0.7245, dtype=torch.float64)\n",
      "hess: tensor(13.2107)\n",
      "grad: tensor(2.5977, dtype=torch.float64)\n",
      "hess: tensor(29.4006)\n",
      "grad: tensor(3.3567, dtype=torch.float64)\n",
      "hess: tensor(31.7347)\n",
      "grad: tensor(3.8564, dtype=torch.float64)\n",
      "hess: tensor(46.7348)\n",
      "grad: tensor(17.9094, dtype=torch.float64)\n",
      "hess: tensor(160.3835)\n",
      "grad: tensor(2.2344, dtype=torch.float64)\n",
      "hess: tensor(29.4114)\n",
      "grad: tensor(3.7441, dtype=torch.float64)\n",
      "hess: tensor(55.8612)\n",
      "grad: tensor(5.5180, dtype=torch.float64)\n",
      "hess: tensor(67.7967)\n",
      "grad: tensor(5.6802, dtype=torch.float64)\n",
      "hess: tensor(42.0999)\n",
      "grad: tensor(5.5073, dtype=torch.float64)\n",
      "hess: tensor(48.5194)\n",
      "grad: tensor(2.5605, dtype=torch.float64)\n",
      "hess: tensor(24.1616)\n",
      "grad: tensor(7.7132, dtype=torch.float64)\n",
      "hess: tensor(40.9917)\n",
      "grad: tensor(17.4326, dtype=torch.float64)\n",
      "hess: tensor(130.8765)\n",
      "grad: tensor(4.3622, dtype=torch.float64)\n",
      "hess: tensor(35.7147)\n",
      "grad: tensor(2.3849, dtype=torch.float64)\n",
      "hess: tensor(17.6597)\n",
      "grad: tensor(5.7190, dtype=torch.float64)\n",
      "hess: tensor(55.7466)\n",
      "grad: tensor(6.1075, dtype=torch.float64)\n",
      "hess: tensor(45.6241)\n",
      "grad: tensor(4.2166, dtype=torch.float64)\n",
      "hess: tensor(37.2131)\n",
      "grad: tensor(0.4676, dtype=torch.float64)\n",
      "hess: tensor(7.8924)\n",
      "grad: tensor(8.5074, dtype=torch.float64)\n",
      "hess: tensor(103.3100)\n",
      "grad: tensor(1.2911, dtype=torch.float64)\n",
      "hess: tensor(18.3066)\n",
      "grad: tensor(7.1455, dtype=torch.float64)\n",
      "hess: tensor(49.8598)\n",
      "grad: tensor(8.8857, dtype=torch.float64)\n",
      "hess: tensor(67.8239)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f95dabbcd84b97bea1b4fc13aee410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(0.0570, dtype=torch.float64)\n",
      "hess: tensor(1.4613)\n",
      "\taccuracy:85.546875\n",
      "\tloss: 0.002388\n",
      "grad: tensor(1.3613, dtype=torch.float64)\n",
      "hess: tensor(10.4944)\n",
      "grad: tensor(1.6814, dtype=torch.float64)\n",
      "hess: tensor(14.3682)\n",
      "grad: tensor(4.4507, dtype=torch.float64)\n",
      "hess: tensor(68.3256)\n",
      "grad: tensor(12.9868, dtype=torch.float64)\n",
      "hess: tensor(108.0172)\n",
      "grad: tensor(14.1767, dtype=torch.float64)\n",
      "hess: tensor(87.5919)\n",
      "grad: tensor(4.6074, dtype=torch.float64)\n",
      "hess: tensor(40.1073)\n",
      "grad: tensor(9.5155, dtype=torch.float64)\n",
      "hess: tensor(46.8900)\n",
      "grad: tensor(2.0006, dtype=torch.float64)\n",
      "hess: tensor(29.4090)\n",
      "grad: tensor(4.3347, dtype=torch.float64)\n",
      "hess: tensor(42.1310)\n",
      "grad: tensor(0.1327, dtype=torch.float64)\n",
      "hess: tensor(2.5619)\n",
      "grad: tensor(17.6388, dtype=torch.float64)\n",
      "hess: tensor(102.0900)\n",
      "grad: tensor(0.1955, dtype=torch.float64)\n",
      "hess: tensor(4.5576)\n",
      "grad: tensor(4.6430, dtype=torch.float64)\n",
      "hess: tensor(39.9449)\n",
      "grad: tensor(1.8025, dtype=torch.float64)\n",
      "hess: tensor(25.6271)\n",
      "grad: tensor(0.7550, dtype=torch.float64)\n",
      "hess: tensor(12.8362)\n",
      "grad: tensor(1.7233, dtype=torch.float64)\n",
      "hess: tensor(27.7248)\n",
      "grad: tensor(4.7631, dtype=torch.float64)\n",
      "hess: tensor(39.9902)\n",
      "grad: tensor(0.4475, dtype=torch.float64)\n",
      "hess: tensor(5.9011)\n",
      "grad: tensor(7.1157, dtype=torch.float64)\n",
      "hess: tensor(65.3507)\n",
      "grad: tensor(1.0033, dtype=torch.float64)\n",
      "hess: tensor(7.5331)\n",
      "grad: tensor(0.4831, dtype=torch.float64)\n",
      "hess: tensor(8.1041)\n",
      "grad: tensor(0.7665, dtype=torch.float64)\n",
      "hess: tensor(13.9323)\n",
      "grad: tensor(5.4758, dtype=torch.float64)\n",
      "hess: tensor(43.9105)\n",
      "grad: tensor(8.0793, dtype=torch.float64)\n",
      "hess: tensor(71.5588)\n",
      "grad: tensor(1.1793, dtype=torch.float64)\n",
      "hess: tensor(13.9296)\n",
      "\taccuracy:82.8125\n",
      "\tloss: 0.101627\n",
      "grad: tensor(1.1153, dtype=torch.float64)\n",
      "hess: tensor(13.7425)\n",
      "grad: tensor(0.2606, dtype=torch.float64)\n",
      "hess: tensor(5.8801)\n",
      "grad: tensor(0.7864, dtype=torch.float64)\n",
      "hess: tensor(12.3010)\n",
      "grad: tensor(2.1948, dtype=torch.float64)\n",
      "hess: tensor(26.8756)\n",
      "grad: tensor(2.3414, dtype=torch.float64)\n",
      "hess: tensor(36.9944)\n",
      "grad: tensor(13.1342, dtype=torch.float64)\n",
      "hess: tensor(143.2760)\n",
      "grad: tensor(1.3924, dtype=torch.float64)\n",
      "hess: tensor(23.6883)\n",
      "grad: tensor(1.5874, dtype=torch.float64)\n",
      "hess: tensor(33.2554)\n",
      "grad: tensor(3.5079, dtype=torch.float64)\n",
      "hess: tensor(57.4223)\n",
      "grad: tensor(4.9927, dtype=torch.float64)\n",
      "hess: tensor(48.0871)\n",
      "grad: tensor(4.6607, dtype=torch.float64)\n",
      "hess: tensor(51.3995)\n",
      "grad: tensor(1.9463, dtype=torch.float64)\n",
      "hess: tensor(23.2021)\n",
      "grad: tensor(8.7060, dtype=torch.float64)\n",
      "hess: tensor(47.6801)\n",
      "grad: tensor(15.5690, dtype=torch.float64)\n",
      "hess: tensor(142.5892)\n",
      "grad: tensor(3.6157, dtype=torch.float64)\n",
      "hess: tensor(37.3464)\n",
      "grad: tensor(1.1847, dtype=torch.float64)\n",
      "hess: tensor(10.2655)\n",
      "grad: tensor(4.9552, dtype=torch.float64)\n",
      "hess: tensor(59.4046)\n",
      "grad: tensor(4.9117, dtype=torch.float64)\n",
      "hess: tensor(47.0596)\n",
      "grad: tensor(2.8557, dtype=torch.float64)\n",
      "hess: tensor(33.1360)\n",
      "grad: tensor(0.1824, dtype=torch.float64)\n",
      "hess: tensor(3.4988)\n",
      "grad: tensor(2.9633, dtype=torch.float64)\n",
      "hess: tensor(61.5193)\n",
      "grad: tensor(0.5485, dtype=torch.float64)\n",
      "hess: tensor(9.4681)\n",
      "grad: tensor(7.5066, dtype=torch.float64)\n",
      "hess: tensor(57.4961)\n",
      "grad: tensor(7.4940, dtype=torch.float64)\n",
      "hess: tensor(74.7831)\n",
      "grad: tensor(0.9548, dtype=torch.float64)\n",
      "hess: tensor(13.1027)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79545820cce941519ecca7823e84d554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(0.0127, dtype=torch.float64)\n",
      "hess: tensor(0.3502)\n",
      "\taccuracy:88.671875\n",
      "\tloss: 0.000493\n",
      "grad: tensor(1.3830, dtype=torch.float64)\n",
      "hess: tensor(24.1108)\n",
      "grad: tensor(3.8416, dtype=torch.float64)\n",
      "hess: tensor(69.2523)\n",
      "grad: tensor(12.8489, dtype=torch.float64)\n",
      "hess: tensor(45.0307)\n",
      "grad: tensor(4.1752, dtype=torch.float64)\n",
      "hess: tensor(42.6748)\n",
      "grad: tensor(1.2323, dtype=torch.float64)\n",
      "hess: tensor(24.6928)\n",
      "grad: tensor(4.9398, dtype=torch.float64)\n",
      "hess: tensor(51.8502)\n",
      "grad: tensor(0.8302, dtype=torch.float64)\n",
      "hess: tensor(8.5015)\n",
      "grad: tensor(0.0756, dtype=torch.float64)\n",
      "hess: tensor(1.9648)\n",
      "grad: tensor(0.0244, dtype=torch.float64)\n",
      "hess: tensor(0.4963)\n",
      "grad: tensor(0.2764, dtype=torch.float64)\n",
      "hess: tensor(5.5910)\n",
      "grad: tensor(2.3986, dtype=torch.float64)\n",
      "hess: tensor(31.5668)\n",
      "grad: tensor(0.2374, dtype=torch.float64)\n",
      "hess: tensor(3.5709)\n",
      "grad: tensor(2.2585, dtype=torch.float64)\n",
      "hess: tensor(28.0621)\n",
      "grad: tensor(0.2940, dtype=torch.float64)\n",
      "hess: tensor(5.5198)\n",
      "grad: tensor(2.4820, dtype=torch.float64)\n",
      "hess: tensor(23.5605)\n",
      "grad: tensor(6.6393, dtype=torch.float64)\n",
      "hess: tensor(71.4743)\n",
      "\taccuracy:83.59375\n",
      "\tloss: 0.043546\n",
      "grad: tensor(20.2565, dtype=torch.float64)\n",
      "hess: tensor(176.0339)\n",
      "grad: tensor(0.1137, dtype=torch.float64)\n",
      "hess: tensor(2.8406)\n",
      "grad: tensor(1.0266, dtype=torch.float64)\n",
      "hess: tensor(15.8185)\n",
      "grad: tensor(1.8054, dtype=torch.float64)\n",
      "hess: tensor(32.2899)\n",
      "grad: tensor(5.9857, dtype=torch.float64)\n",
      "hess: tensor(76.9598)\n",
      "grad: tensor(0.9118, dtype=torch.float64)\n",
      "hess: tensor(22.7159)\n",
      "grad: tensor(1.6414, dtype=torch.float64)\n",
      "hess: tensor(21.9917)\n",
      "grad: tensor(3.7599, dtype=torch.float64)\n",
      "hess: tensor(49.1938)\n",
      "grad: tensor(0.1064, dtype=torch.float64)\n",
      "hess: tensor(1.5350)\n",
      "grad: tensor(11.1462, dtype=torch.float64)\n",
      "hess: tensor(126.2514)\n",
      "grad: tensor(8.9456, dtype=torch.float64)\n",
      "hess: tensor(68.4972)\n",
      "grad: tensor(4.3210, dtype=torch.float64)\n",
      "hess: tensor(58.9700)\n",
      "grad: tensor(0.6885, dtype=torch.float64)\n",
      "hess: tensor(12.8323)\n",
      "grad: tensor(0.1044, dtype=torch.float64)\n",
      "hess: tensor(2.1814)\n",
      "grad: tensor(0.1274, dtype=torch.float64)\n",
      "hess: tensor(3.6565)\n",
      "grad: tensor(7.5381, dtype=torch.float64)\n",
      "hess: tensor(62.5594)\n",
      "grad: tensor(0.7509, dtype=torch.float64)\n",
      "hess: tensor(11.2865)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a078a9448ef746f18b013a75e7f2e265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(0.0047, dtype=torch.float64)\n",
      "hess: tensor(0.1363)\n",
      "\taccuracy:90.234375\n",
      "\tloss: 0.000170\n",
      "grad: tensor(1.0562, dtype=torch.float64)\n",
      "hess: tensor(20.1599)\n",
      "grad: tensor(2.9973, dtype=torch.float64)\n",
      "hess: tensor(62.0847)\n",
      "grad: tensor(13.6782, dtype=torch.float64)\n",
      "hess: tensor(48.3961)\n",
      "grad: tensor(3.7439, dtype=torch.float64)\n",
      "hess: tensor(42.7720)\n",
      "grad: tensor(0.6720, dtype=torch.float64)\n",
      "hess: tensor(15.3844)\n",
      "grad: tensor(6.0550, dtype=torch.float64)\n",
      "hess: tensor(63.6448)\n",
      "grad: tensor(0.6059, dtype=torch.float64)\n",
      "hess: tensor(6.7081)\n",
      "grad: tensor(0.0371, dtype=torch.float64)\n",
      "hess: tensor(1.0276)\n",
      "grad: tensor(0.0115, dtype=torch.float64)\n",
      "hess: tensor(0.2457)\n",
      "grad: tensor(0.1540, dtype=torch.float64)\n",
      "hess: tensor(3.4624)\n",
      "grad: tensor(1.8884, dtype=torch.float64)\n",
      "hess: tensor(27.8997)\n",
      "grad: tensor(0.1576, dtype=torch.float64)\n",
      "hess: tensor(2.5613)\n",
      "grad: tensor(2.0690, dtype=torch.float64)\n",
      "hess: tensor(28.6102)\n",
      "grad: tensor(0.2057, dtype=torch.float64)\n",
      "hess: tensor(4.1395)\n",
      "grad: tensor(1.8438, dtype=torch.float64)\n",
      "hess: tensor(20.3214)\n",
      "grad: tensor(5.0555, dtype=torch.float64)\n",
      "hess: tensor(63.5903)\n",
      "\taccuracy:85.546875\n",
      "\tloss: 0.023642\n",
      "grad: tensor(18.4606, dtype=torch.float64)\n",
      "hess: tensor(166.9221)\n",
      "grad: tensor(0.0578, dtype=torch.float64)\n",
      "hess: tensor(1.5341)\n",
      "grad: tensor(0.9046, dtype=torch.float64)\n",
      "hess: tensor(14.8277)\n",
      "grad: tensor(1.4938, dtype=torch.float64)\n",
      "hess: tensor(28.8589)\n",
      "grad: tensor(5.0595, dtype=torch.float64)\n",
      "hess: tensor(72.8746)\n",
      "grad: tensor(0.6608, dtype=torch.float64)\n",
      "hess: tensor(18.0683)\n",
      "grad: tensor(0.9539, dtype=torch.float64)\n",
      "hess: tensor(14.6273)\n",
      "grad: tensor(3.1709, dtype=torch.float64)\n",
      "hess: tensor(47.4949)\n",
      "grad: tensor(0.0728, dtype=torch.float64)\n",
      "hess: tensor(1.1100)\n",
      "grad: tensor(7.5650, dtype=torch.float64)\n",
      "hess: tensor(108.1889)\n",
      "grad: tensor(8.1776, dtype=torch.float64)\n",
      "hess: tensor(71.1904)\n",
      "grad: tensor(3.7251, dtype=torch.float64)\n",
      "hess: tensor(56.0504)\n",
      "grad: tensor(0.5083, dtype=torch.float64)\n",
      "hess: tensor(10.2252)\n",
      "grad: tensor(0.0725, dtype=torch.float64)\n",
      "hess: tensor(1.6230)\n",
      "grad: tensor(0.0577, dtype=torch.float64)\n",
      "hess: tensor(1.7587)\n",
      "grad: tensor(7.4958, dtype=torch.float64)\n",
      "hess: tensor(66.2520)\n",
      "grad: tensor(0.6232, dtype=torch.float64)\n",
      "hess: tensor(9.9921)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0940dcc3bc4948a197b23d70200712d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(0.0021, dtype=torch.float64)\n",
      "hess: tensor(0.0643)\n",
      "\taccuracy:91.796875\n",
      "\tloss: 0.000071\n",
      "grad: tensor(0.8559, dtype=torch.float64)\n",
      "hess: tensor(17.5364)\n",
      "grad: tensor(2.3424, dtype=torch.float64)\n",
      "hess: tensor(54.0422)\n",
      "grad: tensor(14.1355, dtype=torch.float64)\n",
      "hess: tensor(51.7790)\n",
      "grad: tensor(3.4297, dtype=torch.float64)\n",
      "hess: tensor(42.4703)\n",
      "grad: tensor(0.4164, dtype=torch.float64)\n",
      "hess: tensor(10.3080)\n",
      "grad: tensor(7.1198, dtype=torch.float64)\n",
      "hess: tensor(74.0244)\n",
      "grad: tensor(0.5041, dtype=torch.float64)\n",
      "hess: tensor(5.8616)\n",
      "grad: tensor(0.0220, dtype=torch.float64)\n",
      "hess: tensor(0.6378)\n",
      "grad: tensor(0.0065, dtype=torch.float64)\n",
      "hess: tensor(0.1424)\n",
      "grad: tensor(0.1114, dtype=torch.float64)\n",
      "hess: tensor(2.7004)\n",
      "grad: tensor(1.4579, dtype=torch.float64)\n",
      "hess: tensor(23.6362)\n",
      "grad: tensor(0.1151, dtype=torch.float64)\n",
      "hess: tensor(1.9654)\n",
      "grad: tensor(2.0380, dtype=torch.float64)\n",
      "hess: tensor(30.1407)\n",
      "grad: tensor(0.1544, dtype=torch.float64)\n",
      "hess: tensor(3.2735)\n",
      "grad: tensor(1.5290, dtype=torch.float64)\n",
      "hess: tensor(18.6651)\n",
      "grad: tensor(3.7430, dtype=torch.float64)\n",
      "hess: tensor(53.6722)\n",
      "\taccuracy:87.109375\n",
      "\tloss: 0.015536\n",
      "grad: tensor(16.0727, dtype=torch.float64)\n",
      "hess: tensor(156.1283)\n",
      "grad: tensor(0.0340, dtype=torch.float64)\n",
      "hess: tensor(0.9424)\n",
      "grad: tensor(0.8212, dtype=torch.float64)\n",
      "hess: tensor(14.3232)\n",
      "grad: tensor(1.2674, dtype=torch.float64)\n",
      "hess: tensor(25.9748)\n",
      "grad: tensor(4.4432, dtype=torch.float64)\n",
      "hess: tensor(69.5059)\n",
      "grad: tensor(0.5278, dtype=torch.float64)\n",
      "hess: tensor(15.3171)\n",
      "grad: tensor(0.6217, dtype=torch.float64)\n",
      "hess: tensor(10.3600)\n",
      "grad: tensor(2.7206, dtype=torch.float64)\n",
      "hess: tensor(44.2860)\n",
      "grad: tensor(0.0567, dtype=torch.float64)\n",
      "hess: tensor(0.8993)\n",
      "grad: tensor(5.0337, dtype=torch.float64)\n",
      "hess: tensor(89.0209)\n",
      "grad: tensor(7.4694, dtype=torch.float64)\n",
      "hess: tensor(71.8847)\n",
      "grad: tensor(3.1760, dtype=torch.float64)\n",
      "hess: tensor(51.7995)\n",
      "grad: tensor(0.4070, dtype=torch.float64)\n",
      "hess: tensor(8.6613)\n",
      "grad: tensor(0.0534, dtype=torch.float64)\n",
      "hess: tensor(1.2582)\n",
      "grad: tensor(0.3306, dtype=torch.float64)\n",
      "hess: tensor(10.7183)\n",
      "grad: tensor(7.4960, dtype=torch.float64)\n",
      "hess: tensor(69.4963)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f7229217f834e7eabe407de352f5fc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(0.0010, dtype=torch.float64)\n",
      "hess: tensor(0.0325)\n",
      "\taccuracy:92.1875\n",
      "\tloss: 0.000033\n",
      "grad: tensor(0.4839, dtype=torch.float64)\n",
      "hess: tensor(6.6719)\n",
      "grad: tensor(15.7073, dtype=torch.float64)\n",
      "hess: tensor(152.7978)\n",
      "grad: tensor(3.0679, dtype=torch.float64)\n",
      "hess: tensor(40.8380)\n",
      "grad: tensor(0.4244, dtype=torch.float64)\n",
      "hess: tensor(9.6433)\n",
      "grad: tensor(0.0117, dtype=torch.float64)\n",
      "hess: tensor(0.2871)\n",
      "grad: tensor(0.0147, dtype=torch.float64)\n",
      "hess: tensor(0.4416)\n",
      "grad: tensor(0.2485, dtype=torch.float64)\n",
      "hess: tensor(5.4644)\n",
      "grad: tensor(0.1056, dtype=torch.float64)\n",
      "hess: tensor(2.5048)\n",
      "grad: tensor(0.0879, dtype=torch.float64)\n",
      "hess: tensor(1.5603)\n",
      "grad: tensor(0.3861, dtype=torch.float64)\n",
      "hess: tensor(4.0006)\n",
      "grad: tensor(0.1693, dtype=torch.float64)\n",
      "hess: tensor(4.1047)\n",
      "grad: tensor(2.9633, dtype=torch.float64)\n",
      "hess: tensor(46.3602)\n",
      "\taccuracy:87.890625\n",
      "\tloss: 0.011381\n",
      "grad: tensor(0.1374, dtype=torch.float64)\n",
      "hess: tensor(2.3988)\n",
      "grad: tensor(0.0887, dtype=torch.float64)\n",
      "hess: tensor(1.9255)\n",
      "grad: tensor(1.0136, dtype=torch.float64)\n",
      "hess: tensor(21.8256)\n",
      "grad: tensor(0.3837, dtype=torch.float64)\n",
      "hess: tensor(9.0754)\n",
      "grad: tensor(2.2126, dtype=torch.float64)\n",
      "hess: tensor(49.1442)\n",
      "grad: tensor(2.3331, dtype=torch.float64)\n",
      "hess: tensor(40.5757)\n",
      "grad: tensor(10.0514, dtype=torch.float64)\n",
      "hess: tensor(63.8272)\n",
      "grad: tensor(2.0669, dtype=torch.float64)\n",
      "hess: tensor(30.8073)\n",
      "grad: tensor(2.7458, dtype=torch.float64)\n",
      "hess: tensor(47.5946)\n",
      "grad: tensor(1.3853, dtype=torch.float64)\n",
      "hess: tensor(22.4435)\n",
      "grad: tensor(0.1794, dtype=torch.float64)\n",
      "hess: tensor(6.0029)\n",
      "grad: tensor(7.4924, dtype=torch.float64)\n",
      "hess: tensor(72.1472)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa333f14e8674937bdb1d30ec73aef6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(0.0006, dtype=torch.float64)\n",
      "hess: tensor(0.0185)\n",
      "\taccuracy:91.796875\n",
      "\tloss: 0.000018\n",
      "grad: tensor(0.4476, dtype=torch.float64)\n",
      "hess: tensor(6.4640)\n",
      "grad: tensor(14.8466, dtype=torch.float64)\n",
      "hess: tensor(149.6377)\n",
      "grad: tensor(2.8414, dtype=torch.float64)\n",
      "hess: tensor(39.8985)\n",
      "grad: tensor(0.3709, dtype=torch.float64)\n",
      "hess: tensor(8.8025)\n",
      "grad: tensor(0.0091, dtype=torch.float64)\n",
      "hess: tensor(0.2298)\n",
      "grad: tensor(0.0109, dtype=torch.float64)\n",
      "hess: tensor(0.3358)\n",
      "grad: tensor(0.1644, dtype=torch.float64)\n",
      "hess: tensor(3.7762)\n",
      "grad: tensor(0.0808, dtype=torch.float64)\n",
      "hess: tensor(1.9889)\n",
      "grad: tensor(0.0680, dtype=torch.float64)\n",
      "hess: tensor(1.2517)\n",
      "grad: tensor(0.3410, dtype=torch.float64)\n",
      "hess: tensor(3.6339)\n",
      "grad: tensor(0.1374, dtype=torch.float64)\n",
      "hess: tensor(3.4474)\n",
      "grad: tensor(2.2993, dtype=torch.float64)\n",
      "hess: tensor(38.9082)\n",
      "\taccuracy:87.890625\n",
      "\tloss: 0.008607\n",
      "grad: tensor(0.1049, dtype=torch.float64)\n",
      "hess: tensor(1.9189)\n",
      "grad: tensor(0.0746, dtype=torch.float64)\n",
      "hess: tensor(1.6889)\n",
      "grad: tensor(0.8611, dtype=torch.float64)\n",
      "hess: tensor(19.4059)\n",
      "grad: tensor(0.2894, dtype=torch.float64)\n",
      "hess: tensor(7.1209)\n",
      "grad: tensor(2.0805, dtype=torch.float64)\n",
      "hess: tensor(47.8966)\n",
      "grad: tensor(2.0977, dtype=torch.float64)\n",
      "hess: tensor(38.3992)\n",
      "grad: tensor(10.0300, dtype=torch.float64)\n",
      "hess: tensor(66.9039)\n",
      "grad: tensor(1.8953, dtype=torch.float64)\n",
      "hess: tensor(29.6454)\n",
      "grad: tensor(2.2938, dtype=torch.float64)\n",
      "hess: tensor(42.3169)\n",
      "grad: tensor(1.3391, dtype=torch.float64)\n",
      "hess: tensor(22.4603)\n",
      "grad: tensor(0.1074, dtype=torch.float64)\n",
      "hess: tensor(3.6948)\n",
      "grad: tensor(7.5509, dtype=torch.float64)\n",
      "hess: tensor(69.5511)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1599aadee0514022bcccd4e50a600838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(0.0003, dtype=torch.float64)\n",
      "hess: tensor(0.0102)\n",
      "\taccuracy:91.796875\n",
      "\tloss: 0.000009\n",
      "grad: tensor(0.4419, dtype=torch.float64)\n",
      "hess: tensor(6.6476)\n",
      "grad: tensor(14.0377, dtype=torch.float64)\n",
      "hess: tensor(146.5299)\n",
      "grad: tensor(2.6656, dtype=torch.float64)\n",
      "hess: tensor(39.1916)\n",
      "grad: tensor(0.3182, dtype=torch.float64)\n",
      "hess: tensor(7.8281)\n",
      "grad: tensor(0.0073, dtype=torch.float64)\n",
      "hess: tensor(0.1863)\n",
      "grad: tensor(0.0083, dtype=torch.float64)\n",
      "hess: tensor(0.2640)\n",
      "grad: tensor(0.1019, dtype=torch.float64)\n",
      "hess: tensor(2.3251)\n",
      "grad: tensor(0.0678, dtype=torch.float64)\n",
      "hess: tensor(1.7203)\n",
      "grad: tensor(0.0560, dtype=torch.float64)\n",
      "hess: tensor(1.0641)\n",
      "grad: tensor(0.3108, dtype=torch.float64)\n",
      "hess: tensor(3.3876)\n",
      "grad: tensor(0.1113, dtype=torch.float64)\n",
      "hess: tensor(2.8804)\n",
      "grad: tensor(1.7773, dtype=torch.float64)\n",
      "hess: tensor(31.9950)\n",
      "\taccuracy:88.28125\n",
      "\tloss: 0.007013\n",
      "grad: tensor(0.0869, dtype=torch.float64)\n",
      "hess: tensor(1.6514)\n",
      "grad: tensor(0.0658, dtype=torch.float64)\n",
      "hess: tensor(1.5367)\n",
      "grad: tensor(0.7442, dtype=torch.float64)\n",
      "hess: tensor(17.3452)\n",
      "grad: tensor(0.2276, dtype=torch.float64)\n",
      "hess: tensor(5.7826)\n",
      "grad: tensor(1.8634, dtype=torch.float64)\n",
      "hess: tensor(44.8279)\n",
      "grad: tensor(1.9264, dtype=torch.float64)\n",
      "hess: tensor(36.6605)\n",
      "grad: tensor(9.8907, dtype=torch.float64)\n",
      "hess: tensor(69.6321)\n",
      "grad: tensor(1.7466, dtype=torch.float64)\n",
      "hess: tensor(28.4904)\n",
      "grad: tensor(1.9125, dtype=torch.float64)\n",
      "hess: tensor(37.2238)\n",
      "grad: tensor(1.3186, dtype=torch.float64)\n",
      "hess: tensor(22.7431)\n",
      "grad: tensor(0.0652, dtype=torch.float64)\n",
      "hess: tensor(2.2819)\n",
      "grad: tensor(7.3500, dtype=torch.float64)\n",
      "hess: tensor(72.1337)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db751cab5de74d24bf46f74dc822c64b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(0.0002, dtype=torch.float64)\n",
      "hess: tensor(0.0059)\n",
      "\taccuracy:91.796875\n",
      "\tloss: 0.000005\n",
      "grad: tensor(0.4440, dtype=torch.float64)\n",
      "hess: tensor(6.9204)\n",
      "grad: tensor(12.8449, dtype=torch.float64)\n",
      "hess: tensor(141.7827)\n",
      "grad: tensor(2.4799, dtype=torch.float64)\n",
      "hess: tensor(38.1215)\n",
      "grad: tensor(0.2738, dtype=torch.float64)\n",
      "hess: tensor(6.9522)\n",
      "grad: tensor(0.0058, dtype=torch.float64)\n",
      "hess: tensor(0.1510)\n",
      "grad: tensor(0.0065, dtype=torch.float64)\n",
      "hess: tensor(0.2125)\n",
      "grad: tensor(0.0740, dtype=torch.float64)\n",
      "hess: tensor(1.7408)\n",
      "grad: tensor(0.0568, dtype=torch.float64)\n",
      "hess: tensor(1.4530)\n",
      "grad: tensor(0.0451, dtype=torch.float64)\n",
      "hess: tensor(0.8798)\n",
      "grad: tensor(0.2833, dtype=torch.float64)\n",
      "hess: tensor(3.1542)\n",
      "grad: tensor(0.0914, dtype=torch.float64)\n",
      "hess: tensor(2.4326)\n",
      "grad: tensor(1.3704, dtype=torch.float64)\n",
      "hess: tensor(24.5452)\n",
      "\taccuracy:88.28125\n",
      "\tloss: 0.005864\n",
      "grad: tensor(0.0732, dtype=torch.float64)\n",
      "hess: tensor(1.4418)\n",
      "grad: tensor(0.0588, dtype=torch.float64)\n",
      "hess: tensor(1.4121)\n",
      "grad: tensor(0.6509, dtype=torch.float64)\n",
      "hess: tensor(15.6525)\n",
      "grad: tensor(0.1744, dtype=torch.float64)\n",
      "hess: tensor(4.5585)\n",
      "grad: tensor(1.6348, dtype=torch.float64)\n",
      "hess: tensor(41.1983)\n",
      "grad: tensor(1.7705, dtype=torch.float64)\n",
      "hess: tensor(35.0386)\n",
      "grad: tensor(9.6809, dtype=torch.float64)\n",
      "hess: tensor(72.2728)\n",
      "grad: tensor(1.6252, dtype=torch.float64)\n",
      "hess: tensor(27.5332)\n",
      "grad: tensor(1.5993, dtype=torch.float64)\n",
      "hess: tensor(32.7259)\n",
      "grad: tensor(1.3037, dtype=torch.float64)\n",
      "hess: tensor(23.0909)\n",
      "grad: tensor(0.0221, dtype=torch.float64)\n",
      "hess: tensor(0.5920)\n",
      "grad: tensor(1.5561, dtype=torch.float64)\n",
      "hess: tensor(24.5577)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dba91ef5c8f540e79e46e94729d35a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(0.0001, dtype=torch.float64)\n",
      "hess: tensor(0.0036)\n",
      "\taccuracy:91.015625\n",
      "\tloss: 0.000003\n",
      "grad: tensor(0.4910, dtype=torch.float64)\n",
      "hess: tensor(6.2593)\n",
      "grad: tensor(12.1699, dtype=torch.float64)\n",
      "hess: tensor(132.2207)\n",
      "grad: tensor(0.0581, dtype=torch.float64)\n",
      "hess: tensor(1.6791)\n",
      "grad: tensor(0.0048, dtype=torch.float64)\n",
      "hess: tensor(0.1293)\n",
      "grad: tensor(6.9632, dtype=torch.float64)\n",
      "hess: tensor(104.2412)\n",
      "grad: tensor(0.0773, dtype=torch.float64)\n",
      "hess: tensor(2.2879)\n",
      "grad: tensor(0.4289, dtype=torch.float64)\n",
      "hess: tensor(10.5822)\n",
      "grad: tensor(0.2557, dtype=torch.float64)\n",
      "hess: tensor(2.9074)\n",
      "grad: tensor(1.0138, dtype=torch.float64)\n",
      "hess: tensor(15.1523)\n",
      "grad: tensor(0.0826, dtype=torch.float64)\n",
      "hess: tensor(1.5037)\n",
      "\taccuracy:88.28125\n",
      "\tloss: 0.004871\n",
      "grad: tensor(5.9145, dtype=torch.float64)\n",
      "hess: tensor(58.9840)\n",
      "grad: tensor(0.5534, dtype=torch.float64)\n",
      "hess: tensor(13.7032)\n",
      "grad: tensor(12.4398, dtype=torch.float64)\n",
      "hess: tensor(84.9129)\n",
      "grad: tensor(0.6863, dtype=torch.float64)\n",
      "hess: tensor(15.5742)\n",
      "grad: tensor(0.0415, dtype=torch.float64)\n",
      "hess: tensor(0.7486)\n",
      "grad: tensor(1.5441, dtype=torch.float64)\n",
      "hess: tensor(26.9869)\n",
      "grad: tensor(6.9510, dtype=torch.float64)\n",
      "hess: tensor(110.4374)\n",
      "grad: tensor(0.0184, dtype=torch.float64)\n",
      "hess: tensor(0.5030)\n",
      "grad: tensor(1.4163, dtype=torch.float64)\n",
      "hess: tensor(23.1440)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65897c3d8bde45c289421e91a11806f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(6.2293e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0022)\n",
      "\taccuracy:91.40625\n",
      "\tloss: 0.000002\n",
      "grad: tensor(0.4578, dtype=torch.float64)\n",
      "hess: tensor(5.9837)\n",
      "grad: tensor(11.4830, dtype=torch.float64)\n",
      "hess: tensor(128.8590)\n",
      "grad: tensor(0.0420, dtype=torch.float64)\n",
      "hess: tensor(1.2324)\n",
      "grad: tensor(0.0040, dtype=torch.float64)\n",
      "hess: tensor(0.1094)\n",
      "grad: tensor(6.4777, dtype=torch.float64)\n",
      "hess: tensor(101.7603)\n",
      "grad: tensor(0.0603, dtype=torch.float64)\n",
      "hess: tensor(1.8255)\n",
      "grad: tensor(0.3959, dtype=torch.float64)\n",
      "hess: tensor(9.9893)\n",
      "grad: tensor(0.2333, dtype=torch.float64)\n",
      "hess: tensor(2.7062)\n",
      "grad: tensor(0.9815, dtype=torch.float64)\n",
      "hess: tensor(15.0892)\n",
      "grad: tensor(0.0725, dtype=torch.float64)\n",
      "hess: tensor(1.3582)\n",
      "\taccuracy:88.671875\n",
      "\tloss: 0.004159\n",
      "grad: tensor(6.0410, dtype=torch.float64)\n",
      "hess: tensor(61.6281)\n",
      "grad: tensor(0.4990, dtype=torch.float64)\n",
      "hess: tensor(12.7019)\n",
      "grad: tensor(12.7268, dtype=torch.float64)\n",
      "hess: tensor(88.6700)\n",
      "grad: tensor(0.5407, dtype=torch.float64)\n",
      "hess: tensor(12.1869)\n",
      "grad: tensor(0.0412, dtype=torch.float64)\n",
      "hess: tensor(0.7612)\n",
      "grad: tensor(1.5047, dtype=torch.float64)\n",
      "hess: tensor(26.9852)\n",
      "grad: tensor(6.7763, dtype=torch.float64)\n",
      "hess: tensor(110.6501)\n",
      "grad: tensor(0.0149, dtype=torch.float64)\n",
      "hess: tensor(0.4167)\n",
      "grad: tensor(1.3409, dtype=torch.float64)\n",
      "hess: tensor(22.5342)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8082118facd34b32a5d87f8581c6cc75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(3.7940e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0013)\n",
      "\taccuracy:91.40625\n",
      "\tloss: 0.000001\n",
      "grad: tensor(0.3778, dtype=torch.float64)\n",
      "hess: tensor(4.6075)\n",
      "grad: tensor(10.8265, dtype=torch.float64)\n",
      "hess: tensor(126.8922)\n",
      "grad: tensor(0.0295, dtype=torch.float64)\n",
      "hess: tensor(0.8811)\n",
      "grad: tensor(0.0033, dtype=torch.float64)\n",
      "hess: tensor(0.0923)\n",
      "grad: tensor(6.1786, dtype=torch.float64)\n",
      "hess: tensor(100.4974)\n",
      "grad: tensor(0.0633, dtype=torch.float64)\n",
      "hess: tensor(1.9621)\n",
      "grad: tensor(0.3502, dtype=torch.float64)\n",
      "hess: tensor(9.0502)\n",
      "grad: tensor(0.2187, dtype=torch.float64)\n",
      "hess: tensor(2.5844)\n",
      "grad: tensor(0.9699, dtype=torch.float64)\n",
      "hess: tensor(15.3028)\n",
      "grad: tensor(0.0651, dtype=torch.float64)\n",
      "hess: tensor(1.2541)\n",
      "\taccuracy:88.28125\n",
      "\tloss: 0.003618\n",
      "grad: tensor(6.0345, dtype=torch.float64)\n",
      "hess: tensor(63.3871)\n",
      "grad: tensor(0.4544, dtype=torch.float64)\n",
      "hess: tensor(11.8690)\n",
      "grad: tensor(12.9597, dtype=torch.float64)\n",
      "hess: tensor(92.3549)\n",
      "grad: tensor(0.4366, dtype=torch.float64)\n",
      "hess: tensor(10.1899)\n",
      "grad: tensor(0.0457, dtype=torch.float64)\n",
      "hess: tensor(0.8942)\n",
      "grad: tensor(1.4160, dtype=torch.float64)\n",
      "hess: tensor(26.1513)\n",
      "grad: tensor(6.5250, dtype=torch.float64)\n",
      "hess: tensor(110.0527)\n",
      "grad: tensor(0.0127, dtype=torch.float64)\n",
      "hess: tensor(0.3632)\n",
      "grad: tensor(1.2799, dtype=torch.float64)\n",
      "hess: tensor(22.0444)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0685839837d54b488b31f99a53818f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(2.3460e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0009)\n",
      "\taccuracy:91.40625\n",
      "\tloss: 0.000001\n",
      "grad: tensor(0.3602, dtype=torch.float64)\n",
      "hess: tensor(4.5016)\n",
      "grad: tensor(10.2558, dtype=torch.float64)\n",
      "hess: tensor(124.8490)\n",
      "grad: tensor(0.0213, dtype=torch.float64)\n",
      "hess: tensor(0.6459)\n",
      "grad: tensor(0.0029, dtype=torch.float64)\n",
      "hess: tensor(0.0805)\n",
      "grad: tensor(5.5646, dtype=torch.float64)\n",
      "hess: tensor(95.9778)\n",
      "grad: tensor(0.0586, dtype=torch.float64)\n",
      "hess: tensor(1.8528)\n",
      "grad: tensor(0.3182, dtype=torch.float64)\n",
      "hess: tensor(8.3968)\n",
      "grad: tensor(0.2005, dtype=torch.float64)\n",
      "hess: tensor(2.4093)\n",
      "grad: tensor(0.9658, dtype=torch.float64)\n",
      "hess: tensor(15.5895)\n",
      "grad: tensor(0.0582, dtype=torch.float64)\n",
      "hess: tensor(1.1500)\n",
      "\taccuracy:88.28125\n",
      "\tloss: 0.003143\n",
      "grad: tensor(5.9131, dtype=torch.float64)\n",
      "hess: tensor(64.2652)\n",
      "grad: tensor(0.4027, dtype=torch.float64)\n",
      "hess: tensor(10.7618)\n",
      "grad: tensor(13.1495, dtype=torch.float64)\n",
      "hess: tensor(95.6153)\n",
      "grad: tensor(0.3751, dtype=torch.float64)\n",
      "hess: tensor(8.9214)\n",
      "grad: tensor(0.0453, dtype=torch.float64)\n",
      "hess: tensor(0.9033)\n",
      "grad: tensor(1.3354, dtype=torch.float64)\n",
      "hess: tensor(25.3381)\n",
      "grad: tensor(6.3045, dtype=torch.float64)\n",
      "hess: tensor(109.4443)\n",
      "grad: tensor(0.0111, dtype=torch.float64)\n",
      "hess: tensor(0.3218)\n",
      "grad: tensor(1.2148, dtype=torch.float64)\n",
      "hess: tensor(21.3848)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "128224b736b4468687dfe71b5dfbb6ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(1.6099e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0006)\n",
      "\taccuracy:91.40625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.3747, dtype=torch.float64)\n",
      "hess: tensor(5.0761)\n",
      "grad: tensor(9.4823, dtype=torch.float64)\n",
      "hess: tensor(121.2702)\n",
      "grad: tensor(0.0150, dtype=torch.float64)\n",
      "hess: tensor(0.4598)\n",
      "grad: tensor(0.0024, dtype=torch.float64)\n",
      "hess: tensor(0.0699)\n",
      "grad: tensor(5.3129, dtype=torch.float64)\n",
      "hess: tensor(94.5986)\n",
      "grad: tensor(0.0538, dtype=torch.float64)\n",
      "hess: tensor(1.7354)\n",
      "grad: tensor(0.2904, dtype=torch.float64)\n",
      "hess: tensor(7.8193)\n",
      "grad: tensor(0.1875, dtype=torch.float64)\n",
      "hess: tensor(2.2956)\n",
      "grad: tensor(0.9632, dtype=torch.float64)\n",
      "hess: tensor(15.9161)\n",
      "grad: tensor(0.0518, dtype=torch.float64)\n",
      "hess: tensor(1.0488)\n",
      "\taccuracy:88.28125\n",
      "\tloss: 0.002722\n",
      "grad: tensor(5.8729, dtype=torch.float64)\n",
      "hess: tensor(65.5306)\n",
      "grad: tensor(0.3708, dtype=torch.float64)\n",
      "hess: tensor(10.1465)\n",
      "grad: tensor(13.4596, dtype=torch.float64)\n",
      "hess: tensor(99.0226)\n",
      "grad: tensor(0.2907, dtype=torch.float64)\n",
      "hess: tensor(7.1828)\n",
      "grad: tensor(0.0441, dtype=torch.float64)\n",
      "hess: tensor(0.8955)\n",
      "grad: tensor(1.2705, dtype=torch.float64)\n",
      "hess: tensor(24.7148)\n",
      "grad: tensor(6.2638, dtype=torch.float64)\n",
      "hess: tensor(110.2427)\n",
      "grad: tensor(0.0092, dtype=torch.float64)\n",
      "hess: tensor(0.2731)\n",
      "grad: tensor(6.8389, dtype=torch.float64)\n",
      "hess: tensor(78.9185)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "636b952fcc74463a9b9d9690b5850b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(1.2453e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0005)\n",
      "\taccuracy:91.796875\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.1984, dtype=torch.float64)\n",
      "hess: tensor(7.2624)\n",
      "grad: tensor(1.5960, dtype=torch.float64)\n",
      "hess: tensor(30.4332)\n",
      "grad: tensor(6.0210, dtype=torch.float64)\n",
      "hess: tensor(87.4867)\n",
      "grad: tensor(0.0023, dtype=torch.float64)\n",
      "hess: tensor(0.0836)\n",
      "grad: tensor(0.0412, dtype=torch.float64)\n",
      "hess: tensor(1.3561)\n",
      "grad: tensor(0.0175, dtype=torch.float64)\n",
      "hess: tensor(0.3931)\n",
      "grad: tensor(0.0452, dtype=torch.float64)\n",
      "hess: tensor(1.1978)\n",
      "grad: tensor(0.6419, dtype=torch.float64)\n",
      "hess: tensor(13.3573)\n",
      "\taccuracy:88.28125\n",
      "\tloss: 0.002317\n",
      "grad: tensor(0.0018, dtype=torch.float64)\n",
      "hess: tensor(0.0593)\n",
      "grad: tensor(0.3288, dtype=torch.float64)\n",
      "hess: tensor(9.2051)\n",
      "grad: tensor(0.0967, dtype=torch.float64)\n",
      "hess: tensor(3.5704)\n",
      "grad: tensor(0.9079, dtype=torch.float64)\n",
      "hess: tensor(20.5602)\n",
      "grad: tensor(0.0644, dtype=torch.float64)\n",
      "hess: tensor(2.3398)\n",
      "grad: tensor(0.6483, dtype=torch.float64)\n",
      "hess: tensor(15.8097)\n",
      "grad: tensor(0.0079, dtype=torch.float64)\n",
      "hess: tensor(0.2387)\n",
      "grad: tensor(7.2087, dtype=torch.float64)\n",
      "hess: tensor(78.0342)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "464d219e0a3d45c4a16f1af4bd891b93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(9.0501e-06, dtype=torch.float64)\n",
      "hess: tensor(0.0004)\n",
      "\taccuracy:91.40625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.1459, dtype=torch.float64)\n",
      "hess: tensor(5.3534)\n",
      "grad: tensor(1.5229, dtype=torch.float64)\n",
      "hess: tensor(29.8117)\n",
      "grad: tensor(5.4781, dtype=torch.float64)\n",
      "hess: tensor(84.1148)\n",
      "grad: tensor(0.0022, dtype=torch.float64)\n",
      "hess: tensor(0.0818)\n",
      "grad: tensor(0.0371, dtype=torch.float64)\n",
      "hess: tensor(1.2432)\n",
      "grad: tensor(0.0148, dtype=torch.float64)\n",
      "hess: tensor(0.3397)\n",
      "grad: tensor(0.0415, dtype=torch.float64)\n",
      "hess: tensor(1.1186)\n",
      "grad: tensor(0.5395, dtype=torch.float64)\n",
      "hess: tensor(11.5234)\n",
      "\taccuracy:88.671875\n",
      "\tloss: 0.001990\n",
      "grad: tensor(0.0014, dtype=torch.float64)\n",
      "hess: tensor(0.0493)\n",
      "grad: tensor(0.3062, dtype=torch.float64)\n",
      "hess: tensor(8.7621)\n",
      "grad: tensor(0.0854, dtype=torch.float64)\n",
      "hess: tensor(3.2047)\n",
      "grad: tensor(0.7988, dtype=torch.float64)\n",
      "hess: tensor(18.5407)\n",
      "grad: tensor(0.0421, dtype=torch.float64)\n",
      "hess: tensor(1.5634)\n",
      "grad: tensor(0.5928, dtype=torch.float64)\n",
      "hess: tensor(14.7598)\n",
      "grad: tensor(0.0068, dtype=torch.float64)\n",
      "hess: tensor(0.2081)\n",
      "grad: tensor(6.8279, dtype=torch.float64)\n",
      "hess: tensor(77.1345)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "401894cefdae4fba8e3b721f91cd4fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(5.8837e-06, dtype=torch.float64)\n",
      "hess: tensor(0.0003)\n",
      "\taccuracy:91.796875\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.1233, dtype=torch.float64)\n",
      "hess: tensor(4.7169)\n",
      "grad: tensor(1.4768, dtype=torch.float64)\n",
      "hess: tensor(29.5663)\n",
      "grad: tensor(4.7817, dtype=torch.float64)\n",
      "hess: tensor(78.5743)\n",
      "grad: tensor(0.0018, dtype=torch.float64)\n",
      "hess: tensor(0.0682)\n",
      "grad: tensor(0.0368, dtype=torch.float64)\n",
      "hess: tensor(1.2547)\n",
      "grad: tensor(0.0118, dtype=torch.float64)\n",
      "hess: tensor(0.2760)\n",
      "grad: tensor(0.0370, dtype=torch.float64)\n",
      "hess: tensor(1.0124)\n",
      "grad: tensor(0.4734, dtype=torch.float64)\n",
      "hess: tensor(10.2912)\n",
      "\taccuracy:88.28125\n",
      "\tloss: 0.001734\n",
      "grad: tensor(0.0011, dtype=torch.float64)\n",
      "hess: tensor(0.0397)\n",
      "grad: tensor(0.2737, dtype=torch.float64)\n",
      "hess: tensor(7.9889)\n",
      "grad: tensor(0.0778, dtype=torch.float64)\n",
      "hess: tensor(3.0010)\n",
      "grad: tensor(0.7299, dtype=torch.float64)\n",
      "hess: tensor(17.2920)\n",
      "grad: tensor(0.0337, dtype=torch.float64)\n",
      "hess: tensor(1.2742)\n",
      "grad: tensor(0.5300, dtype=torch.float64)\n",
      "hess: tensor(13.5000)\n",
      "grad: tensor(0.0063, dtype=torch.float64)\n",
      "hess: tensor(0.1980)\n",
      "grad: tensor(6.6101, dtype=torch.float64)\n",
      "hess: tensor(77.4060)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fb66a6415434e8182a43773e0420d25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(5.3314e-06, dtype=torch.float64)\n",
      "hess: tensor(0.0002)\n",
      "\taccuracy:91.796875\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.0994, dtype=torch.float64)\n",
      "hess: tensor(3.7890)\n",
      "grad: tensor(1.3657, dtype=torch.float64)\n",
      "hess: tensor(28.1425)\n",
      "grad: tensor(4.4730, dtype=torch.float64)\n",
      "hess: tensor(76.5511)\n",
      "grad: tensor(0.0017, dtype=torch.float64)\n",
      "hess: tensor(0.0641)\n",
      "grad: tensor(0.0282, dtype=torch.float64)\n",
      "hess: tensor(0.9772)\n",
      "grad: tensor(0.0109, dtype=torch.float64)\n",
      "hess: tensor(0.2598)\n",
      "grad: tensor(0.0347, dtype=torch.float64)\n",
      "hess: tensor(0.9660)\n",
      "grad: tensor(0.3988, dtype=torch.float64)\n",
      "hess: tensor(8.8837)\n",
      "\taccuracy:88.28125\n",
      "\tloss: 0.001476\n",
      "grad: tensor(0.0010, dtype=torch.float64)\n",
      "hess: tensor(0.0352)\n",
      "grad: tensor(0.2512, dtype=torch.float64)\n",
      "hess: tensor(6.9588)\n",
      "grad: tensor(0.0711, dtype=torch.float64)\n",
      "hess: tensor(2.7463)\n",
      "grad: tensor(0.6293, dtype=torch.float64)\n",
      "hess: tensor(15.2743)\n",
      "grad: tensor(0.0227, dtype=torch.float64)\n",
      "hess: tensor(0.8744)\n",
      "grad: tensor(0.4949, dtype=torch.float64)\n",
      "hess: tensor(12.8243)\n",
      "grad: tensor(0.0057, dtype=torch.float64)\n",
      "hess: tensor(0.1798)\n",
      "grad: tensor(5.7966, dtype=torch.float64)\n",
      "hess: tensor(76.5311)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f655c6c7c241b68aa4242012c4a709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(4.9676e-06, dtype=torch.float64)\n",
      "hess: tensor(0.0002)\n",
      "\taccuracy:91.40625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.0856, dtype=torch.float64)\n",
      "hess: tensor(3.3197)\n",
      "grad: tensor(1.3240, dtype=torch.float64)\n",
      "hess: tensor(27.8469)\n",
      "grad: tensor(3.8496, dtype=torch.float64)\n",
      "hess: tensor(68.4986)\n",
      "grad: tensor(0.0014, dtype=torch.float64)\n",
      "hess: tensor(0.0546)\n",
      "grad: tensor(0.0268, dtype=torch.float64)\n",
      "hess: tensor(0.9445)\n",
      "grad: tensor(0.0087, dtype=torch.float64)\n",
      "hess: tensor(0.2114)\n",
      "grad: tensor(0.0323, dtype=torch.float64)\n",
      "hess: tensor(0.9121)\n",
      "grad: tensor(0.3476, dtype=torch.float64)\n",
      "hess: tensor(7.8757)\n",
      "\taccuracy:88.671875\n",
      "\tloss: 0.001300\n",
      "grad: tensor(0.0008, dtype=torch.float64)\n",
      "hess: tensor(0.0280)\n",
      "grad: tensor(0.2359, dtype=torch.float64)\n",
      "hess: tensor(6.9675)\n",
      "grad: tensor(0.0661, dtype=torch.float64)\n",
      "hess: tensor(2.6185)\n",
      "grad: tensor(0.5791, dtype=torch.float64)\n",
      "hess: tensor(14.3411)\n",
      "grad: tensor(0.0180, dtype=torch.float64)\n",
      "hess: tensor(0.7039)\n",
      "grad: tensor(0.4437, dtype=torch.float64)\n",
      "hess: tensor(11.7236)\n",
      "grad: tensor(0.0052, dtype=torch.float64)\n",
      "hess: tensor(0.1688)\n",
      "grad: tensor(6.2036, dtype=torch.float64)\n",
      "hess: tensor(76.3523)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c4996bc076c4359a41cfaa2d2945afa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(4.6742e-06, dtype=torch.float64)\n",
      "hess: tensor(0.0002)\n",
      "\taccuracy:91.015625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.0735, dtype=torch.float64)\n",
      "hess: tensor(2.8972)\n",
      "grad: tensor(1.2015, dtype=torch.float64)\n",
      "hess: tensor(26.0081)\n",
      "grad: tensor(3.6872, dtype=torch.float64)\n",
      "hess: tensor(67.5667)\n",
      "grad: tensor(0.0013, dtype=torch.float64)\n",
      "hess: tensor(0.0508)\n",
      "grad: tensor(0.0203, dtype=torch.float64)\n",
      "hess: tensor(0.7237)\n",
      "grad: tensor(0.0079, dtype=torch.float64)\n",
      "hess: tensor(0.1958)\n",
      "grad: tensor(0.0297, dtype=torch.float64)\n",
      "hess: tensor(0.8491)\n",
      "grad: tensor(0.3131, dtype=torch.float64)\n",
      "hess: tensor(7.2425)\n",
      "\taccuracy:88.28125\n",
      "\tloss: 0.001122\n",
      "grad: tensor(0.0007, dtype=torch.float64)\n",
      "hess: tensor(0.0243)\n",
      "grad: tensor(0.2121, dtype=torch.float64)\n",
      "hess: tensor(6.3744)\n",
      "grad: tensor(0.0593, dtype=torch.float64)\n",
      "hess: tensor(2.3809)\n",
      "grad: tensor(0.5116, dtype=torch.float64)\n",
      "hess: tensor(12.8907)\n",
      "grad: tensor(0.1340, dtype=torch.float64)\n",
      "hess: tensor(4.8640)\n",
      "grad: tensor(0.4296, dtype=torch.float64)\n",
      "hess: tensor(11.5280)\n",
      "grad: tensor(0.0153, dtype=torch.float64)\n",
      "hess: tensor(0.5409)\n",
      "grad: tensor(0.1548, dtype=torch.float64)\n",
      "hess: tensor(4.5284)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9fb2bc9682643769ce93bf83d9f56f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(4.5087e-06, dtype=torch.float64)\n",
      "hess: tensor(0.0002)\n",
      "\taccuracy:91.015625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.0711, dtype=torch.float64)\n",
      "hess: tensor(1.7666)\n",
      "grad: tensor(8.5918, dtype=torch.float64)\n",
      "hess: tensor(84.4208)\n",
      "grad: tensor(0.3713, dtype=torch.float64)\n",
      "hess: tensor(5.8301)\n",
      "grad: tensor(0.0044, dtype=torch.float64)\n",
      "hess: tensor(0.1311)\n",
      "grad: tensor(0.1915, dtype=torch.float64)\n",
      "hess: tensor(5.8307)\n",
      "grad: tensor(0.0282, dtype=torch.float64)\n",
      "hess: tensor(0.8161)\n",
      "grad: tensor(0.0302, dtype=torch.float64)\n",
      "hess: tensor(1.0191)\n",
      "\taccuracy:88.28125\n",
      "\tloss: 0.000932\n",
      "grad: tensor(0.0270, dtype=torch.float64)\n",
      "hess: tensor(0.8078)\n",
      "grad: tensor(1.6562, dtype=torch.float64)\n",
      "hess: tensor(43.1041)\n",
      "grad: tensor(0.0958, dtype=torch.float64)\n",
      "hess: tensor(2.7267)\n",
      "grad: tensor(0.1200, dtype=torch.float64)\n",
      "hess: tensor(4.4183)\n",
      "grad: tensor(0.4021, dtype=torch.float64)\n",
      "hess: tensor(10.9837)\n",
      "grad: tensor(0.0131, dtype=torch.float64)\n",
      "hess: tensor(0.4644)\n",
      "grad: tensor(0.1432, dtype=torch.float64)\n",
      "hess: tensor(4.2465)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c6fe959cdf4754a50a11f58cd0f2c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(4.3802e-06, dtype=torch.float64)\n",
      "hess: tensor(0.0002)\n",
      "\taccuracy:91.015625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.0669, dtype=torch.float64)\n",
      "hess: tensor(1.6842)\n",
      "grad: tensor(8.1681, dtype=torch.float64)\n",
      "hess: tensor(83.3781)\n",
      "grad: tensor(0.3643, dtype=torch.float64)\n",
      "hess: tensor(5.7994)\n",
      "grad: tensor(0.0037, dtype=torch.float64)\n",
      "hess: tensor(0.1115)\n",
      "grad: tensor(0.1860, dtype=torch.float64)\n",
      "hess: tensor(5.7461)\n",
      "grad: tensor(0.0263, dtype=torch.float64)\n",
      "hess: tensor(0.7726)\n",
      "grad: tensor(0.0259, dtype=torch.float64)\n",
      "hess: tensor(0.8886)\n",
      "\taccuracy:88.671875\n",
      "\tloss: 0.000815\n",
      "grad: tensor(0.0256, dtype=torch.float64)\n",
      "hess: tensor(0.7770)\n",
      "grad: tensor(1.6150, dtype=torch.float64)\n",
      "hess: tensor(42.8860)\n",
      "grad: tensor(0.0839, dtype=torch.float64)\n",
      "hess: tensor(2.4262)\n",
      "grad: tensor(0.1137, dtype=torch.float64)\n",
      "hess: tensor(4.3301)\n",
      "grad: tensor(0.3677, dtype=torch.float64)\n",
      "hess: tensor(10.2130)\n",
      "grad: tensor(0.0123, dtype=torch.float64)\n",
      "hess: tensor(0.4396)\n",
      "grad: tensor(0.1377, dtype=torch.float64)\n",
      "hess: tensor(4.1254)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "984fe9a7f343494da910a4545dd9968a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(1.8538e-06, dtype=torch.float64)\n",
      "hess: tensor(0.0001)\n",
      "\taccuracy:90.625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.0593, dtype=torch.float64)\n",
      "hess: tensor(1.5085)\n",
      "grad: tensor(8.0308, dtype=torch.float64)\n",
      "hess: tensor(83.6492)\n",
      "grad: tensor(0.3565, dtype=torch.float64)\n",
      "hess: tensor(5.7524)\n",
      "grad: tensor(0.0031, dtype=torch.float64)\n",
      "hess: tensor(0.0954)\n",
      "grad: tensor(0.1777, dtype=torch.float64)\n",
      "hess: tensor(5.5663)\n",
      "grad: tensor(0.0254, dtype=torch.float64)\n",
      "hess: tensor(0.7546)\n",
      "grad: tensor(0.0216, dtype=torch.float64)\n",
      "hess: tensor(0.7533)\n",
      "\taccuracy:88.671875\n",
      "\tloss: 0.000710\n",
      "grad: tensor(0.0237, dtype=torch.float64)\n",
      "hess: tensor(0.7279)\n",
      "grad: tensor(1.5766, dtype=torch.float64)\n",
      "hess: tensor(42.7063)\n",
      "grad: tensor(0.0745, dtype=torch.float64)\n",
      "hess: tensor(2.1854)\n",
      "grad: tensor(0.1056, dtype=torch.float64)\n",
      "hess: tensor(4.0736)\n",
      "grad: tensor(0.3487, dtype=torch.float64)\n",
      "hess: tensor(9.8402)\n",
      "grad: tensor(0.0106, dtype=torch.float64)\n",
      "hess: tensor(0.3829)\n",
      "grad: tensor(0.1309, dtype=torch.float64)\n",
      "hess: tensor(3.9811)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5d221948efd463db8bcc10261c8543f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(1.4072e-06, dtype=torch.float64)\n",
      "hess: tensor(8.4611e-05)\n",
      "\taccuracy:90.625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.0569, dtype=torch.float64)\n",
      "hess: tensor(1.4662)\n",
      "grad: tensor(7.8922, dtype=torch.float64)\n",
      "hess: tensor(83.7503)\n",
      "grad: tensor(0.3934, dtype=torch.float64)\n",
      "hess: tensor(6.4052)\n",
      "grad: tensor(0.0025, dtype=torch.float64)\n",
      "hess: tensor(0.0775)\n",
      "grad: tensor(0.1758, dtype=torch.float64)\n",
      "hess: tensor(5.5773)\n",
      "grad: tensor(0.0238, dtype=torch.float64)\n",
      "hess: tensor(0.7176)\n",
      "grad: tensor(0.0197, dtype=torch.float64)\n",
      "hess: tensor(0.6971)\n",
      "\taccuracy:88.671875\n",
      "\tloss: 0.000596\n",
      "grad: tensor(0.0222, dtype=torch.float64)\n",
      "hess: tensor(0.6888)\n",
      "grad: tensor(1.5798, dtype=torch.float64)\n",
      "hess: tensor(43.5024)\n",
      "grad: tensor(0.0662, dtype=torch.float64)\n",
      "hess: tensor(1.9728)\n",
      "grad: tensor(0.0978, dtype=torch.float64)\n",
      "hess: tensor(3.8236)\n",
      "grad: tensor(0.3516, dtype=torch.float64)\n",
      "hess: tensor(10.3903)\n",
      "grad: tensor(0.0096, dtype=torch.float64)\n",
      "hess: tensor(0.3499)\n",
      "grad: tensor(0.1259, dtype=torch.float64)\n",
      "hess: tensor(3.8723)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "301d8e4f19d549b58731161f0c80dcfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(1.2325e-06, dtype=torch.float64)\n",
      "hess: tensor(7.1971e-05)\n",
      "\taccuracy:90.625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.0537, dtype=torch.float64)\n",
      "hess: tensor(1.4012)\n",
      "grad: tensor(7.3074, dtype=torch.float64)\n",
      "hess: tensor(81.0316)\n",
      "grad: tensor(0.3609, dtype=torch.float64)\n",
      "hess: tensor(5.9636)\n",
      "grad: tensor(0.0022, dtype=torch.float64)\n",
      "hess: tensor(0.0691)\n",
      "grad: tensor(0.1681, dtype=torch.float64)\n",
      "hess: tensor(5.4059)\n",
      "grad: tensor(0.0228, dtype=torch.float64)\n",
      "hess: tensor(0.6941)\n",
      "grad: tensor(0.0161, dtype=torch.float64)\n",
      "hess: tensor(0.5754)\n",
      "\taccuracy:88.671875\n",
      "\tloss: 0.000531\n",
      "grad: tensor(0.0212, dtype=torch.float64)\n",
      "hess: tensor(0.6650)\n",
      "grad: tensor(1.6398, dtype=torch.float64)\n",
      "hess: tensor(45.6310)\n",
      "grad: tensor(0.0603, dtype=torch.float64)\n",
      "hess: tensor(1.8191)\n",
      "grad: tensor(0.0923, dtype=torch.float64)\n",
      "hess: tensor(3.6377)\n",
      "grad: tensor(0.3410, dtype=torch.float64)\n",
      "hess: tensor(10.2164)\n",
      "grad: tensor(0.0087, dtype=torch.float64)\n",
      "hess: tensor(0.3183)\n",
      "grad: tensor(0.1226, dtype=torch.float64)\n",
      "hess: tensor(3.8148)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03cae616b24b4c82be3c50c9183e1adb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(1.0477e-06, dtype=torch.float64)\n",
      "hess: tensor(5.7547e-05)\n",
      "\taccuracy:90.625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.0509, dtype=torch.float64)\n",
      "hess: tensor(1.3434)\n",
      "grad: tensor(7.3010, dtype=torch.float64)\n",
      "hess: tensor(81.9415)\n",
      "grad: tensor(0.3866, dtype=torch.float64)\n",
      "hess: tensor(6.4574)\n",
      "grad: tensor(0.0017, dtype=torch.float64)\n",
      "hess: tensor(0.0564)\n",
      "grad: tensor(0.1598, dtype=torch.float64)\n",
      "hess: tensor(5.2057)\n",
      "grad: tensor(0.0222, dtype=torch.float64)\n",
      "hess: tensor(0.6851)\n",
      "grad: tensor(0.0136, dtype=torch.float64)\n",
      "hess: tensor(0.4938)\n",
      "\taccuracy:88.671875\n",
      "\tloss: 0.000456\n",
      "grad: tensor(0.0198, dtype=torch.float64)\n",
      "hess: tensor(0.6298)\n",
      "grad: tensor(1.6131, dtype=torch.float64)\n",
      "hess: tensor(45.7815)\n",
      "grad: tensor(0.0551, dtype=torch.float64)\n",
      "hess: tensor(1.6859)\n",
      "grad: tensor(0.0862, dtype=torch.float64)\n",
      "hess: tensor(3.4420)\n",
      "grad: tensor(0.3303, dtype=torch.float64)\n",
      "hess: tensor(10.0305)\n",
      "grad: tensor(0.0079, dtype=torch.float64)\n",
      "hess: tensor(0.2900)\n",
      "grad: tensor(0.1203, dtype=torch.float64)\n",
      "hess: tensor(3.7871)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66d5e290dda54eb88480571998538194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(9.4423e-07, dtype=torch.float64)\n",
      "hess: tensor(5.2876e-05)\n",
      "\taccuracy:89.84375\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.0472, dtype=torch.float64)\n",
      "hess: tensor(1.2597)\n",
      "grad: tensor(6.7906, dtype=torch.float64)\n",
      "hess: tensor(79.0881)\n",
      "grad: tensor(0.3640, dtype=torch.float64)\n",
      "hess: tensor(6.1514)\n",
      "grad: tensor(0.0015, dtype=torch.float64)\n",
      "hess: tensor(0.0497)\n",
      "grad: tensor(0.1550, dtype=torch.float64)\n",
      "hess: tensor(5.1121)\n",
      "grad: tensor(0.0599, dtype=torch.float64)\n",
      "hess: tensor(0.8811)\n",
      "grad: tensor(0.1319, dtype=torch.float64)\n",
      "hess: tensor(3.4571)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.000398\n",
      "grad: tensor(0.0192, dtype=torch.float64)\n",
      "hess: tensor(0.6154)\n",
      "grad: tensor(0.0069, dtype=torch.float64)\n",
      "hess: tensor(0.2395)\n",
      "grad: tensor(0.3218, dtype=torch.float64)\n",
      "hess: tensor(9.0809)\n",
      "grad: tensor(0.7629, dtype=torch.float64)\n",
      "hess: tensor(18.5771)\n",
      "grad: tensor(1.6221, dtype=torch.float64)\n",
      "hess: tensor(36.5300)\n",
      "grad: tensor(4.7710, dtype=torch.float64)\n",
      "hess: tensor(80.6860)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf09555aa46d4cdf8bb846372524aace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(8.1143e-07, dtype=torch.float64)\n",
      "hess: tensor(4.5481e-05)\n",
      "\taccuracy:89.84375\n",
      "\tloss: 0.000000\n",
      "grad: tensor(1.2433, dtype=torch.float64)\n",
      "hess: tensor(42.0706)\n",
      "grad: tensor(0.0176, dtype=torch.float64)\n",
      "hess: tensor(0.6127)\n",
      "grad: tensor(0.0006, dtype=torch.float64)\n",
      "hess: tensor(0.0264)\n",
      "grad: tensor(0.0212, dtype=torch.float64)\n",
      "hess: tensor(0.7470)\n",
      "grad: tensor(0.0551, dtype=torch.float64)\n",
      "hess: tensor(0.8212)\n",
      "grad: tensor(0.1113, dtype=torch.float64)\n",
      "hess: tensor(2.9617)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000345\n",
      "grad: tensor(0.0178, dtype=torch.float64)\n",
      "hess: tensor(0.5782)\n",
      "grad: tensor(0.0062, dtype=torch.float64)\n",
      "hess: tensor(0.2177)\n",
      "grad: tensor(0.3073, dtype=torch.float64)\n",
      "hess: tensor(8.7701)\n",
      "grad: tensor(0.7446, dtype=torch.float64)\n",
      "hess: tensor(18.3738)\n",
      "grad: tensor(1.6387, dtype=torch.float64)\n",
      "hess: tensor(37.1372)\n",
      "grad: tensor(4.5910, dtype=torch.float64)\n",
      "hess: tensor(79.3843)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06e43144250d4ad795dbfe8fa067f134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(7.0519e-07, dtype=torch.float64)\n",
      "hess: tensor(3.9777e-05)\n",
      "\taccuracy:89.84375\n",
      "\tloss: 0.000000\n",
      "grad: tensor(1.1481, dtype=torch.float64)\n",
      "hess: tensor(39.7071)\n",
      "grad: tensor(0.0166, dtype=torch.float64)\n",
      "hess: tensor(0.5805)\n",
      "grad: tensor(0.0006, dtype=torch.float64)\n",
      "hess: tensor(0.0237)\n",
      "grad: tensor(0.0209, dtype=torch.float64)\n",
      "hess: tensor(0.7457)\n",
      "grad: tensor(0.0503, dtype=torch.float64)\n",
      "hess: tensor(0.7557)\n",
      "grad: tensor(0.1031, dtype=torch.float64)\n",
      "hess: tensor(2.7787)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.000317\n",
      "grad: tensor(0.0167, dtype=torch.float64)\n",
      "hess: tensor(0.5483)\n",
      "grad: tensor(0.0055, dtype=torch.float64)\n",
      "hess: tensor(0.1930)\n",
      "grad: tensor(0.2696, dtype=torch.float64)\n",
      "hess: tensor(7.8159)\n",
      "grad: tensor(0.6915, dtype=torch.float64)\n",
      "hess: tensor(17.3580)\n",
      "grad: tensor(1.6502, dtype=torch.float64)\n",
      "hess: tensor(37.8012)\n",
      "grad: tensor(4.3902, dtype=torch.float64)\n",
      "hess: tensor(77.8052)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "191f3400fdd64f5ba47be95685b89184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(6.0998e-07, dtype=torch.float64)\n",
      "hess: tensor(3.4732e-05)\n",
      "\taccuracy:89.84375\n",
      "\tloss: 0.000000\n",
      "grad: tensor(1.0356, dtype=torch.float64)\n",
      "hess: tensor(36.6882)\n",
      "grad: tensor(0.0141, dtype=torch.float64)\n",
      "hess: tensor(0.5026)\n",
      "grad: tensor(0.0005, dtype=torch.float64)\n",
      "hess: tensor(0.0225)\n",
      "grad: tensor(0.0203, dtype=torch.float64)\n",
      "hess: tensor(0.7322)\n",
      "grad: tensor(0.0468, dtype=torch.float64)\n",
      "hess: tensor(0.7115)\n",
      "grad: tensor(0.0908, dtype=torch.float64)\n",
      "hess: tensor(2.4812)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.000267\n",
      "grad: tensor(0.0158, dtype=torch.float64)\n",
      "hess: tensor(0.5222)\n",
      "grad: tensor(0.0050, dtype=torch.float64)\n",
      "hess: tensor(0.1790)\n",
      "grad: tensor(0.2502, dtype=torch.float64)\n",
      "hess: tensor(7.3551)\n",
      "grad: tensor(0.6555, dtype=torch.float64)\n",
      "hess: tensor(16.7001)\n",
      "grad: tensor(1.6730, dtype=torch.float64)\n",
      "hess: tensor(38.5905)\n",
      "grad: tensor(4.1501, dtype=torch.float64)\n",
      "hess: tensor(75.5694)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aad0266dbe0f4348b75c3cab374e261e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(5.3054e-07, dtype=torch.float64)\n",
      "hess: tensor(3.0459e-05)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.9733, dtype=torch.float64)\n",
      "hess: tensor(35.0846)\n",
      "grad: tensor(0.0130, dtype=torch.float64)\n",
      "hess: tensor(0.4688)\n",
      "grad: tensor(0.0005, dtype=torch.float64)\n",
      "hess: tensor(0.0205)\n",
      "grad: tensor(0.0205, dtype=torch.float64)\n",
      "hess: tensor(0.7462)\n",
      "grad: tensor(0.0421, dtype=torch.float64)\n",
      "hess: tensor(0.6453)\n",
      "grad: tensor(0.0775, dtype=torch.float64)\n",
      "hess: tensor(2.1455)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.000246\n",
      "grad: tensor(0.0154, dtype=torch.float64)\n",
      "hess: tensor(0.5122)\n",
      "grad: tensor(0.0046, dtype=torch.float64)\n",
      "hess: tensor(0.1655)\n",
      "grad: tensor(0.2324, dtype=torch.float64)\n",
      "hess: tensor(6.9106)\n",
      "grad: tensor(0.6276, dtype=torch.float64)\n",
      "hess: tensor(16.1976)\n",
      "grad: tensor(1.6898, dtype=torch.float64)\n",
      "hess: tensor(39.2846)\n",
      "grad: tensor(3.9362, dtype=torch.float64)\n",
      "hess: tensor(73.4879)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8b84e00db60405a97a632493e73ecff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(4.6575e-07, dtype=torch.float64)\n",
      "hess: tensor(2.7041e-05)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.9024, dtype=torch.float64)\n",
      "hess: tensor(33.1288)\n",
      "grad: tensor(0.0118, dtype=torch.float64)\n",
      "hess: tensor(0.4321)\n",
      "grad: tensor(0.0005, dtype=torch.float64)\n",
      "hess: tensor(0.0207)\n",
      "grad: tensor(0.0198, dtype=torch.float64)\n",
      "hess: tensor(0.7280)\n",
      "grad: tensor(0.0398, dtype=torch.float64)\n",
      "hess: tensor(0.6155)\n",
      "grad: tensor(0.0712, dtype=torch.float64)\n",
      "hess: tensor(1.9943)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.000213\n",
      "grad: tensor(0.0144, dtype=torch.float64)\n",
      "hess: tensor(0.4828)\n",
      "grad: tensor(0.0043, dtype=torch.float64)\n",
      "hess: tensor(0.1542)\n",
      "grad: tensor(0.2118, dtype=torch.float64)\n",
      "hess: tensor(6.3848)\n",
      "grad: tensor(0.6305, dtype=torch.float64)\n",
      "hess: tensor(16.9306)\n",
      "grad: tensor(1.6689, dtype=torch.float64)\n",
      "hess: tensor(39.2217)\n",
      "grad: tensor(3.7281, dtype=torch.float64)\n",
      "hess: tensor(71.3691)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92ed526fcd464f65bedc183e5741b4a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(4.1553e-07, dtype=torch.float64)\n",
      "hess: tensor(2.4537e-05)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.8879, dtype=torch.float64)\n",
      "hess: tensor(32.9404)\n",
      "grad: tensor(0.0110, dtype=torch.float64)\n",
      "hess: tensor(0.4050)\n",
      "grad: tensor(0.0004, dtype=torch.float64)\n",
      "hess: tensor(0.0179)\n",
      "grad: tensor(0.0195, dtype=torch.float64)\n",
      "hess: tensor(0.7263)\n",
      "grad: tensor(0.0356, dtype=torch.float64)\n",
      "hess: tensor(0.5572)\n",
      "grad: tensor(0.0611, dtype=torch.float64)\n",
      "hess: tensor(1.7286)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.000196\n",
      "grad: tensor(0.0138, dtype=torch.float64)\n",
      "hess: tensor(0.4684)\n",
      "grad: tensor(0.0039, dtype=torch.float64)\n",
      "hess: tensor(0.1406)\n",
      "grad: tensor(0.2060, dtype=torch.float64)\n",
      "hess: tensor(6.2766)\n",
      "grad: tensor(0.5932, dtype=torch.float64)\n",
      "hess: tensor(16.1457)\n",
      "grad: tensor(1.6567, dtype=torch.float64)\n",
      "hess: tensor(39.3377)\n",
      "grad: tensor(3.5732, dtype=torch.float64)\n",
      "hess: tensor(69.8194)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db870231bdb42349c32ca755cfe2db3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(3.7522e-07, dtype=torch.float64)\n",
      "hess: tensor(2.2630e-05)\n",
      "\taccuracy:89.84375\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.8079, dtype=torch.float64)\n",
      "hess: tensor(30.5665)\n",
      "grad: tensor(0.0096, dtype=torch.float64)\n",
      "hess: tensor(0.3594)\n",
      "grad: tensor(0.0004, dtype=torch.float64)\n",
      "hess: tensor(0.0171)\n",
      "grad: tensor(0.0189, dtype=torch.float64)\n",
      "hess: tensor(0.7115)\n",
      "grad: tensor(0.0339, dtype=torch.float64)\n",
      "hess: tensor(0.5345)\n",
      "grad: tensor(0.0547, dtype=torch.float64)\n",
      "hess: tensor(1.5618)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000171\n",
      "grad: tensor(0.0130, dtype=torch.float64)\n",
      "hess: tensor(0.4426)\n",
      "grad: tensor(0.0035, dtype=torch.float64)\n",
      "hess: tensor(0.1278)\n",
      "grad: tensor(0.1879, dtype=torch.float64)\n",
      "hess: tensor(5.7940)\n",
      "grad: tensor(0.5633, dtype=torch.float64)\n",
      "hess: tensor(14.6394)\n",
      "grad: tensor(1.6372, dtype=torch.float64)\n",
      "hess: tensor(39.2423)\n",
      "grad: tensor(3.3710, dtype=torch.float64)\n",
      "hess: tensor(67.4768)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3ae28e2f1604e5497727e736c8c7455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(3.2470e-07, dtype=torch.float64)\n",
      "hess: tensor(1.9012e-05)\n",
      "\taccuracy:89.84375\n",
      "\tloss: 0.000000\n",
      "grad: tensor(1.6098, dtype=torch.float64)\n",
      "hess: tensor(31.5776)\n",
      "grad: tensor(1.6893, dtype=torch.float64)\n",
      "hess: tensor(44.3888)\n",
      "grad: tensor(7.5542e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0032)\n",
      "grad: tensor(0.0008, dtype=torch.float64)\n",
      "hess: tensor(0.0234)\n",
      "grad: tensor(0.6717, dtype=torch.float64)\n",
      "hess: tensor(15.6997)\n",
      "\taccuracy:88.671875\n",
      "\tloss: 0.000150\n",
      "grad: tensor(8.1000e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0032)\n",
      "grad: tensor(1.3479, dtype=torch.float64)\n",
      "hess: tensor(43.9940)\n",
      "grad: tensor(0.1823, dtype=torch.float64)\n",
      "hess: tensor(5.6395)\n",
      "grad: tensor(1.1668, dtype=torch.float64)\n",
      "hess: tensor(31.7425)\n",
      "grad: tensor(0.0016, dtype=torch.float64)\n",
      "hess: tensor(0.0632)\n",
      "grad: tensor(0.0421, dtype=torch.float64)\n",
      "hess: tensor(1.1489)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44fdfa7bf4e44b178aa4f7d294afeb67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(2.7396e-07, dtype=torch.float64)\n",
      "hess: tensor(1.7196e-05)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.000000\n",
      "grad: tensor(1.5429, dtype=torch.float64)\n",
      "hess: tensor(30.6720)\n",
      "grad: tensor(1.5927, dtype=torch.float64)\n",
      "hess: tensor(42.6472)\n",
      "grad: tensor(7.1055e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0030)\n",
      "grad: tensor(0.0007, dtype=torch.float64)\n",
      "hess: tensor(0.0204)\n",
      "grad: tensor(0.6407, dtype=torch.float64)\n",
      "hess: tensor(15.1311)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000144\n",
      "grad: tensor(7.6912e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0030)\n",
      "grad: tensor(1.3177, dtype=torch.float64)\n",
      "hess: tensor(43.5505)\n",
      "grad: tensor(0.1608, dtype=torch.float64)\n",
      "hess: tensor(5.0495)\n",
      "grad: tensor(1.0899, dtype=torch.float64)\n",
      "hess: tensor(29.3798)\n",
      "grad: tensor(0.0015, dtype=torch.float64)\n",
      "hess: tensor(0.0594)\n",
      "grad: tensor(0.0391, dtype=torch.float64)\n",
      "hess: tensor(1.0761)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22970a726800491d98b4a9c40ac237b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(2.4827e-07, dtype=torch.float64)\n",
      "hess: tensor(1.5706e-05)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.000000\n",
      "grad: tensor(1.4618, dtype=torch.float64)\n",
      "hess: tensor(29.4861)\n",
      "grad: tensor(1.5530, dtype=torch.float64)\n",
      "hess: tensor(42.1062)\n",
      "grad: tensor(6.7268e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0029)\n",
      "grad: tensor(0.0006, dtype=torch.float64)\n",
      "hess: tensor(0.0188)\n",
      "grad: tensor(0.6403, dtype=torch.float64)\n",
      "hess: tensor(15.2814)\n",
      "\taccuracy:88.671875\n",
      "\tloss: 0.000125\n",
      "grad: tensor(6.8922e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0028)\n",
      "grad: tensor(1.2165, dtype=torch.float64)\n",
      "hess: tensor(41.0682)\n",
      "grad: tensor(0.1456, dtype=torch.float64)\n",
      "hess: tensor(4.6176)\n",
      "grad: tensor(1.0367, dtype=torch.float64)\n",
      "hess: tensor(28.3779)\n",
      "grad: tensor(0.0014, dtype=torch.float64)\n",
      "hess: tensor(0.0530)\n",
      "grad: tensor(0.0375, dtype=torch.float64)\n",
      "hess: tensor(1.0403)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f3c2a2bd8542a892990bbd3f9aa58b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(2.1481e-07, dtype=torch.float64)\n",
      "hess: tensor(1.2670e-05)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.000000\n",
      "grad: tensor(1.3908, dtype=torch.float64)\n",
      "hess: tensor(28.4698)\n",
      "grad: tensor(1.5393, dtype=torch.float64)\n",
      "hess: tensor(42.1203)\n",
      "grad: tensor(6.2613e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0027)\n",
      "grad: tensor(0.0006, dtype=torch.float64)\n",
      "hess: tensor(0.0168)\n",
      "grad: tensor(0.6072, dtype=torch.float64)\n",
      "hess: tensor(14.6434)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000113\n",
      "grad: tensor(6.1259e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0025)\n",
      "grad: tensor(1.1793, dtype=torch.float64)\n",
      "hess: tensor(40.3146)\n",
      "grad: tensor(0.1391, dtype=torch.float64)\n",
      "hess: tensor(4.4455)\n",
      "grad: tensor(0.9971, dtype=torch.float64)\n",
      "hess: tensor(27.6522)\n",
      "grad: tensor(0.0012, dtype=torch.float64)\n",
      "hess: tensor(0.0486)\n",
      "grad: tensor(0.0360, dtype=torch.float64)\n",
      "hess: tensor(1.0064)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6773a454dc5e4855ba03c3ad4af281e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(1.8301e-07, dtype=torch.float64)\n",
      "hess: tensor(1.1062e-05)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(1.3444, dtype=torch.float64)\n",
      "hess: tensor(27.8266)\n",
      "grad: tensor(1.4595, dtype=torch.float64)\n",
      "hess: tensor(40.5925)\n",
      "grad: tensor(6.0641e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0026)\n",
      "grad: tensor(0.0005, dtype=torch.float64)\n",
      "hess: tensor(0.0151)\n",
      "grad: tensor(0.5991, dtype=torch.float64)\n",
      "hess: tensor(14.5802)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.000105\n",
      "grad: tensor(5.6760e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0023)\n",
      "grad: tensor(1.1321, dtype=torch.float64)\n",
      "hess: tensor(39.2272)\n",
      "grad: tensor(0.1272, dtype=torch.float64)\n",
      "hess: tensor(4.1025)\n",
      "grad: tensor(0.9330, dtype=torch.float64)\n",
      "hess: tensor(26.2766)\n",
      "grad: tensor(0.0012, dtype=torch.float64)\n",
      "hess: tensor(0.0461)\n",
      "grad: tensor(0.0339, dtype=torch.float64)\n",
      "hess: tensor(0.9562)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93fa985aa81846afb769d8d1922ed8a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(1.6653e-07, dtype=torch.float64)\n",
      "hess: tensor(1.0209e-05)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(1.2869, dtype=torch.float64)\n",
      "hess: tensor(26.9530)\n",
      "grad: tensor(1.4510, dtype=torch.float64)\n",
      "hess: tensor(40.6855)\n",
      "grad: tensor(5.7436e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0025)\n",
      "grad: tensor(0.0005, dtype=torch.float64)\n",
      "hess: tensor(0.0137)\n",
      "grad: tensor(0.5907, dtype=torch.float64)\n",
      "hess: tensor(14.4959)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.000093\n",
      "grad: tensor(5.2179e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0021)\n",
      "grad: tensor(1.0880, dtype=torch.float64)\n",
      "hess: tensor(38.1940)\n",
      "grad: tensor(0.1172, dtype=torch.float64)\n",
      "hess: tensor(3.8174)\n",
      "grad: tensor(0.8901, dtype=torch.float64)\n",
      "hess: tensor(25.3937)\n",
      "grad: tensor(0.0011, dtype=torch.float64)\n",
      "hess: tensor(0.0421)\n",
      "grad: tensor(0.0323, dtype=torch.float64)\n",
      "hess: tensor(0.9178)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae69b44ff114e4b97be5a123f49f806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(1.4355e-07, dtype=torch.float64)\n",
      "hess: tensor(8.6041e-06)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(1.2462, dtype=torch.float64)\n",
      "hess: tensor(26.3926)\n",
      "grad: tensor(1.3605, dtype=torch.float64)\n",
      "hess: tensor(38.8179)\n",
      "grad: tensor(5.6197e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0024)\n",
      "grad: tensor(0.0004, dtype=torch.float64)\n",
      "hess: tensor(0.0126)\n",
      "grad: tensor(0.5602, dtype=torch.float64)\n",
      "hess: tensor(13.8608)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.000089\n",
      "grad: tensor(4.5574e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0020)\n",
      "grad: tensor(1.0563, dtype=torch.float64)\n",
      "hess: tensor(37.4933)\n",
      "grad: tensor(0.1083, dtype=torch.float64)\n",
      "hess: tensor(3.5541)\n",
      "grad: tensor(0.8494, dtype=torch.float64)\n",
      "hess: tensor(24.5300)\n",
      "grad: tensor(0.0010, dtype=torch.float64)\n",
      "hess: tensor(0.0405)\n",
      "grad: tensor(0.0305, dtype=torch.float64)\n",
      "hess: tensor(0.8727)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7402e5c3ed2f4d20bf9f7e5156ac9e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(1.3119e-07, dtype=torch.float64)\n",
      "hess: tensor(7.9178e-06)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(1.1792, dtype=torch.float64)\n",
      "hess: tensor(25.2836)\n",
      "grad: tensor(1.4030, dtype=torch.float64)\n",
      "hess: tensor(40.0954)\n",
      "grad: tensor(5.2181e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0023)\n",
      "grad: tensor(0.0004, dtype=torch.float64)\n",
      "hess: tensor(0.0113)\n",
      "grad: tensor(0.5588, dtype=torch.float64)\n",
      "hess: tensor(13.9280)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.000079\n",
      "grad: tensor(4.4216e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0019)\n",
      "grad: tensor(1.0025, dtype=torch.float64)\n",
      "hess: tensor(36.0789)\n",
      "grad: tensor(0.1114, dtype=torch.float64)\n",
      "hess: tensor(3.6613)\n",
      "grad: tensor(0.8115, dtype=torch.float64)\n",
      "hess: tensor(23.7082)\n",
      "grad: tensor(0.0009, dtype=torch.float64)\n",
      "hess: tensor(0.0370)\n",
      "grad: tensor(0.0300, dtype=torch.float64)\n",
      "hess: tensor(0.8628)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc36a2cd53cf4ffda87615478d6f1087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(1.1189e-07, dtype=torch.float64)\n",
      "hess: tensor(6.5530e-06)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(1.1414, dtype=torch.float64)\n",
      "hess: tensor(24.7154)\n",
      "grad: tensor(1.3313, dtype=torch.float64)\n",
      "hess: tensor(38.6145)\n",
      "grad: tensor(4.7181e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0021)\n",
      "grad: tensor(0.0003, dtype=torch.float64)\n",
      "hess: tensor(0.0102)\n",
      "grad: tensor(0.0205, dtype=torch.float64)\n",
      "hess: tensor(0.3404)\n",
      "grad: tensor(0.0021, dtype=torch.float64)\n",
      "hess: tensor(0.0634)\n",
      "\taccuracy:89.84375\n",
      "\tloss: 0.000073\n",
      "grad: tensor(0.0947, dtype=torch.float64)\n",
      "hess: tensor(3.6044)\n",
      "grad: tensor(0.0171, dtype=torch.float64)\n",
      "hess: tensor(0.6110)\n",
      "grad: tensor(0.3954, dtype=torch.float64)\n",
      "hess: tensor(11.5540)\n",
      "grad: tensor(0.0009, dtype=torch.float64)\n",
      "hess: tensor(0.0344)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ddbf20769af47c3b2acc1692144f7ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(9.8407e-08, dtype=torch.float64)\n",
      "hess: tensor(5.8539e-06)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(2.2448, dtype=torch.float64)\n",
      "hess: tensor(61.7234)\n",
      "grad: tensor(3.8110e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0015)\n",
      "grad: tensor(0.0009, dtype=torch.float64)\n",
      "hess: tensor(0.0373)\n",
      "grad: tensor(0.0195, dtype=torch.float64)\n",
      "hess: tensor(0.3255)\n",
      "grad: tensor(0.0019, dtype=torch.float64)\n",
      "hess: tensor(0.0582)\n",
      "\taccuracy:89.84375\n",
      "\tloss: 0.000066\n",
      "grad: tensor(0.0894, dtype=torch.float64)\n",
      "hess: tensor(3.4309)\n",
      "grad: tensor(0.0156, dtype=torch.float64)\n",
      "hess: tensor(0.5610)\n",
      "grad: tensor(0.3785, dtype=torch.float64)\n",
      "hess: tensor(11.1565)\n",
      "grad: tensor(0.0008, dtype=torch.float64)\n",
      "hess: tensor(0.0325)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e9d2db8d8bd46a5b368e6c9d6e2e39e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(8.9034e-08, dtype=torch.float64)\n",
      "hess: tensor(5.3711e-06)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(2.1767, dtype=torch.float64)\n",
      "hess: tensor(60.5765)\n",
      "grad: tensor(3.3747e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0013)\n",
      "grad: tensor(0.0008, dtype=torch.float64)\n",
      "hess: tensor(0.0364)\n",
      "grad: tensor(0.0180, dtype=torch.float64)\n",
      "hess: tensor(0.3040)\n",
      "grad: tensor(0.0019, dtype=torch.float64)\n",
      "hess: tensor(0.0571)\n",
      "\taccuracy:89.84375\n",
      "\tloss: 0.000064\n",
      "grad: tensor(0.0863, dtype=torch.float64)\n",
      "hess: tensor(3.3264)\n",
      "grad: tensor(0.0153, dtype=torch.float64)\n",
      "hess: tensor(0.5540)\n",
      "grad: tensor(0.3706, dtype=torch.float64)\n",
      "hess: tensor(11.0002)\n",
      "grad: tensor(0.0007, dtype=torch.float64)\n",
      "hess: tensor(0.0302)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2068211f5494e29a8d69fed9982c069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(7.9316e-08, dtype=torch.float64)\n",
      "hess: tensor(4.8710e-06)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(2.2293, dtype=torch.float64)\n",
      "hess: tensor(62.0023)\n",
      "grad: tensor(2.8838e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0012)\n",
      "grad: tensor(0.0007, dtype=torch.float64)\n",
      "hess: tensor(0.0317)\n",
      "grad: tensor(0.0177, dtype=torch.float64)\n",
      "hess: tensor(0.2997)\n",
      "grad: tensor(0.0017, dtype=torch.float64)\n",
      "hess: tensor(0.0520)\n",
      "\taccuracy:89.84375\n",
      "\tloss: 0.000057\n",
      "grad: tensor(0.0850, dtype=torch.float64)\n",
      "hess: tensor(3.3009)\n",
      "grad: tensor(0.0145, dtype=torch.float64)\n",
      "hess: tensor(0.5279)\n",
      "grad: tensor(0.3516, dtype=torch.float64)\n",
      "hess: tensor(10.5235)\n",
      "grad: tensor(0.0007, dtype=torch.float64)\n",
      "hess: tensor(0.0281)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6fd69e057ed450195b2fd5918081f6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(7.0509e-08, dtype=torch.float64)\n",
      "hess: tensor(4.4072e-06)\n",
      "\taccuracy:88.671875\n",
      "\tloss: 0.000000\n",
      "grad: tensor(2.1379, dtype=torch.float64)\n",
      "hess: tensor(60.3447)\n",
      "grad: tensor(2.5601e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0010)\n",
      "grad: tensor(0.0007, dtype=torch.float64)\n",
      "hess: tensor(0.0320)\n",
      "grad: tensor(0.0164, dtype=torch.float64)\n",
      "hess: tensor(0.2809)\n",
      "grad: tensor(0.0016, dtype=torch.float64)\n",
      "hess: tensor(0.0505)\n",
      "\taccuracy:89.84375\n",
      "\tloss: 0.000055\n",
      "grad: tensor(0.0802, dtype=torch.float64)\n",
      "hess: tensor(3.1298)\n",
      "grad: tensor(0.0139, dtype=torch.float64)\n",
      "hess: tensor(0.5083)\n",
      "grad: tensor(0.3429, dtype=torch.float64)\n",
      "hess: tensor(10.3322)\n",
      "grad: tensor(0.0007, dtype=torch.float64)\n",
      "hess: tensor(0.0271)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68a9284dd94e4feab73b6789ca574add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(6.2452e-08, dtype=torch.float64)\n",
      "hess: tensor(3.9878e-06)\n",
      "\taccuracy:88.28125\n",
      "\tloss: 0.000000\n",
      "grad: tensor(2.1432, dtype=torch.float64)\n",
      "hess: tensor(60.7683)\n",
      "grad: tensor(2.4677e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0010)\n",
      "grad: tensor(0.0007, dtype=torch.float64)\n",
      "hess: tensor(0.0303)\n",
      "grad: tensor(0.0157, dtype=torch.float64)\n",
      "hess: tensor(0.2706)\n",
      "grad: tensor(0.0015, dtype=torch.float64)\n",
      "hess: tensor(0.0461)\n",
      "\taccuracy:89.84375\n",
      "\tloss: 0.000050\n",
      "grad: tensor(0.0782, dtype=torch.float64)\n",
      "hess: tensor(3.0777)\n",
      "grad: tensor(0.0135, dtype=torch.float64)\n",
      "hess: tensor(0.4964)\n",
      "grad: tensor(0.3307, dtype=torch.float64)\n",
      "hess: tensor(10.0333)\n",
      "grad: tensor(0.0006, dtype=torch.float64)\n",
      "hess: tensor(0.0254)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d78a656ea0412db0b2ca6e0e9c3081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(5.6514e-08, dtype=torch.float64)\n",
      "hess: tensor(3.6611e-06)\n",
      "\taccuracy:88.671875\n",
      "\tloss: 0.000000\n",
      "grad: tensor(1.9687, dtype=torch.float64)\n",
      "hess: tensor(57.1319)\n",
      "grad: tensor(2.0352e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0008)\n",
      "grad: tensor(0.0007, dtype=torch.float64)\n",
      "hess: tensor(0.0292)\n",
      "grad: tensor(0.0151, dtype=torch.float64)\n",
      "hess: tensor(0.2611)\n",
      "grad: tensor(0.0014, dtype=torch.float64)\n",
      "hess: tensor(0.0447)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.000048\n",
      "grad: tensor(0.0783, dtype=torch.float64)\n",
      "hess: tensor(3.0939)\n",
      "grad: tensor(0.0126, dtype=torch.float64)\n",
      "hess: tensor(0.4675)\n",
      "grad: tensor(0.3227, dtype=torch.float64)\n",
      "hess: tensor(9.8544)\n",
      "grad: tensor(0.0006, dtype=torch.float64)\n",
      "hess: tensor(0.0241)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49b48b9e285d490394d5872d8d953050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(5.0127e-08, dtype=torch.float64)\n",
      "hess: tensor(2.8854e-06)\n",
      "\taccuracy:88.671875\n",
      "\tloss: 0.000000\n",
      "grad: tensor(1.9771, dtype=torch.float64)\n",
      "hess: tensor(57.6282)\n",
      "grad: tensor(1.9096e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0008)\n",
      "grad: tensor(0.0006, dtype=torch.float64)\n",
      "hess: tensor(0.0275)\n",
      "grad: tensor(0.0144, dtype=torch.float64)\n",
      "hess: tensor(0.2501)\n",
      "grad: tensor(0.0013, dtype=torch.float64)\n",
      "hess: tensor(0.0415)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.000044\n",
      "grad: tensor(0.0716, dtype=torch.float64)\n",
      "hess: tensor(2.8441)\n",
      "grad: tensor(0.0118, dtype=torch.float64)\n",
      "hess: tensor(0.4397)\n",
      "grad: tensor(0.3106, dtype=torch.float64)\n",
      "hess: tensor(9.5482)\n",
      "grad: tensor(0.0005, dtype=torch.float64)\n",
      "hess: tensor(0.0223)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51e664694e114b68bb4d5ca8ee08f176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(4.5094e-08, dtype=torch.float64)\n",
      "hess: tensor(2.6048e-06)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(2.0029, dtype=torch.float64)\n",
      "hess: tensor(58.4790)\n",
      "grad: tensor(1.8466e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0007)\n",
      "grad: tensor(0.0006, dtype=torch.float64)\n",
      "hess: tensor(0.0266)\n",
      "grad: tensor(0.0135, dtype=torch.float64)\n",
      "hess: tensor(0.2357)\n",
      "grad: tensor(0.0013, dtype=torch.float64)\n",
      "hess: tensor(0.0400)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.000042\n",
      "grad: tensor(0.0669, dtype=torch.float64)\n",
      "hess: tensor(2.6716)\n",
      "grad: tensor(0.0111, dtype=torch.float64)\n",
      "hess: tensor(0.4136)\n",
      "grad: tensor(0.2965, dtype=torch.float64)\n",
      "hess: tensor(9.1766)\n",
      "grad: tensor(0.0005, dtype=torch.float64)\n",
      "hess: tensor(0.0223)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ad1de67723043aeaec27fd71f0ff79d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(3.8974e-08, dtype=torch.float64)\n",
      "hess: tensor(2.2584e-06)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.000000\n",
      "grad: tensor(1.9791, dtype=torch.float64)\n",
      "hess: tensor(58.1846)\n",
      "grad: tensor(1.4906e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0006)\n",
      "grad: tensor(0.0006, dtype=torch.float64)\n",
      "hess: tensor(0.0260)\n",
      "grad: tensor(0.0128, dtype=torch.float64)\n",
      "hess: tensor(0.2249)\n",
      "grad: tensor(0.0012, dtype=torch.float64)\n",
      "hess: tensor(0.0373)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.000039\n",
      "grad: tensor(0.0677, dtype=torch.float64)\n",
      "hess: tensor(2.7215)\n",
      "grad: tensor(0.0104, dtype=torch.float64)\n",
      "hess: tensor(0.3910)\n",
      "grad: tensor(0.2934, dtype=torch.float64)\n",
      "hess: tensor(9.1291)\n",
      "grad: tensor(0.0005, dtype=torch.float64)\n",
      "hess: tensor(0.0204)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e23a650edd3432282c289b7527e4c6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(3.5894e-08, dtype=torch.float64)\n",
      "hess: tensor(2.0871e-06)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(1.9878, dtype=torch.float64)\n",
      "hess: tensor(58.6555)\n",
      "grad: tensor(1.4342e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0006)\n",
      "grad: tensor(0.0005, dtype=torch.float64)\n",
      "hess: tensor(0.0238)\n",
      "grad: tensor(0.0073, dtype=torch.float64)\n",
      "hess: tensor(0.2557)\n",
      "grad: tensor(0.0001, dtype=torch.float64)\n",
      "hess: tensor(0.0056)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.000036\n",
      "grad: tensor(2.3005, dtype=torch.float64)\n",
      "hess: tensor(46.5263)\n",
      "grad: tensor(0.0040, dtype=torch.float64)\n",
      "hess: tensor(0.2085)\n",
      "grad: tensor(0.0139, dtype=torch.float64)\n",
      "hess: tensor(0.6803)\n",
      "grad: tensor(1.2370, dtype=torch.float64)\n",
      "hess: tensor(35.3252)\n",
      "grad: tensor(0.0200, dtype=torch.float64)\n",
      "hess: tensor(0.6093)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f83fef5e4e5f49bd8db91f991ec368a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(3.1421e-08, dtype=torch.float64)\n",
      "hess: tensor(1.8327e-06)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.0002, dtype=torch.float64)\n",
      "hess: tensor(0.0062)\n",
      "grad: tensor(4.7906, dtype=torch.float64)\n",
      "hess: tensor(106.0392)\n",
      "grad: tensor(0.0069, dtype=torch.float64)\n",
      "hess: tensor(0.2440)\n",
      "grad: tensor(0.0001, dtype=torch.float64)\n",
      "hess: tensor(0.0052)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.000034\n",
      "grad: tensor(2.2894, dtype=torch.float64)\n",
      "hess: tensor(46.8753)\n",
      "grad: tensor(0.0039, dtype=torch.float64)\n",
      "hess: tensor(0.2007)\n",
      "grad: tensor(0.0132, dtype=torch.float64)\n",
      "hess: tensor(0.6452)\n",
      "grad: tensor(1.2309, dtype=torch.float64)\n",
      "hess: tensor(35.3153)\n",
      "grad: tensor(0.0193, dtype=torch.float64)\n",
      "hess: tensor(0.5926)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1026701067cb4537acf43dce1370f14d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(2.8297e-08, dtype=torch.float64)\n",
      "hess: tensor(1.6551e-06)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.0002, dtype=torch.float64)\n",
      "hess: tensor(0.0057)\n",
      "grad: tensor(4.7318, dtype=torch.float64)\n",
      "hess: tensor(105.6490)\n",
      "grad: tensor(0.0064, dtype=torch.float64)\n",
      "hess: tensor(0.2274)\n",
      "grad: tensor(0.0001, dtype=torch.float64)\n",
      "hess: tensor(0.0047)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.000032\n",
      "grad: tensor(2.2681, dtype=torch.float64)\n",
      "hess: tensor(46.9829)\n",
      "grad: tensor(0.0037, dtype=torch.float64)\n",
      "hess: tensor(0.1923)\n",
      "grad: tensor(0.0125, dtype=torch.float64)\n",
      "hess: tensor(0.6153)\n",
      "grad: tensor(1.1977, dtype=torch.float64)\n",
      "hess: tensor(34.6399)\n",
      "grad: tensor(0.0190, dtype=torch.float64)\n",
      "hess: tensor(0.5833)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d66b4945924df19ea2ab225126c234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(2.4783e-08, dtype=torch.float64)\n",
      "hess: tensor(1.4543e-06)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.0001, dtype=torch.float64)\n",
      "hess: tensor(0.0052)\n",
      "grad: tensor(4.6598, dtype=torch.float64)\n",
      "hess: tensor(105.0817)\n",
      "grad: tensor(0.0061, dtype=torch.float64)\n",
      "hess: tensor(0.2169)\n",
      "grad: tensor(9.8339e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0044)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000030\n",
      "grad: tensor(2.1174, dtype=torch.float64)\n",
      "hess: tensor(46.7357)\n",
      "grad: tensor(0.0034, dtype=torch.float64)\n",
      "hess: tensor(0.1769)\n",
      "grad: tensor(0.0117, dtype=torch.float64)\n",
      "hess: tensor(0.5789)\n",
      "grad: tensor(1.1576, dtype=torch.float64)\n",
      "hess: tensor(33.7579)\n",
      "grad: tensor(0.0183, dtype=torch.float64)\n",
      "hess: tensor(0.5654)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a95a8ef599464bc8842db76b0474a920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(2.2448e-08, dtype=torch.float64)\n",
      "hess: tensor(1.3210e-06)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.0001, dtype=torch.float64)\n",
      "hess: tensor(0.0047)\n",
      "grad: tensor(4.7387, dtype=torch.float64)\n",
      "hess: tensor(106.1014)\n",
      "grad: tensor(0.0058, dtype=torch.float64)\n",
      "hess: tensor(0.2089)\n",
      "grad: tensor(8.8051e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0040)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000028\n",
      "grad: tensor(2.1929, dtype=torch.float64)\n",
      "hess: tensor(46.0995)\n",
      "grad: tensor(0.0030, dtype=torch.float64)\n",
      "hess: tensor(0.1576)\n",
      "grad: tensor(0.0111, dtype=torch.float64)\n",
      "hess: tensor(0.5533)\n",
      "grad: tensor(1.1392, dtype=torch.float64)\n",
      "hess: tensor(33.4185)\n",
      "grad: tensor(0.0178, dtype=torch.float64)\n",
      "hess: tensor(0.5512)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "322377bdc3ee433ab344a26fe8b81303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(2.0140e-08, dtype=torch.float64)\n",
      "hess: tensor(1.1882e-06)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.0001, dtype=torch.float64)\n",
      "hess: tensor(0.0044)\n",
      "grad: tensor(4.4220, dtype=torch.float64)\n",
      "hess: tensor(102.8636)\n",
      "grad: tensor(0.0057, dtype=torch.float64)\n",
      "hess: tensor(0.2037)\n",
      "grad: tensor(8.4035e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0038)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000027\n",
      "grad: tensor(2.0333, dtype=torch.float64)\n",
      "hess: tensor(45.6058)\n",
      "grad: tensor(0.0030, dtype=torch.float64)\n",
      "hess: tensor(0.1583)\n",
      "grad: tensor(0.0102, dtype=torch.float64)\n",
      "hess: tensor(0.5101)\n",
      "grad: tensor(1.1132, dtype=torch.float64)\n",
      "hess: tensor(32.8811)\n",
      "grad: tensor(0.0171, dtype=torch.float64)\n",
      "hess: tensor(0.5340)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ab2bdd85d204a4f9c1e5b9ab3a0c173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(1.8201e-08, dtype=torch.float64)\n",
      "hess: tensor(1.0767e-06)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.0001, dtype=torch.float64)\n",
      "hess: tensor(0.0040)\n",
      "grad: tensor(4.2351, dtype=torch.float64)\n",
      "hess: tensor(100.8134)\n",
      "grad: tensor(0.0052, dtype=torch.float64)\n",
      "hess: tensor(0.1895)\n",
      "grad: tensor(7.6595e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0034)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000025\n",
      "grad: tensor(2.0195, dtype=torch.float64)\n",
      "hess: tensor(45.5280)\n",
      "grad: tensor(0.0030, dtype=torch.float64)\n",
      "hess: tensor(0.1564)\n",
      "grad: tensor(0.0100, dtype=torch.float64)\n",
      "hess: tensor(0.4994)\n",
      "grad: tensor(1.0970, dtype=torch.float64)\n",
      "hess: tensor(32.5761)\n",
      "grad: tensor(0.0167, dtype=torch.float64)\n",
      "hess: tensor(0.5233)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c6f91593e1446baf53cc3b824c2d4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(1.5955e-08, dtype=torch.float64)\n",
      "hess: tensor(9.4656e-07)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.000000\n",
      "grad: tensor(9.7400e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0037)\n",
      "grad: tensor(4.2387, dtype=torch.float64)\n",
      "hess: tensor(101.0181)\n",
      "grad: tensor(0.0047, dtype=torch.float64)\n",
      "hess: tensor(0.1711)\n",
      "grad: tensor(6.4821e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0030)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000024\n",
      "grad: tensor(2.1300, dtype=torch.float64)\n",
      "hess: tensor(45.1171)\n",
      "grad: tensor(0.0028, dtype=torch.float64)\n",
      "hess: tensor(0.1476)\n",
      "grad: tensor(0.0092, dtype=torch.float64)\n",
      "hess: tensor(0.4635)\n",
      "grad: tensor(1.0721, dtype=torch.float64)\n",
      "hess: tensor(32.0531)\n",
      "grad: tensor(0.0163, dtype=torch.float64)\n",
      "hess: tensor(0.5120)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ff9c07717fb439a8f25491ea0d4f10d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(1.4283e-08, dtype=torch.float64)\n",
      "hess: tensor(8.4947e-07)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.000000\n",
      "grad: tensor(9.1347e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0035)\n",
      "grad: tensor(4.2067, dtype=torch.float64)\n",
      "hess: tensor(100.8916)\n",
      "grad: tensor(0.0047, dtype=torch.float64)\n",
      "hess: tensor(0.1726)\n",
      "grad: tensor(6.4142e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0030)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.000022\n",
      "grad: tensor(2.0780, dtype=torch.float64)\n",
      "hess: tensor(44.7843)\n",
      "grad: tensor(0.0026, dtype=torch.float64)\n",
      "hess: tensor(0.1382)\n",
      "grad: tensor(0.0087, dtype=torch.float64)\n",
      "hess: tensor(0.4358)\n",
      "grad: tensor(1.0483, dtype=torch.float64)\n",
      "hess: tensor(31.5264)\n",
      "grad: tensor(0.0159, dtype=torch.float64)\n",
      "hess: tensor(0.5008)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "060e9189ca0d49428ee792c0da78c6b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(1.2812e-08, dtype=torch.float64)\n",
      "hess: tensor(7.6384e-07)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.000000\n",
      "grad: tensor(8.3577e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0032)\n",
      "grad: tensor(4.2355, dtype=torch.float64)\n",
      "hess: tensor(101.4089)\n",
      "grad: tensor(0.0045, dtype=torch.float64)\n",
      "hess: tensor(0.1642)\n",
      "grad: tensor(5.7639e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0026)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000021\n",
      "grad: tensor(1.9109, dtype=torch.float64)\n",
      "hess: tensor(44.0179)\n",
      "grad: tensor(0.0024, dtype=torch.float64)\n",
      "hess: tensor(0.1282)\n",
      "grad: tensor(0.0085, dtype=torch.float64)\n",
      "hess: tensor(0.4285)\n",
      "grad: tensor(1.0249, dtype=torch.float64)\n",
      "hess: tensor(31.0169)\n",
      "grad: tensor(0.0155, dtype=torch.float64)\n",
      "hess: tensor(0.4885)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bee03409e894ee09bf42cf9677ea9be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(1.1900e-08, dtype=torch.float64)\n",
      "hess: tensor(7.1128e-07)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.000000\n",
      "grad: tensor(7.8793e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0030)\n",
      "grad: tensor(3.9399, dtype=torch.float64)\n",
      "hess: tensor(97.8527)\n",
      "grad: tensor(0.0001, dtype=torch.float64)\n",
      "hess: tensor(0.0053)\n",
      "grad: tensor(0.0001, dtype=torch.float64)\n",
      "hess: tensor(0.0034)\n",
      "grad: tensor(0.0097, dtype=torch.float64)\n",
      "hess: tensor(0.3217)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000020\n",
      "grad: tensor(0.0417, dtype=torch.float64)\n",
      "hess: tensor(1.7565)\n",
      "grad: tensor(0.0313, dtype=torch.float64)\n",
      "hess: tensor(1.1388)\n",
      "grad: tensor(0.1593, dtype=torch.float64)\n",
      "hess: tensor(6.4819)\n",
      "grad: tensor(1.0748, dtype=torch.float64)\n",
      "hess: tensor(30.7160)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8131615a154245008eb76b51a2b02fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(1.0449e-08, dtype=torch.float64)\n",
      "hess: tensor(6.2605e-07)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.2147, dtype=torch.float64)\n",
      "hess: tensor(7.4870)\n",
      "grad: tensor(0.0001, dtype=torch.float64)\n",
      "hess: tensor(0.0051)\n",
      "grad: tensor(9.9650e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0033)\n",
      "grad: tensor(0.0095, dtype=torch.float64)\n",
      "hess: tensor(0.3158)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000019\n",
      "grad: tensor(0.0408, dtype=torch.float64)\n",
      "hess: tensor(1.7265)\n",
      "grad: tensor(0.0288, dtype=torch.float64)\n",
      "hess: tensor(1.0533)\n",
      "grad: tensor(0.1540, dtype=torch.float64)\n",
      "hess: tensor(6.2932)\n",
      "grad: tensor(1.0547, dtype=torch.float64)\n",
      "hess: tensor(30.3030)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fca4e18e40064e329d4b82ce2997910e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(9.7031e-09, dtype=torch.float64)\n",
      "hess: tensor(5.8266e-07)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.2092, dtype=torch.float64)\n",
      "hess: tensor(7.3248)\n",
      "grad: tensor(0.0001, dtype=torch.float64)\n",
      "hess: tensor(0.0050)\n",
      "grad: tensor(9.3275e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0031)\n",
      "grad: tensor(0.0091, dtype=torch.float64)\n",
      "hess: tensor(0.3046)\n",
      "\taccuracy:89.84375\n",
      "\tloss: 0.000017\n",
      "grad: tensor(0.0398, dtype=torch.float64)\n",
      "hess: tensor(1.6930)\n",
      "grad: tensor(0.0272, dtype=torch.float64)\n",
      "hess: tensor(0.9965)\n",
      "grad: tensor(0.1516, dtype=torch.float64)\n",
      "hess: tensor(6.2263)\n",
      "grad: tensor(1.0211, dtype=torch.float64)\n",
      "hess: tensor(29.5380)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96793cb9e2e64b91bc4ea64f0c82c4d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(8.6422e-09, dtype=torch.float64)\n",
      "hess: tensor(5.2010e-07)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.2075, dtype=torch.float64)\n",
      "hess: tensor(7.2967)\n",
      "grad: tensor(9.8984e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0048)\n",
      "grad: tensor(9.0498e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0030)\n",
      "grad: tensor(0.0085, dtype=torch.float64)\n",
      "hess: tensor(0.2855)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.000016\n",
      "grad: tensor(0.0388, dtype=torch.float64)\n",
      "hess: tensor(1.6550)\n",
      "grad: tensor(0.0258, dtype=torch.float64)\n",
      "hess: tensor(0.9508)\n",
      "grad: tensor(0.1468, dtype=torch.float64)\n",
      "hess: tensor(6.0420)\n",
      "grad: tensor(0.9960, dtype=torch.float64)\n",
      "hess: tensor(28.9761)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "593743a660494d67bda942515b8e6f94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(7.9129e-09, dtype=torch.float64)\n",
      "hess: tensor(4.7735e-07)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.2024, dtype=torch.float64)\n",
      "hess: tensor(7.1468)\n",
      "grad: tensor(9.8003e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0048)\n",
      "grad: tensor(8.9582e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0030)\n",
      "grad: tensor(0.0082, dtype=torch.float64)\n",
      "hess: tensor(0.2770)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.000016\n",
      "grad: tensor(0.0360, dtype=torch.float64)\n",
      "hess: tensor(1.5419)\n",
      "grad: tensor(0.0257, dtype=torch.float64)\n",
      "hess: tensor(0.9479)\n",
      "grad: tensor(0.1458, dtype=torch.float64)\n",
      "hess: tensor(6.0362)\n",
      "grad: tensor(0.9808, dtype=torch.float64)\n",
      "hess: tensor(28.6659)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d60fbdfd0b346e0a192d5aac534f63d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(7.1988e-09, dtype=torch.float64)\n",
      "hess: tensor(4.3519e-07)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.1970, dtype=torch.float64)\n",
      "hess: tensor(6.9833)\n",
      "grad: tensor(9.4856e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0046)\n",
      "grad: tensor(8.4263e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0028)\n",
      "grad: tensor(0.0081, dtype=torch.float64)\n",
      "hess: tensor(0.2737)\n",
      "\taccuracy:89.84375\n",
      "\tloss: 0.000015\n",
      "grad: tensor(0.0355, dtype=torch.float64)\n",
      "hess: tensor(1.5263)\n",
      "grad: tensor(0.0236, dtype=torch.float64)\n",
      "hess: tensor(0.8740)\n",
      "grad: tensor(0.1411, dtype=torch.float64)\n",
      "hess: tensor(5.8565)\n",
      "grad: tensor(0.9627, dtype=torch.float64)\n",
      "hess: tensor(28.2712)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a9cdbad769043fea4d789aaf00f4b33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(6.5152e-09, dtype=torch.float64)\n",
      "hess: tensor(3.9466e-07)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.1934, dtype=torch.float64)\n",
      "hess: tensor(6.8779)\n",
      "grad: tensor(9.3678e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0045)\n",
      "grad: tensor(7.8809e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0027)\n",
      "grad: tensor(0.0077, dtype=torch.float64)\n",
      "hess: tensor(0.2595)\n",
      "\taccuracy:89.84375\n",
      "\tloss: 0.000014\n",
      "grad: tensor(0.0352, dtype=torch.float64)\n",
      "hess: tensor(1.5148)\n",
      "grad: tensor(0.0221, dtype=torch.float64)\n",
      "hess: tensor(0.8216)\n",
      "grad: tensor(0.1372, dtype=torch.float64)\n",
      "hess: tensor(5.7144)\n",
      "grad: tensor(0.9431, dtype=torch.float64)\n",
      "hess: tensor(27.8293)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4394bd36874d08bbd596344871e691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(5.9629e-09, dtype=torch.float64)\n",
      "hess: tensor(3.6194e-07)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.1902, dtype=torch.float64)\n",
      "hess: tensor(6.7909)\n",
      "grad: tensor(8.7239e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0043)\n",
      "grad: tensor(7.6308e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0026)\n",
      "grad: tensor(0.0073, dtype=torch.float64)\n",
      "hess: tensor(0.2472)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.000013\n",
      "grad: tensor(0.0332, dtype=torch.float64)\n",
      "hess: tensor(1.4363)\n",
      "grad: tensor(0.0216, dtype=torch.float64)\n",
      "hess: tensor(0.8074)\n",
      "grad: tensor(0.1341, dtype=torch.float64)\n",
      "hess: tensor(5.6026)\n",
      "grad: tensor(0.9165, dtype=torch.float64)\n",
      "hess: tensor(27.1978)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a8499172cb943a5a00027c4149b6a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(5.4259e-09, dtype=torch.float64)\n",
      "hess: tensor(3.3000e-07)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.1875, dtype=torch.float64)\n",
      "hess: tensor(6.7172)\n",
      "grad: tensor(8.7306e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0043)\n",
      "grad: tensor(7.2228e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0025)\n",
      "grad: tensor(0.0071, dtype=torch.float64)\n",
      "hess: tensor(0.2396)\n",
      "\taccuracy:89.84375\n",
      "\tloss: 0.000013\n",
      "grad: tensor(0.0329, dtype=torch.float64)\n",
      "hess: tensor(1.4270)\n",
      "grad: tensor(0.0211, dtype=torch.float64)\n",
      "hess: tensor(0.7852)\n",
      "grad: tensor(0.1331, dtype=torch.float64)\n",
      "hess: tensor(5.5853)\n",
      "grad: tensor(0.8997, dtype=torch.float64)\n",
      "hess: tensor(26.8074)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae3c77af4f9c4b6b8354629aec3152df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(5.0651e-09, dtype=torch.float64)\n",
      "hess: tensor(3.0869e-07)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.1855, dtype=torch.float64)\n",
      "hess: tensor(6.6719)\n",
      "grad: tensor(8.6564e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0042)\n",
      "grad: tensor(7.1734e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0025)\n",
      "grad: tensor(0.0070, dtype=torch.float64)\n",
      "hess: tensor(0.2368)\n",
      "\taccuracy:89.84375\n",
      "\tloss: 0.000012\n",
      "grad: tensor(0.0321, dtype=torch.float64)\n",
      "hess: tensor(1.3944)\n",
      "grad: tensor(0.0196, dtype=torch.float64)\n",
      "hess: tensor(0.7345)\n",
      "grad: tensor(0.1274, dtype=torch.float64)\n",
      "hess: tensor(5.3628)\n",
      "grad: tensor(0.8827, dtype=torch.float64)\n",
      "hess: tensor(26.4251)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b264b6b47af34d42a6dac9387d921236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(4.5939e-09, dtype=torch.float64)\n",
      "hess: tensor(2.8052e-07)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.1801, dtype=torch.float64)\n",
      "hess: tensor(6.4965)\n",
      "grad: tensor(8.3385e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0041)\n",
      "grad: tensor(7.1174e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0024)\n",
      "grad: tensor(0.0065, dtype=torch.float64)\n",
      "hess: tensor(0.2211)\n",
      "\taccuracy:89.84375\n",
      "\tloss: 0.000011\n",
      "grad: tensor(0.0305, dtype=torch.float64)\n",
      "hess: tensor(1.3301)\n",
      "grad: tensor(0.0192, dtype=torch.float64)\n",
      "hess: tensor(0.7217)\n",
      "grad: tensor(0.1255, dtype=torch.float64)\n",
      "hess: tensor(5.3029)\n",
      "grad: tensor(0.8681, dtype=torch.float64)\n",
      "hess: tensor(26.1064)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1b1e9248123414f86c9691d0b830cdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(4.1702e-09, dtype=torch.float64)\n",
      "hess: tensor(2.5514e-07)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.1772, dtype=torch.float64)\n",
      "hess: tensor(6.4107)\n",
      "grad: tensor(8.3259e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0041)\n",
      "grad: tensor(0.2289, dtype=torch.float64)\n",
      "hess: tensor(6.8390)\n",
      "grad: tensor(0.4320, dtype=torch.float64)\n",
      "hess: tensor(14.5381)\n",
      "\taccuracy:89.84375\n",
      "\tloss: 0.000011\n",
      "grad: tensor(0.0005, dtype=torch.float64)\n",
      "hess: tensor(0.0190)\n",
      "grad: tensor(3.1891, dtype=torch.float64)\n",
      "hess: tensor(69.4228)\n",
      "grad: tensor(5.5398e-06, dtype=torch.float64)\n",
      "hess: tensor(0.0003)\n",
      "grad: tensor(0.0015, dtype=torch.float64)\n",
      "hess: tensor(0.0695)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a11b90fcf44fe6bbbedc73287d3336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(3.8370e-09, dtype=torch.float64)\n",
      "hess: tensor(2.3515e-07)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.1767, dtype=torch.float64)\n",
      "hess: tensor(6.9142)\n",
      "grad: tensor(0.2323, dtype=torch.float64)\n",
      "hess: tensor(6.9579)\n",
      "grad: tensor(0.4351, dtype=torch.float64)\n",
      "hess: tensor(14.6796)\n",
      "\taccuracy:89.84375\n",
      "\tloss: 0.000010\n",
      "grad: tensor(0.0005, dtype=torch.float64)\n",
      "hess: tensor(0.0184)\n",
      "grad: tensor(3.1476, dtype=torch.float64)\n",
      "hess: tensor(68.9398)\n",
      "grad: tensor(5.5134e-06, dtype=torch.float64)\n",
      "hess: tensor(0.0003)\n",
      "grad: tensor(0.0015, dtype=torch.float64)\n",
      "hess: tensor(0.0680)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b2be1e919a4423aeb232daed6b467d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(3.5434e-09, dtype=torch.float64)\n",
      "hess: tensor(2.1756e-07)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.1663, dtype=torch.float64)\n",
      "hess: tensor(6.5349)\n",
      "grad: tensor(0.2299, dtype=torch.float64)\n",
      "hess: tensor(6.9062)\n",
      "grad: tensor(0.4259, dtype=torch.float64)\n",
      "hess: tensor(14.4272)\n",
      "\taccuracy:90.234375\n",
      "\tloss: 0.000010\n",
      "grad: tensor(0.0005, dtype=torch.float64)\n",
      "hess: tensor(0.0181)\n",
      "grad: tensor(3.1300, dtype=torch.float64)\n",
      "hess: tensor(68.8122)\n",
      "grad: tensor(5.3651e-06, dtype=torch.float64)\n",
      "hess: tensor(0.0002)\n",
      "grad: tensor(0.0014, dtype=torch.float64)\n",
      "hess: tensor(0.0661)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50f59d58f450411f84efcac0e961ed5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(3.2608e-09, dtype=torch.float64)\n",
      "hess: tensor(2.0059e-07)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.1623, dtype=torch.float64)\n",
      "hess: tensor(6.3950)\n",
      "grad: tensor(0.2247, dtype=torch.float64)\n",
      "hess: tensor(6.7630)\n",
      "grad: tensor(0.4028, dtype=torch.float64)\n",
      "hess: tensor(13.7173)\n",
      "\taccuracy:90.234375\n",
      "\tloss: 0.000009\n",
      "grad: tensor(0.0005, dtype=torch.float64)\n",
      "hess: tensor(0.0175)\n",
      "grad: tensor(2.9905, dtype=torch.float64)\n",
      "hess: tensor(66.7894)\n",
      "grad: tensor(5.2914e-06, dtype=torch.float64)\n",
      "hess: tensor(0.0002)\n",
      "grad: tensor(0.0014, dtype=torch.float64)\n",
      "hess: tensor(0.0652)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8a5d951fb304c7a9831b26bd3851f18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(2.9797e-09, dtype=torch.float64)\n",
      "hess: tensor(1.8362e-07)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.1513, dtype=torch.float64)\n",
      "hess: tensor(5.9862)\n",
      "grad: tensor(0.2246, dtype=torch.float64)\n",
      "hess: tensor(6.7780)\n",
      "grad: tensor(0.4063, dtype=torch.float64)\n",
      "hess: tensor(13.8692)\n",
      "\taccuracy:90.234375\n",
      "\tloss: 0.000009\n",
      "grad: tensor(0.0005, dtype=torch.float64)\n",
      "hess: tensor(0.0170)\n",
      "grad: tensor(2.9460, dtype=torch.float64)\n",
      "hess: tensor(66.2247)\n",
      "grad: tensor(2.3485e-06, dtype=torch.float64)\n",
      "hess: tensor(0.0002)\n",
      "grad: tensor(0.0013, dtype=torch.float64)\n",
      "hess: tensor(0.0627)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6acdec8095d7410d90c6c26c66c63b91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(2.7581e-09, dtype=torch.float64)\n",
      "hess: tensor(1.7026e-07)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.1467, dtype=torch.float64)\n",
      "hess: tensor(5.8216)\n",
      "grad: tensor(0.2206, dtype=torch.float64)\n",
      "hess: tensor(6.6753)\n",
      "grad: tensor(0.3962, dtype=torch.float64)\n",
      "hess: tensor(13.5717)\n",
      "\taccuracy:90.234375\n",
      "\tloss: 0.000008\n",
      "grad: tensor(0.0004, dtype=torch.float64)\n",
      "hess: tensor(0.0167)\n",
      "grad: tensor(2.9329, dtype=torch.float64)\n",
      "hess: tensor(66.1414)\n",
      "grad: tensor(2.2231e-06, dtype=torch.float64)\n",
      "hess: tensor(0.0001)\n",
      "grad: tensor(0.0013, dtype=torch.float64)\n",
      "hess: tensor(0.0613)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c091f39235b4728b63c6811b254c0b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(2.5089e-09, dtype=torch.float64)\n",
      "hess: tensor(1.5514e-07)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.1396, dtype=torch.float64)\n",
      "hess: tensor(5.5601)\n",
      "grad: tensor(0.2210, dtype=torch.float64)\n",
      "hess: tensor(6.7019)\n",
      "grad: tensor(0.3886, dtype=torch.float64)\n",
      "hess: tensor(13.3570)\n",
      "\taccuracy:90.234375\n",
      "\tloss: 0.000008\n",
      "grad: tensor(0.0004, dtype=torch.float64)\n",
      "hess: tensor(0.0162)\n",
      "grad: tensor(2.8822, dtype=torch.float64)\n",
      "hess: tensor(65.4607)\n",
      "grad: tensor(2.0088e-06, dtype=torch.float64)\n",
      "hess: tensor(0.0001)\n",
      "grad: tensor(0.0013, dtype=torch.float64)\n",
      "hess: tensor(0.0601)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "854aa1cbbf8d42c1a56fcca1db6b4eb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(2.3454e-09, dtype=torch.float64)\n",
      "hess: tensor(1.4526e-07)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.1366, dtype=torch.float64)\n",
      "hess: tensor(5.4534)\n",
      "grad: tensor(0.2164, dtype=torch.float64)\n",
      "hess: tensor(6.5761)\n",
      "grad: tensor(0.3877, dtype=torch.float64)\n",
      "hess: tensor(13.3659)\n",
      "\taccuracy:90.234375\n",
      "\tloss: 0.000008\n",
      "grad: tensor(0.0004, dtype=torch.float64)\n",
      "hess: tensor(0.0158)\n",
      "grad: tensor(2.7955, dtype=torch.float64)\n",
      "hess: tensor(64.1666)\n",
      "grad: tensor(1.9104e-06, dtype=torch.float64)\n",
      "hess: tensor(0.0001)\n",
      "grad: tensor(0.0013, dtype=torch.float64)\n",
      "hess: tensor(0.0590)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c84dcb391a4ef2a1d5d1a8ba893ee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(2.1736e-09, dtype=torch.float64)\n",
      "hess: tensor(1.3483e-07)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.1284, dtype=torch.float64)\n",
      "hess: tensor(5.1430)\n",
      "grad: tensor(0.2191, dtype=torch.float64)\n",
      "hess: tensor(6.6766)\n",
      "grad: tensor(0.3806, dtype=torch.float64)\n",
      "hess: tensor(13.1639)\n",
      "\taccuracy:90.234375\n",
      "\tloss: 0.000007\n",
      "grad: tensor(0.0004, dtype=torch.float64)\n",
      "hess: tensor(0.0156)\n",
      "grad: tensor(2.7823, dtype=torch.float64)\n",
      "hess: tensor(64.0646)\n",
      "grad: tensor(1.7950e-06, dtype=torch.float64)\n",
      "hess: tensor(0.0001)\n",
      "grad: tensor(0.0012, dtype=torch.float64)\n",
      "hess: tensor(0.0570)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94aa090e18284f12a732ef18c4baff59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(1.9789e-09, dtype=torch.float64)\n",
      "hess: tensor(1.2297e-07)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.1239, dtype=torch.float64)\n",
      "hess: tensor(4.9805)\n",
      "grad: tensor(0.2115, dtype=torch.float64)\n",
      "hess: tensor(6.4588)\n",
      "grad: tensor(0.3749, dtype=torch.float64)\n",
      "hess: tensor(13.0024)\n",
      "\taccuracy:90.234375\n",
      "\tloss: 0.000007\n",
      "grad: tensor(0.0004, dtype=torch.float64)\n",
      "hess: tensor(0.0149)\n",
      "grad: tensor(2.7438, dtype=torch.float64)\n",
      "hess: tensor(63.5459)\n",
      "grad: tensor(1.6841e-06, dtype=torch.float64)\n",
      "hess: tensor(0.0001)\n",
      "grad: tensor(0.0012, dtype=torch.float64)\n",
      "hess: tensor(0.0557)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc93aa4455c4439b863240759be1886c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(1.8513e-09, dtype=torch.float64)\n",
      "hess: tensor(1.1522e-07)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.1181, dtype=torch.float64)\n",
      "hess: tensor(4.7609)\n",
      "grad: tensor(0.2145, dtype=torch.float64)\n",
      "hess: tensor(6.5608)\n",
      "grad: tensor(0.3583, dtype=torch.float64)\n",
      "hess: tensor(12.4825)\n",
      "\taccuracy:90.234375\n",
      "\tloss: 0.000007\n",
      "grad: tensor(0.0004, dtype=torch.float64)\n",
      "hess: tensor(0.0147)\n",
      "grad: tensor(2.6650, dtype=torch.float64)\n",
      "hess: tensor(62.3239)\n",
      "grad: tensor(1.5741e-06, dtype=torch.float64)\n",
      "hess: tensor(0.0001)\n",
      "grad: tensor(0.0011, dtype=torch.float64)\n",
      "hess: tensor(0.0542)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa3791a62f3948ed954b5643e792dc4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(1.6924e-09, dtype=torch.float64)\n",
      "hess: tensor(1.0551e-07)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.1146, dtype=torch.float64)\n",
      "hess: tensor(4.6330)\n",
      "grad: tensor(0.2122, dtype=torch.float64)\n",
      "hess: tensor(6.5093)\n",
      "grad: tensor(0.3502, dtype=torch.float64)\n",
      "hess: tensor(12.2422)\n",
      "\taccuracy:90.234375\n",
      "\tloss: 0.000006\n",
      "grad: tensor(0.0004, dtype=torch.float64)\n",
      "hess: tensor(0.0146)\n",
      "grad: tensor(2.5990, dtype=torch.float64)\n",
      "hess: tensor(61.3047)\n",
      "grad: tensor(1.4877e-06, dtype=torch.float64)\n",
      "hess: tensor(0.0001)\n",
      "grad: tensor(0.0011, dtype=torch.float64)\n",
      "hess: tensor(0.0532)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02bd11b7f9fe4a699eeb37aa3cf8ed07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(1.5657e-09, dtype=torch.float64)\n",
      "hess: tensor(9.7752e-08)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.1098, dtype=torch.float64)\n",
      "hess: tensor(4.4507)\n",
      "grad: tensor(0.2098, dtype=torch.float64)\n",
      "hess: tensor(6.4495)\n",
      "grad: tensor(0.3543, dtype=torch.float64)\n",
      "hess: tensor(12.4090)\n",
      "\taccuracy:90.234375\n",
      "\tloss: 0.000006\n",
      "grad: tensor(0.0004, dtype=torch.float64)\n",
      "hess: tensor(0.0140)\n",
      "grad: tensor(2.5839, dtype=torch.float64)\n",
      "hess: tensor(61.1463)\n",
      "grad: tensor(0.0014, dtype=torch.float64)\n",
      "hess: tensor(0.0573)\n",
      "grad: tensor(0.0958, dtype=torch.float64)\n",
      "hess: tensor(4.2128)\n",
      "grad: tensor(0.0139, dtype=torch.float64)\n",
      "hess: tensor(0.6114)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f7f2731eeeb4c32864d4a697ef46dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(1.4574e-09, dtype=torch.float64)\n",
      "hess: tensor(9.1127e-08)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(1.4005, dtype=torch.float64)\n",
      "hess: tensor(30.7156)\n",
      "grad: tensor(9.1628e-07, dtype=torch.float64)\n",
      "hess: tensor(5.2649e-05)\n",
      "grad: tensor(0.0019, dtype=torch.float64)\n",
      "hess: tensor(0.0781)\n",
      "\taccuracy:90.234375\n",
      "\tloss: 0.000006\n",
      "grad: tensor(0.0042, dtype=torch.float64)\n",
      "hess: tensor(0.1734)\n",
      "grad: tensor(0.0013, dtype=torch.float64)\n",
      "hess: tensor(0.0544)\n",
      "grad: tensor(0.0934, dtype=torch.float64)\n",
      "hess: tensor(4.1147)\n",
      "grad: tensor(0.0134, dtype=torch.float64)\n",
      "hess: tensor(0.5887)\n",
      "hess: tensor(0.0745)\n",
      "\taccuracy:89.84375\n",
      "\tloss: 0.000005\n",
      "grad: tensor(0.0042, dtype=torch.float64)\n",
      "hess: tensor(0.1721)\n",
      "grad: tensor(0.0012, dtype=torch.float64)\n",
      "hess: tensor(0.0488)\n",
      "grad: tensor(0.0903, dtype=torch.float64)\n",
      "hess: tensor(3.9996)\n",
      "grad: tensor(0.0127, dtype=torch.float64)\n",
      "hess: tensor(0.5626)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5143e8d87f1c4e1e8c812528a8615a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(1.1614e-09, dtype=torch.float64)\n",
      "hess: tensor(7.2937e-08)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(1.3309, dtype=torch.float64)\n",
      "hess: tensor(29.5283)\n",
      "grad: tensor(7.7951e-07, dtype=torch.float64)\n",
      "hess: tensor(4.4892e-05)\n",
      "grad: tensor(0.0018, dtype=torch.float64)\n",
      "hess: tensor(0.0727)\n",
      "\taccuracy:89.84375\n",
      "\tloss: 0.000005\n",
      "grad: tensor(0.0041, dtype=torch.float64)\n",
      "hess: tensor(0.1679)\n",
      "grad: tensor(0.0011, dtype=torch.float64)\n",
      "hess: tensor(0.0465)\n",
      "grad: tensor(0.0886, dtype=torch.float64)\n",
      "hess: tensor(3.9297)\n",
      "grad: tensor(0.0126, dtype=torch.float64)\n",
      "hess: tensor(0.5593)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3280c7d2f564fd480d283696bf9afe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(1.0600e-09, dtype=torch.float64)\n",
      "hess: tensor(6.6662e-08)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(1.3005, dtype=torch.float64)\n",
      "hess: tensor(28.9822)\n",
      "grad: tensor(7.3507e-07, dtype=torch.float64)\n",
      "hess: tensor(4.2463e-05)\n",
      "grad: tensor(0.0018, dtype=torch.float64)\n",
      "hess: tensor(0.0726)\n",
      "\taccuracy:89.84375\n",
      "\tloss: 0.000005\n",
      "grad: tensor(0.0041, dtype=torch.float64)\n",
      "hess: tensor(0.1692)\n",
      "grad: tensor(0.0011, dtype=torch.float64)\n",
      "hess: tensor(0.0443)\n",
      "grad: tensor(0.0880, dtype=torch.float64)\n",
      "hess: tensor(3.9192)\n",
      "grad: tensor(0.0121, dtype=torch.float64)\n",
      "hess: tensor(0.5358)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71f44540cd584cebadb96589c5bb2e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(1.0024e-09, dtype=torch.float64)\n",
      "hess: tensor(6.3124e-08)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(1.2868, dtype=torch.float64)\n",
      "hess: tensor(28.7659)\n",
      "grad: tensor(6.7718e-07, dtype=torch.float64)\n",
      "hess: tensor(3.9510e-05)\n",
      "grad: tensor(0.0017, dtype=torch.float64)\n",
      "hess: tensor(0.0693)\n",
      "\taccuracy:89.84375\n",
      "\tloss: 0.000005\n",
      "grad: tensor(0.0040, dtype=torch.float64)\n",
      "hess: tensor(0.1644)\n",
      "grad: tensor(0.0010, dtype=torch.float64)\n",
      "hess: tensor(0.0422)\n",
      "grad: tensor(0.0865, dtype=torch.float64)\n",
      "hess: tensor(3.8596)\n",
      "grad: tensor(0.0116, dtype=torch.float64)\n",
      "hess: tensor(0.5176)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5ea18c0eb4f4279bd9b43603652b734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(9.2529e-10, dtype=torch.float64)\n",
      "hess: tensor(5.8351e-08)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.000000\n",
      "grad: tensor(1.2635, dtype=torch.float64)\n",
      "hess: tensor(28.3506)\n",
      "grad: tensor(6.5364e-07, dtype=torch.float64)\n",
      "hess: tensor(3.8217e-05)\n",
      "grad: tensor(0.0016, dtype=torch.float64)\n",
      "hess: tensor(0.0685)\n",
      "\taccuracy:89.84375\n",
      "\tloss: 0.000005\n",
      "grad: tensor(0.0040, dtype=torch.float64)\n",
      "hess: tensor(0.1653)\n",
      "grad: tensor(0.0010, dtype=torch.float64)\n",
      "hess: tensor(0.0401)\n",
      "grad: tensor(0.0844, dtype=torch.float64)\n",
      "hess: tensor(3.7727)\n",
      "grad: tensor(0.0115, dtype=torch.float64)\n",
      "hess: tensor(0.5112)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd70675b31ff41e884f590ebe68fa615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(8.5826e-10, dtype=torch.float64)\n",
      "hess: tensor(5.4199e-08)\n",
      "\taccuracy:88.671875\n",
      "\tloss: 0.000000\n",
      "grad: tensor(1.2300, dtype=torch.float64)\n",
      "hess: tensor(27.7278)\n",
      "grad: tensor(6.0712e-07, dtype=torch.float64)\n",
      "hess: tensor(3.5452e-05)\n",
      "grad: tensor(0.0016, dtype=torch.float64)\n",
      "hess: tensor(0.0671)\n",
      "\taccuracy:89.84375\n",
      "\tloss: 0.000004\n",
      "grad: tensor(0.0039, dtype=torch.float64)\n",
      "hess: tensor(0.1631)\n",
      "grad: tensor(0.0009, dtype=torch.float64)\n",
      "hess: tensor(0.0386)\n",
      "grad: tensor(0.0837, dtype=torch.float64)\n",
      "hess: tensor(3.7504)\n",
      "grad: tensor(0.0113, dtype=torch.float64)\n",
      "hess: tensor(0.5016)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da735d9a2e494956b904e75fd534d84c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(8.0799e-10, dtype=torch.float64)\n",
      "hess: tensor(5.1090e-08)\n",
      "\taccuracy:88.671875\n",
      "\tloss: 0.000000\n",
      "grad: tensor(1.2180, dtype=torch.float64)\n",
      "hess: tensor(27.5281)\n",
      "grad: tensor(5.8518e-07, dtype=torch.float64)\n",
      "hess: tensor(3.4180e-05)\n",
      "grad: tensor(0.0016, dtype=torch.float64)\n",
      "hess: tensor(0.0647)\n",
      "\taccuracy:89.84375\n",
      "\tloss: 0.000004\n",
      "grad: tensor(0.0039, dtype=torch.float64)\n",
      "hess: tensor(0.1605)\n",
      "grad: tensor(0.0009, dtype=torch.float64)\n",
      "hess: tensor(0.0360)\n",
      "grad: tensor(0.0806, dtype=torch.float64)\n",
      "hess: tensor(3.6161)\n",
      "grad: tensor(0.0110, dtype=torch.float64)\n",
      "hess: tensor(0.4897)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec527b3b31ce4c2bb5f90d9bc2fa1333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(7.4826e-10, dtype=torch.float64)\n",
      "hess: tensor(4.7372e-08)\n",
      "\taccuracy:88.671875\n",
      "\tloss: 0.000000\n",
      "grad: tensor(1.2102, dtype=torch.float64)\n",
      "hess: tensor(27.4153)\n",
      "grad: tensor(5.5230e-07, dtype=torch.float64)\n",
      "hess: tensor(3.2474e-05)\n",
      "grad: tensor(0.0015, dtype=torch.float64)\n",
      "hess: tensor(0.0647)\n",
      "\taccuracy:89.84375\n",
      "\tloss: 0.000004\n",
      "grad: tensor(0.0038, dtype=torch.float64)\n",
      "hess: tensor(0.1572)\n",
      "grad: tensor(0.0008, dtype=torch.float64)\n",
      "hess: tensor(0.0344)\n",
      "grad: tensor(0.0792, dtype=torch.float64)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 397>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    393\u001b[0m         grad_list\u001b[38;5;241m.\u001b[39mappend(train_model\u001b[38;5;241m.\u001b[39mgrads_norms)\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grad_list, hess_norm_list\n\u001b[0;32m--> 397\u001b[0m grad_list, hess_norm_list \u001b[38;5;241m=\u001b[39m \u001b[43mexp_get_lp_sm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m              \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mg_weight\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mg_times\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m              \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mg_epochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresult_root_dir\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m              \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresult_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq_reduce_by\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfreq_reduce_by\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m              \u001b[49m\u001b[43mfreq_reduce_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfreq_reduce_after\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mexp_get_lp_sm\u001b[0;34m(train_data_all, test_data_all, op_features, weight, times, epochs, root_dir, path, clear_file, freq_reduce_by, freq_reduce_after)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    384\u001b[0m train_model \u001b[38;5;241m=\u001b[39m Train_nn(\u001b[38;5;241m784\u001b[39m, weight, op_features, lr\u001b[38;5;241m=\u001b[39m details[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha_0\u001b[39m\u001b[38;5;124m'\u001b[39m], decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 385\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_grads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_hessian\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbook_keep_freq\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mstore_gen_err\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_pt_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq_reduce_by\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfreq_reduce_by\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq_reduce_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreq_reduce_after\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(grad_file_path,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    388\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(grad) \u001b[38;5;28;01mfor\u001b[39;00m grad \u001b[38;5;129;01min\u001b[39;00m train_model\u001b[38;5;241m.\u001b[39mgrads_norms]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mTrain_nn.fit\u001b[0;34m(self, train_loader, test_loader, epochs, store_grads, store_hessian, store_gen_err, store_weights, store_pt_loss, store_freq, freq_reduce_by, freq_reduce_after)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m store_hessian \u001b[38;5;129;01mand\u001b[39;00m (batch\u001b[38;5;241m%\u001b[39m store_freq\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;66;03m#assert False, \"Not implemented\"\u001b[39;00m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 321\u001b[0m     hess_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_hessianv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhess:\u001b[39m\u001b[38;5;124m'\u001b[39m,hess_val)\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhess_norms\u001b[38;5;241m.\u001b[39mappend(hess_val)\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mTrain_nn.get_hessianv2\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    250\u001b[0m     local_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(out, y)\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m local_loss\n\u001b[0;32m--> 252\u001b[0m hess_mat \u001b[38;5;241m=\u001b[39m \u001b[43mhessian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_fun_hess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# print(f'len of hess mat:{len(hess_mat)}')\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m# print(f'hess_mat[0] shape:{len(hess_mat[0])}')\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;66;03m# print(f'hess_mat[0][0] shape:{hess_mat[0][0].shape}')\u001b[39;00m\n\u001b[1;32m    256\u001b[0m hess_norm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/functional.py:808\u001b[0m, in \u001b[0;36mhessian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, outer_jacobian_strategy)\u001b[0m\n\u001b[1;32m    805\u001b[0m     _check_requires_grad(jac, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacobian\u001b[39m\u001b[38;5;124m\"\u001b[39m, strict\u001b[38;5;241m=\u001b[39mstrict)\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jac\n\u001b[0;32m--> 808\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mjacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjac_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvectorize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m               \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mouter_jacobian_strategy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _tuple_postprocess(res, (is_inputs_tuple, is_inputs_tuple))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/functional.py:670\u001b[0m, in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[1;32m    668\u001b[0m jac_i: Tuple[List[torch\u001b[38;5;241m.\u001b[39mTensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m([] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs)))  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(out\u001b[38;5;241m.\u001b[39mnelement()):\n\u001b[0;32m--> 670\u001b[0m     vj \u001b[38;5;241m=\u001b[39m \u001b[43m_autograd_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m el_idx, (jac_i_el, vj_el, inp_el) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(jac_i, vj, inputs)):\n\u001b[1;32m    674\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m vj_el \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/functional.py:159\u001b[0m, in \u001b[0;36m_autograd_grad\u001b[0;34m(outputs, inputs, grad_outputs, create_graph, retain_graph, is_grads_batched)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m,) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_grad_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_grads_batched\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:276\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd.functional import hessian\n",
    "import torch.utils.data as data_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from tqdm.notebook import tqdm \n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import seaborn as sns\n",
    "import os\n",
    "import copy\n",
    "from torch.nn.utils import _stateless\n",
    "\n",
    "batch_size = 1\n",
    "num_workers = 1\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "# device\n",
    "\n",
    "details = {}\n",
    "details['use_db'] = 'mnist'\n",
    "details['result_root_dir']='results/t0/'\n",
    "details['result_path']='try1_t8_w12'\n",
    "details['g_weight'] = [12]\n",
    "# details['ratio'] = 15\n",
    "details['book_keep_freq'] = 20\n",
    "details['g_times'] = 8\n",
    "details['g_epochs'] = 10000\n",
    "details['alpha_0']= 0.003\n",
    "details['freq_reduce_by'] = 20\n",
    "details['freq_reduce_after'] = 100\n",
    "\n",
    "details['training_step_limit'] = 2000000 ## this is to train for max updates per epochs\n",
    "details['stop_hess_computation'] = 1000000 ## Stop computing hessian after calculated these many times\n",
    "\n",
    "\n",
    "print(f'selected weight:{details[\"g_weight\"]}')\n",
    "\n",
    "with open(details['result_root_dir']+'details_'+details['result_path']+'.txt', 'w+') as f:\n",
    "    for key, val in details.items():\n",
    "        content = key + ' : '+str(val) + '\\n'\n",
    "        f.write(content)\n",
    "        \n",
    "torch.manual_seed(3407)\n",
    "np.random.seed(3407)\n",
    "torch.cuda.manual_seed_all(3407)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "# os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":4096:2\"\n",
    "\n",
    "train_data_all = torchvision.datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "test_data_all = torchvision.datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "# print(f'train data:{train_data}')\n",
    "# print(f'test data:{test_data}')\n",
    "\n",
    "def get_random_subset(train_data_all, test_data_all):    \n",
    "    # train_indices = torch.arange(20000)\n",
    "    test_indices = torch.arange(256)\n",
    "    train_indices = torch.randint(60000-1, (2000,))\n",
    "    # print(f'train indices:{train_indices[:10]}')\n",
    "    train_data = data_utils.Subset(train_data_all, train_indices)\n",
    "    test_data = data_utils.Subset(test_data_all, test_indices)\n",
    "    # print(f'train data:{train_data}')\n",
    "    # print(f'test data:{test_data}')\n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(0)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g,\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_data,\n",
    "        batch_size=256,\n",
    "        num_workers=num_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g,\n",
    "    )\n",
    "    print(f'train data size:{len(train_loader.dataset)}')\n",
    "    print(f'test data size:{len(test_loader.dataset)}')\n",
    "    # X_mat, y_mat = torch.Tensor(len(train_loader.dataset),784), torch.Tensor(len(train_loader.dataset)).long()\n",
    "    # for i, (data, label) in enumerate(train_loader):\n",
    "    #     X_mat[i] = data.flatten()\n",
    "    #     y_mat[i] = label.flatten()\n",
    "    # print(f'X_mat shape:{X_mat.shape}, y_mat shape:{y_mat.shape}')\n",
    "    return train_loader, test_loader #, X_mat, y_mat\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_features, hidden_layers, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = len(hidden_layers) + 1\n",
    "        self.total_params_len = 0\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        prev_weight = input_features\n",
    "        \n",
    "        for i, weight in enumerate(hidden_layers):\n",
    "            self.fc_layers.append(nn.Linear(prev_weight, weight))\n",
    "            self.total_params_len += prev_weight*weight + weight\n",
    "            prev_weight = weight\n",
    "        \n",
    "        self.fc_last = nn.Linear(hidden_layers[-1], output_size)\n",
    "        self.total_params_len += hidden_layers[-1]*output_size + output_size\n",
    "        \n",
    "        ### Others required params\n",
    "        self.param_list = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        # print('x shape in forward',x.shape)\n",
    "        for fc_layer in self.fc_layers:\n",
    "            x = F.relu(fc_layer(x))\n",
    "        x = self.fc_last(x)\n",
    "        return x\n",
    "    \n",
    "    def fit(self, X, Y, X_val, Y_val, epochs, batch_size=1, **kwargs):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.SGD(self.parameters())\n",
    "        \n",
    "\n",
    "class Train_nn:\n",
    "    \n",
    "    def __init__(self, input_features, hidden_layers, output_size, lr, decay=True):\n",
    "        self.model = Net(input_features, hidden_layers=hidden_layers, output_size=output_size)\n",
    "        self.model.to(device)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        if decay:\n",
    "            lr_lambda = lambda it: 1/(it+1)\n",
    "        else:\n",
    "            lr_lambda = lambda it: 1\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda= lr_lambda)\n",
    "        \n",
    "    def get_loss(self, X, y, params=None):\n",
    "        # if params is not None:\n",
    "        assert False, \"Model not initialized with given params\"\n",
    "        op = self.model(X)\n",
    "        loss = self.loss_fn(pred, y)\n",
    "        return loss\n",
    "    \n",
    "    def get_gradient(self):\n",
    "        params = (self.model.parameters())\n",
    "        grad_norm_sq = torch.tensor(0, dtype=float).to(device)\n",
    "        # print('grad Norm init:', grad_norm_sq)\n",
    "        for param in self.model.parameters():\n",
    "            temp = param.grad.data.pow(2).sum()\n",
    "            # print(f'param grad norm \\n\\tsum:{temp.data}')#,\\n\\tshape:{param.shape}')\n",
    "            grad_norm_sq += temp\n",
    "            \n",
    "        return grad_norm_sq.sqrt().cpu()\n",
    "    \n",
    "    def get_gradientv2(self, X, y):\n",
    "        names = list(n for n, _ in self.model.named_parameters())\n",
    "        def loss_fun_grad(*params):\n",
    "            out: torch.Tensor = _stateless.functional_call(self.model, {n: p for n, p in zip(names, params)}, X)\n",
    "            local_loss = self.loss_fn(out, y)\n",
    "            return local_loss\n",
    "        grad_mat = torch.autograd.grad(loss_fun_grad, tuple(self.model.parameters()))\n",
    "        # print(f'len of hess mat:{len(hess_mat)}')\n",
    "        # print(f'hess_mat[0] shape:{len(hess_mat[0])}')\n",
    "        # print(f'hess_mat[0][0] shape:{hess_mat[0][0].shape}')\n",
    "        grad_norm = torch.tensor(0.).to(device)\n",
    "        for i in range(len(grad_mat)):\n",
    "            for j in range(len(grad_mat[0])):\n",
    "                grad_norm+= grad_mat[i][j].pow(2).sum()\n",
    "        grad_norm = grad_norm.sqrt()\n",
    "        # print(f'v2 hess norm{hess_norm}')\n",
    "        return grad_norm.cpu()\n",
    "    \n",
    "    def try_operator_norm(self, hess_mat):\n",
    "        for i in len(hess_mat):\n",
    "            for j in len(hess_mat[0]):\n",
    "                torch.unsqueeze(hess_mat[i][i],0)\n",
    "        hess_tensor_dim = list(hess_mat[0][0].shape)\n",
    "        hess_tensor_dim += [n*2,n*2]\n",
    "        hess_mat_np = np.zeros(shape=hess_tensor_dim)\n",
    "        hess_tensor = torch.tensor(hess_mat_np)\n",
    "        torch.cat(hess_mat, out=hess_tensor)\n",
    "        \n",
    "        hess_mat.reshpe(n*2,n*2)\n",
    "        hess_norm = torch.linalg.norm(hess_mat, 2)\n",
    "        assert False, \"Not working\"\n",
    "    \n",
    "    def get_hessian(self, X, y):\n",
    "        prev_params = copy.deepcopy(list(self.model.parameters()))\n",
    "        n = self.model.layers\n",
    "        def local_model(*params):\n",
    "            # print(f'len of params:{len(params)}')\n",
    "            # print(f'shape of params[0]:{params[0].shape}')\n",
    "            # with torch.no_grad():\n",
    "            #initialize model with given params\n",
    "            i = 0\n",
    "            for i, param in enumerate(self.model.parameters()):\n",
    "                param.data = params[i]\n",
    "            pred = self.model(X)\n",
    "            loss = self.loss_fn(pred, y)\n",
    "            # print(f'loss type:{type(loss)}')\n",
    "            return loss\n",
    "        p =list(self.model.parameters())\n",
    "        hess_mat = hessian(local_model, tuple(p))\n",
    "        hess_norm = torch.tensor(0.).to(device)\n",
    "        for i in range(len(hess_mat)):\n",
    "            for j in range(len(hess_mat[0])):\n",
    "                hess_norm+= hess_mat[i][j].pow(2).sum()\n",
    "        \n",
    "        # print(f'Hess mat len:{len(hess_mat)}')\n",
    "        # print(f'Hess mat[0] len:{len(hess_mat[0])}')\n",
    "        # print(f'Hess mat[0][0] shape:{hess_mat[0][0].shape}')\n",
    "        \n",
    "        hess_norm = hess_norm.sqrt()\n",
    "        # print(f'hess norm:{hess_norm}')\n",
    "        \n",
    "        # Reinitialize the original params to model\n",
    "        for i, param in enumerate(self.model.parameters()):\n",
    "                param.data = prev_params[i]\n",
    "        \n",
    "        return hess_norm\n",
    "    \n",
    "    def get_hessianv2(self, X,y):\n",
    "        names = list(n for n, _ in self.model.named_parameters())\n",
    "        def loss_fun_hess(*params):\n",
    "            out: torch.Tensor = _stateless.functional_call(self.model, {n: p for n, p in zip(names, params)}, X)\n",
    "            local_loss = self.loss_fn(out, y)\n",
    "            return local_loss\n",
    "        hess_mat = hessian(loss_fun_hess, tuple(self.model.parameters()))\n",
    "        # print(f'len of hess mat:{len(hess_mat)}')\n",
    "        # print(f'hess_mat[0] shape:{len(hess_mat[0])}')\n",
    "        # print(f'hess_mat[0][0] shape:{hess_mat[0][0].shape}')\n",
    "        hess_norm = torch.tensor(0.).to(device)\n",
    "        for i in range(len(hess_mat)):\n",
    "            for j in range(len(hess_mat[0])):\n",
    "                hess_norm+= hess_mat[i][j].pow(2).sum()\n",
    "        hess_norm = hess_norm.sqrt()\n",
    "        # print(f'v2 hess norm{hess_norm}')\n",
    "        return hess_norm.cpu()\n",
    "        \n",
    "    def fit(self, train_loader, test_loader, epochs, store_grads=False, store_hessian=False, store_gen_err=False, store_weights=False, store_pt_loss=True, store_freq = 20, freq_reduce_by=None, freq_reduce_after=None):\n",
    "        \n",
    "        ## For Book keeping results ##\n",
    "        self.grads_norms = []\n",
    "        self.grads_normsv2 = []\n",
    "        self.param_list = []\n",
    "        self.hess_norms = []\n",
    "        self.gen_err = []\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        self.point_loss = []\n",
    "        ## Initializing values ##\n",
    "        terminate_training = False\n",
    "        store_count = 0\n",
    "        \n",
    "        for epoch in tqdm(range(epochs), total=epochs, unit=\"epoch\", disable=True):\n",
    "            if terminate_training == True:\n",
    "                break\n",
    "            for batch, (X, y) in tqdm(enumerate(train_loader), total=len(train_loader), unit='batch'):\n",
    "                if batch>details['training_step_limit']:\n",
    "                    terminate_training = True\n",
    "                    break\n",
    "                \n",
    "                X, y =X.to(device), y.to(device)\n",
    "                pred = self.model(X)\n",
    "                loss = self.loss_fn(pred, y)\n",
    "\n",
    "                # Backpropagation\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                ## Saving point loss\n",
    "                if store_pt_loss and (batch%store_freq==0):\n",
    "                    self.point_loss.append(loss.item())\n",
    "                    \n",
    "                ## Saving the weights\n",
    "                if store_weights and (batch%store_freq==0):\n",
    "                    current_params = tuple(self.model.parameters())\n",
    "                    self.param_list.append(current_params)\n",
    "                \n",
    "                ## computing and saving the gradient\n",
    "                if store_grads and (batch% store_freq == 0):\n",
    "                    # store_count += 1\n",
    "                    # # print(f'\\tstore_freq:{store_freq}, batch:{batch}')\n",
    "                    # if store_count%freq_reduce_after==0:\n",
    "                    #     store_freq += freq_reduce_by\n",
    "                    #     # print(f'store freq:{store_freq}, batch:{batch}')\n",
    "                    grad_norm_per_update = self.get_gradient()\n",
    "                    print('grad:', grad_norm_per_update)\n",
    "                    # print('\\tgrad norm:', grad_norm_per_update)\n",
    "                    self.grads_norms.append(grad_norm_per_update)\n",
    "                    # self.grads_normsv2.append(self.get_gradientv2(X,y))\n",
    "                ## computing and saving hessian\n",
    "                if store_hessian and (batch% store_freq==0):\n",
    "                    #assert False, \"Not implemented\"\n",
    "                    self.optimizer.zero_grad()\n",
    "                    hess_val = self.get_hessianv2(X,y)\n",
    "                    print('hess:',hess_val)\n",
    "                    self.hess_norms.append(hess_val)\n",
    "                    store_count += 1\n",
    "                    if store_count%freq_reduce_after==0:\n",
    "                        store_freq += freq_reduce_by\n",
    "                \n",
    "                ## computing and storing the generalization error\n",
    "                if store_gen_err and (batch% store_freq == 0):\n",
    "                    assert False, \"fix reducing freq to get it working and fastX, fasty\"\n",
    "                    train_loss, test_loss, point_loss=0, 0, 0\n",
    "                    with torch.no_grad():\n",
    "                        for sub_batch, (X_local,y_local) in enumerate(train_loader):\n",
    "                            # if sub_batch> batch: # only taking the encountered points to calculate train loss\n",
    "                            #     break\n",
    "                            X_local, y_local = X_local.to(device), y_local.to(device)\n",
    "                            pred_local = self.model(X_local)\n",
    "                            train_loss += self.loss_fn(pred_local, y_local).item()\n",
    "                    train_loss = train_loss/(batch+1)\n",
    "                    with torch.no_grad():\n",
    "                        for sub_batch, (X_local,y_local) in enumerate(test_loader):\n",
    "                            X_local, y_local = X_local.to(device), y_local.to(device)\n",
    "                            pred_local = self.model(X_local)\n",
    "                            test_loss += self.loss_fn(pred_local, y_local).item()\n",
    "                    test_batch_size = len(test_loader)\n",
    "                    # print(f\"Number of batches in test:{len(test_loader)}\")\n",
    "                    test_loss = test_loss/ len(test_loader)\n",
    "                    self.train_loss.append(train_loss)\n",
    "                    self.val_loss.append(test_loss)\n",
    "                \n",
    "                if batch % 1000 == 0:\n",
    "                    loss, current = loss.item(), batch * len(X)\n",
    "                    correct = 0\n",
    "                    test_loss = 0\n",
    "                    with torch.no_grad():\n",
    "                        for X, y in test_loader:\n",
    "                            X, y = X.to(device), y.to(device)\n",
    "                            pred = self.model(X)\n",
    "                            test_loss += self.loss_fn(pred, y).item()\n",
    "                            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "                    acc = 100*correct/len(test_loader.dataset)\n",
    "                    print(f\"\\taccuracy:{acc}\")#, at batch:{batch}\")\n",
    "                    print(f\"\\tloss: {loss:>7f}\")\n",
    "                \n",
    "                    # print(f'Learning rate:{self.scheduler.get_last_lr()}')\n",
    "                self.scheduler.step()\n",
    "            \n",
    "def exp_get_lp_sm(train_data_all, test_data_all, op_features, weight = 10, times = 8, epochs = 1, root_dir='', path=None, clear_file = True, freq_reduce_by=10, freq_reduce_after=100):\n",
    "    grad_list        = []\n",
    "    hess_norm_list   = []\n",
    "    if path is not None:\n",
    "        grad_file_path = root_dir+'grad_'+path\n",
    "        hess_file_path = root_dir+'hess_'+path\n",
    "        # gen_file_path = root_dir+'gen_'+path\n",
    "        if clear_file:\n",
    "            with open(grad_file_path, 'w+') as f:\n",
    "                f.write('')\n",
    "            with open(hess_file_path, 'w+') as f:\n",
    "                f.write('')\n",
    "    \n",
    "    train_loader, test_loader = get_random_subset(train_data_all, test_data_all)\n",
    "    for t in range(times):\n",
    "        print(f'Time:{t}')\n",
    "        train_model = Train_nn(784, weight, op_features, lr= details['alpha_0'], decay=False)\n",
    "        train_model.fit(train_loader, test_loader, epochs=epochs, store_grads=True, store_hessian=True, store_freq=details['book_keep_freq'],  store_gen_err=False, store_pt_loss=False, store_weights=False, freq_reduce_by = freq_reduce_by, freq_reduce_after=freq_reduce_after, )\n",
    "        \n",
    "        with open(grad_file_path,'a+') as f:\n",
    "            f.write(' '.join([str(grad) for grad in train_model.grads_norms]) + '\\n')\n",
    "        with open(hess_file_path,'a+') as f:\n",
    "            f.write(' '.join([str(hess) for hess in train_model.hess_norms]) + '\\n')\n",
    "        \n",
    "        hess_norm_list.append(train_model.hess_norms)\n",
    "        grad_list.append(train_model.grads_norms)\n",
    "         \n",
    "    return grad_list, hess_norm_list\n",
    "\n",
    "grad_list, hess_norm_list = exp_get_lp_sm(train_data_all, test_data_all, op_features=10, \n",
    "              weight=details['g_weight'], times=details['g_times'], \n",
    "              epochs=details['g_epochs'], root_dir=details['result_root_dir'], \n",
    "              path=details['result_path'], freq_reduce_by=details['freq_reduce_by'], \n",
    "              freq_reduce_after=details['freq_reduce_after'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c132d7-b5c6-44e8-9242-bd0fda72a2fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda69478-0505-4c08-880c-baee0b406ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e443d93-2ce3-4a4d-aa61-0e8a75b9f5a2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## For n =2k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63ab2744-6525-49b4-8380-b1506162d149",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected weight:25\n",
      "train data size:2000\n",
      "test data size:256\n",
      "Time:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fea14a9c9d24023a4fb8c78a3d63023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 364>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    360\u001b[0m         grad_list\u001b[38;5;241m.\u001b[39mappend(train_model\u001b[38;5;241m.\u001b[39mgrads_norms)\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grad_list, hess_norm_list\n\u001b[0;32m--> 364\u001b[0m grad_list, hess_norm_list \u001b[38;5;241m=\u001b[39m \u001b[43mexp_get_lp_sm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m              \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mg_weight\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mg_times\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m              \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mg_epochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresult_root_dir\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m              \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresult_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq_reduce_by\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfreq_reduce_by\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m              \u001b[49m\u001b[43mfreq_reduce_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfreq_reduce_after\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mexp_get_lp_sm\u001b[0;34m(train_data_all, test_data_all, op_features, weight, times, epochs, root_dir, path, clear_file, freq_reduce_by, freq_reduce_after)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    351\u001b[0m train_model \u001b[38;5;241m=\u001b[39m Train_nn(\u001b[38;5;241m784\u001b[39m, [weight], op_features, lr\u001b[38;5;241m=\u001b[39m details[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha_0\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 352\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_grads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_hessian\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbook_keep_freq\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mstore_gen_err\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_pt_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq_reduce_by\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfreq_reduce_by\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq_reduce_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreq_reduce_after\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(grad_file_path,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    355\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(grad) \u001b[38;5;28;01mfor\u001b[39;00m grad \u001b[38;5;129;01min\u001b[39;00m train_model\u001b[38;5;241m.\u001b[39mgrads_norms]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mTrain_nn.fit\u001b[0;34m(self, train_loader, test_loader, epochs, store_grads, store_hessian, store_gen_err, store_weights, store_pt_loss, store_freq, freq_reduce_by, freq_reduce_after)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m store_hessian \u001b[38;5;129;01mand\u001b[39;00m (batch\u001b[38;5;241m%\u001b[39m store_freq\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;66;03m#assert False, \"Not implemented\"\u001b[39;00m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 290\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhess_norms\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_hessianv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    291\u001b[0m     store_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m store_count\u001b[38;5;241m%\u001b[39mfreq_reduce_after\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mTrain_nn.get_hessianv2\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    221\u001b[0m     local_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(out, y)\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m local_loss\n\u001b[0;32m--> 223\u001b[0m hess_mat \u001b[38;5;241m=\u001b[39m \u001b[43mhessian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_fun_hess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;66;03m# print(f'len of hess mat:{len(hess_mat)}')\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# print(f'hess_mat[0] shape:{len(hess_mat[0])}')\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# print(f'hess_mat[0][0] shape:{hess_mat[0][0].shape}')\u001b[39;00m\n\u001b[1;32m    227\u001b[0m hess_norm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/functional.py:808\u001b[0m, in \u001b[0;36mhessian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, outer_jacobian_strategy)\u001b[0m\n\u001b[1;32m    805\u001b[0m     _check_requires_grad(jac, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacobian\u001b[39m\u001b[38;5;124m\"\u001b[39m, strict\u001b[38;5;241m=\u001b[39mstrict)\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jac\n\u001b[0;32m--> 808\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mjacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjac_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvectorize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m               \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mouter_jacobian_strategy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _tuple_postprocess(res, (is_inputs_tuple, is_inputs_tuple))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/functional.py:670\u001b[0m, in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[1;32m    668\u001b[0m jac_i: Tuple[List[torch\u001b[38;5;241m.\u001b[39mTensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m([] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs)))  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(out\u001b[38;5;241m.\u001b[39mnelement()):\n\u001b[0;32m--> 670\u001b[0m     vj \u001b[38;5;241m=\u001b[39m \u001b[43m_autograd_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m el_idx, (jac_i_el, vj_el, inp_el) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(jac_i, vj, inputs)):\n\u001b[1;32m    674\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m vj_el \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/functional.py:159\u001b[0m, in \u001b[0;36m_autograd_grad\u001b[0;34m(outputs, inputs, grad_outputs, create_graph, retain_graph, is_grads_batched)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m,) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_grad_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_grads_batched\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:261\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    256\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly_inputs argument is deprecated and is ignored now \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    257\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(defaults to True). To accumulate gradient for other \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    258\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparts of the graph, please use torch.autograd.backward.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    260\u001b[0m grad_outputs_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_outputs, \u001b[38;5;28mlen\u001b[39m(t_outputs))\n\u001b[0;32m--> 261\u001b[0m grad_outputs_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_grads_batched\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:33\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_grads\u001b[39m(outputs: Sequence[torch\u001b[38;5;241m.\u001b[39mTensor], grads: Sequence[_OptionalTensor],\n\u001b[1;32m     31\u001b[0m                 is_grads_batched: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[_OptionalTensor, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[1;32m     32\u001b[0m     new_grads: List[_OptionalTensor] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m out, grad \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(grad, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m     35\u001b[0m             grad_shape \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_grads_batched \u001b[38;5;28;01melse\u001b[39;00m grad\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd.functional import hessian\n",
    "import torch.utils.data as data_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from tqdm.notebook import tqdm \n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import seaborn as sns\n",
    "import os\n",
    "import copy\n",
    "from torch.nn.utils import _stateless\n",
    "\n",
    "batch_size = 1\n",
    "num_workers = 1\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "# device\n",
    "\n",
    "details = {}\n",
    "details['use_db'] = 'mnist'\n",
    "details['result_root_dir']='results/t0/'\n",
    "details['result_path']='try1_t6_r10'\n",
    "details['ratio'] = 10\n",
    "details['book_keep_freq'] = 20\n",
    "details['g_times'] = 8\n",
    "details['g_epochs'] = 1\n",
    "details['alpha_0']= 0.001\n",
    "details['freq_reduce_by'] = 20\n",
    "details['freq_reduce_after'] = 100\n",
    "\n",
    "details['training_step_limit'] = 100000 ## this is to train for max updates per epochs\n",
    "details['stop_hess_computation'] = 20000 ## Stop computing hessian after calculated these many times\n",
    "\n",
    "details['g_weight'] = int(details['ratio']*2000/(784+10))\n",
    "print(f'selected weight:{details[\"g_weight\"]}')\n",
    "\n",
    "with open(details['result_root_dir']+'details_'+details['result_path']+'.txt', 'w+') as f:\n",
    "    for key, val in details.items():\n",
    "        content = key + ' : '+str(val) + '\\n'\n",
    "        f.write(content)\n",
    "        \n",
    "torch.manual_seed(3407)\n",
    "np.random.seed(3407)\n",
    "torch.cuda.manual_seed_all(3407)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "# os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":4096:2\"\n",
    "\n",
    "train_data_all = torchvision.datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "test_data_all = torchvision.datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "# print(f'train data:{train_data}')\n",
    "# print(f'test data:{test_data}')\n",
    "\n",
    "def get_random_subset(train_data_all, test_data_all):    \n",
    "    # train_indices = torch.arange(20000)\n",
    "    test_indices = torch.arange(256)\n",
    "    train_indices = torch.randint(60000-1, (2000,))\n",
    "    # print(f'train indices:{train_indices[:10]}')\n",
    "    train_data = data_utils.Subset(train_data_all, train_indices)\n",
    "    test_data = data_utils.Subset(test_data_all, test_indices)\n",
    "    # print(f'train data:{train_data}')\n",
    "    # print(f'test data:{test_data}')\n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(0)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g,\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_data,\n",
    "        batch_size=256,\n",
    "        num_workers=num_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g,\n",
    "    )\n",
    "    print(f'train data size:{len(train_loader.dataset)}')\n",
    "    print(f'test data size:{len(test_loader.dataset)}')\n",
    "    # X_mat, y_mat = torch.Tensor(len(train_loader.dataset),784), torch.Tensor(len(train_loader.dataset)).long()\n",
    "    # for i, (data, label) in enumerate(train_loader):\n",
    "    #     X_mat[i] = data.flatten()\n",
    "    #     y_mat[i] = label.flatten()\n",
    "    # print(f'X_mat shape:{X_mat.shape}, y_mat shape:{y_mat.shape}')\n",
    "    return train_loader, test_loader #, X_mat, y_mat\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_features, hidden_layers, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = len(hidden_layers) + 1\n",
    "        self.total_params_len = 0\n",
    "        self.fc1 = nn.Linear(input_features, hidden_layers[0])\n",
    "        self.total_params_len += input_features*hidden_layers[0] + hidden_layers[0]\n",
    "        self.fc2 = nn.Linear(hidden_layers[0], output_size)\n",
    "        self.total_params_len += hidden_layers[0]*output_size + output_size\n",
    "        \n",
    "        ### Others required params\n",
    "        self.param_list = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        # print('x shape in forward',x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def fit(self, X, Y, X_val, Y_val, epochs, batch_size=1, **kwargs):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.SGD(self.parameters())\n",
    "        \n",
    "\n",
    "class Train_nn:\n",
    "    \n",
    "    def __init__(self, input_features, hidden_layers, output_size, lr):\n",
    "        self.model = Net(input_features, hidden_layers=hidden_layers, output_size=output_size)\n",
    "        self.model.to(device)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda= lambda it: 1/(it+1))\n",
    "        \n",
    "    def get_loss(self, X, y, params=None):\n",
    "        # if params is not None:\n",
    "        assert False, \"Model not initialized with given params\"\n",
    "        op = self.model(X)\n",
    "        loss = self.loss_fn(pred, y)\n",
    "        return loss\n",
    "    \n",
    "    def get_gradient(self):\n",
    "        params = (self.model.parameters())\n",
    "        grad_norm_sq = torch.tensor(0, dtype=float).to(device)\n",
    "        # print('grad Norm init:', grad_norm_sq)\n",
    "        for param in self.model.parameters():\n",
    "            temp = param.grad.data.pow(2).sum()\n",
    "            # print(f'param grad norm \\n\\tsum:{temp.data}')#,\\n\\tshape:{param.shape}')\n",
    "            grad_norm_sq += temp\n",
    "            \n",
    "        return grad_norm_sq.sqrt().cpu()\n",
    "    \n",
    "    def try_operator_norm(self, hess_mat):\n",
    "        for i in len(hess_mat):\n",
    "            for j in len(hess_mat[0]):\n",
    "                torch.unsqueeze(hess_mat[i][i],0)\n",
    "        hess_tensor_dim = list(hess_mat[0][0].shape)\n",
    "        hess_tensor_dim += [n*2,n*2]\n",
    "        hess_mat_np = np.zeros(shape=hess_tensor_dim)\n",
    "        hess_tensor = torch.tensor(hess_mat_np)\n",
    "        torch.cat(hess_mat, out=hess_tensor)\n",
    "        \n",
    "        hess_mat.reshpe(n*2,n*2)\n",
    "        hess_norm = torch.linalg.norm(hess_mat, 2)\n",
    "        assert False, \"Not working\"\n",
    "    \n",
    "    def get_hessian(self, X, y):\n",
    "        prev_params = copy.deepcopy(list(self.model.parameters()))\n",
    "        n = self.model.layers\n",
    "        def local_model(*params):\n",
    "            # print(f'len of params:{len(params)}')\n",
    "            # print(f'shape of params[0]:{params[0].shape}')\n",
    "            # with torch.no_grad():\n",
    "            #initialize model with given params\n",
    "            i = 0\n",
    "            for i, param in enumerate(self.model.parameters()):\n",
    "                param.data = params[i]\n",
    "            pred = self.model(X)\n",
    "            loss = self.loss_fn(pred, y)\n",
    "            # print(f'loss type:{type(loss)}')\n",
    "            return loss\n",
    "        p =list(self.model.parameters())\n",
    "        hess_mat = hessian(local_model, tuple(p))\n",
    "        hess_norm = torch.tensor(0.).to(device)\n",
    "        for i in range(len(hess_mat)):\n",
    "            for j in range(len(hess_mat[0])):\n",
    "                hess_norm+= hess_mat[i][j].pow(2).sum()\n",
    "        \n",
    "        # print(f'Hess mat len:{len(hess_mat)}')\n",
    "        # print(f'Hess mat[0] len:{len(hess_mat[0])}')\n",
    "        # print(f'Hess mat[0][0] shape:{hess_mat[0][0].shape}')\n",
    "        \n",
    "        hess_norm = hess_norm.sqrt()\n",
    "        # print(f'hess norm:{hess_norm}')\n",
    "        \n",
    "        # Reinitialize the original params to model\n",
    "        for i, param in enumerate(self.model.parameters()):\n",
    "                param.data = prev_params[i]\n",
    "        \n",
    "        return hess_norm\n",
    "    \n",
    "    def get_hessianv2(self, X,y):\n",
    "        names = list(n for n, _ in self.model.named_parameters())\n",
    "        def loss_fun_hess(*params):\n",
    "            out: torch.Tensor = _stateless.functional_call(self.model, {n: p for n, p in zip(names, params)}, X)\n",
    "            local_loss = self.loss_fn(out, y)\n",
    "            return local_loss\n",
    "        hess_mat = hessian(loss_fun_hess, tuple(self.model.parameters()))\n",
    "        # print(f'len of hess mat:{len(hess_mat)}')\n",
    "        # print(f'hess_mat[0] shape:{len(hess_mat[0])}')\n",
    "        # print(f'hess_mat[0][0] shape:{hess_mat[0][0].shape}')\n",
    "        hess_norm = torch.tensor(0.).to(device)\n",
    "        for i in range(len(hess_mat)):\n",
    "            for j in range(len(hess_mat[0])):\n",
    "                hess_norm+= hess_mat[i][j].pow(2).sum()\n",
    "        hess_norm = hess_norm.sqrt()\n",
    "        # print(f'v2 hess norm{hess_norm}')\n",
    "        return hess_norm.cpu()\n",
    "        \n",
    "    def fit(self, train_loader, test_loader, epochs, store_grads=False, store_hessian=False, store_gen_err=False, store_weights=False, store_pt_loss=True, store_freq = 20, freq_reduce_by=None, freq_reduce_after=None):\n",
    "        \n",
    "        ## For Book keeping results ##\n",
    "        self.grads_norms = []\n",
    "        self.param_list = []\n",
    "        self.hess_norms = []\n",
    "        self.gen_err = []\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        self.point_loss = []\n",
    "        ## Initializing values ##\n",
    "        terminate_training = False\n",
    "        store_count = 0\n",
    "        \n",
    "        for epoch in tqdm(range(epochs), total=epochs, unit=\"epoch\", disable=True):\n",
    "            if terminate_training == True:\n",
    "                break\n",
    "            for batch, (X, y) in tqdm(enumerate(train_loader), total=len(train_loader), unit='batch'):\n",
    "                # if batch>300:\n",
    "                #     terminate_training = True\n",
    "                #     break\n",
    "                \n",
    "                X, y =X.to(device), y.to(device)\n",
    "                pred = self.model(X)\n",
    "                loss = self.loss_fn(pred, y)\n",
    "\n",
    "                # Backpropagation\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                ## Saving point loss\n",
    "                if store_pt_loss and (batch%store_freq==0):\n",
    "                    self.point_loss.append(loss.item())\n",
    "                    \n",
    "                ## Saving the weights\n",
    "                if store_weights and (batch%store_freq==0):\n",
    "                    current_params = tuple(self.model.parameters())\n",
    "                    self.param_list.append(current_params)\n",
    "                \n",
    "                ## computing and saving the gradient\n",
    "                if store_grads and (batch% store_freq == 0):\n",
    "                    # store_count += 1\n",
    "                    # # print(f'\\tstore_freq:{store_freq}, batch:{batch}')\n",
    "                    # if store_count%freq_reduce_after==0:\n",
    "                    #     store_freq += freq_reduce_by\n",
    "                    #     # print(f'store freq:{store_freq}, batch:{batch}')\n",
    "                    grad_norm_per_update = self.get_gradient()\n",
    "                    # print('\\tgrad norm:', grad_norm_per_update)\n",
    "                    self.grads_norms.append(grad_norm_per_update)\n",
    "                    \n",
    "                ## computing and saving hessian\n",
    "                if store_hessian and (batch% store_freq==0):\n",
    "                    #assert False, \"Not implemented\"\n",
    "                    self.optimizer.zero_grad()\n",
    "                    self.hess_norms.append(self.get_hessianv2(X,y))\n",
    "                    store_count += 1\n",
    "                    if store_count%freq_reduce_after==0:\n",
    "                        store_freq += freq_reduce_by\n",
    "                \n",
    "                ## computing and storing the generalization error\n",
    "                if store_gen_err and (batch% store_freq == 0):\n",
    "                    assert False, \"fix reducing freq to get it working\"\n",
    "                    train_loss, test_loss, point_loss=0, 0, 0\n",
    "                    with torch.no_grad():\n",
    "                        for sub_batch, (X_local,y_local) in enumerate(train_loader):\n",
    "                            # if sub_batch> batch: # only taking the encountered points to calculate train loss\n",
    "                            #     break\n",
    "                            X_local, y_local = X_local.to(device), y_local.to(device)\n",
    "                            pred_local = self.model(X_local)\n",
    "                            train_loss += self.loss_fn(pred_local, y_local).item()\n",
    "                    train_loss = train_loss/(batch+1)\n",
    "                    with torch.no_grad():\n",
    "                        for sub_batch, (X_local,y_local) in enumerate(test_loader):\n",
    "                            X_local, y_local = X_local.to(device), y_local.to(device)\n",
    "                            pred_local = self.model(X_local)\n",
    "                            test_loss += self.loss_fn(pred_local, y_local).item()\n",
    "                    test_batch_size = len(test_loader)\n",
    "                    # print(f\"Number of batches in test:{len(test_loader)}\")\n",
    "                    test_loss = test_loss/ len(test_loader)\n",
    "                    self.train_loss.append(train_loss)\n",
    "                    self.val_loss.append(test_loss)\n",
    "                \n",
    "                if batch % 10000 == 9999:\n",
    "                    loss, current = loss.item(), batch * len(X)\n",
    "                    correct = 0\n",
    "                    test_loss = 0\n",
    "                    with torch.no_grad():\n",
    "                        for X, y in test_loader:\n",
    "                            X, y = X.to(device), y.to(device)\n",
    "                            pred = self.model(X)\n",
    "                            test_loss += self.loss_fn(pred, y).item()\n",
    "                            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "                    acc = 100*correct/len(test_loader.dataset)\n",
    "                    print(f\"\\taccuracy:{acc}\")#, at batch:{batch}\")\n",
    "                    print(f\"\\tloss: {loss:>7f}\")\n",
    "                \n",
    "                    # print(f'Learning rate:{self.scheduler.get_last_lr()}')\n",
    "                self.scheduler.step()\n",
    "            \n",
    "def exp_get_lp_sm(train_data_all, test_data_all, op_features, weight = 10, times = 8, epochs = 1, root_dir='', path=None, clear_file = True, freq_reduce_by=10, freq_reduce_after=100):\n",
    "    grad_list        = []\n",
    "    hess_norm_list   = []\n",
    "    if path is not None:\n",
    "        grad_file_path = root_dir+'grad_'+path\n",
    "        hess_file_path = root_dir+'hess_'+path\n",
    "        # gen_file_path = root_dir+'gen_'+path\n",
    "        if clear_file:\n",
    "            with open(grad_file_path, 'w+') as f:\n",
    "                f.write('')\n",
    "            with open(hess_file_path, 'w+') as f:\n",
    "                f.write('')\n",
    "    \n",
    "    train_loader, test_loader = get_random_subset(train_data_all, test_data_all)\n",
    "    for t in range(times):\n",
    "        print(f'Time:{t}')\n",
    "        train_model = Train_nn(784, [weight], op_features, lr= details['alpha_0'])\n",
    "        train_model.fit(train_loader, test_loader, epochs=epochs, store_grads=True, store_hessian=True, store_freq=details['book_keep_freq'],  store_gen_err=False, store_pt_loss=False, store_weights=False, freq_reduce_by = freq_reduce_by, freq_reduce_after=freq_reduce_after)\n",
    "        \n",
    "        with open(grad_file_path,'a+') as f:\n",
    "            f.write(' '.join([str(grad) for grad in train_model.grads_norms]) + '\\n')\n",
    "        with open(hess_file_path,'a+') as f:\n",
    "            f.write(' '.join([str(hess) for hess in train_model.hess_norms]) + '\\n')\n",
    "        \n",
    "        hess_norm_list.append(train_model.hess_norms)\n",
    "        grad_list.append(train_model.grads_norms)\n",
    "        \n",
    "    return grad_list, hess_norm_list\n",
    "\n",
    "grad_list, hess_norm_list = exp_get_lp_sm(train_data_all, test_data_all, op_features=10, \n",
    "              weight=details['g_weight'], times=details['g_times'], \n",
    "              epochs=details['g_epochs'], root_dir=details['result_root_dir'], \n",
    "              path=details['result_path'], freq_reduce_by=details['freq_reduce_by'], \n",
    "              freq_reduce_after=details['freq_reduce_after'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c712f703-aade-48e2-a73b-5e8a06cd39ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c25ff685-b974-4535-9bd3-60942c69a995",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## For n=2k and depth inc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56559357-a5ab-49d1-b01b-070f27ee5ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected weight:[18, 18]\n",
      "train data size:2000\n",
      "test data size:256\n",
      "Time:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "244bdecac3f3444e9234389919818413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 390>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    386\u001b[0m         grad_list\u001b[38;5;241m.\u001b[39mappend(train_model\u001b[38;5;241m.\u001b[39mgrads_norms)\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grad_list, hess_norm_list\n\u001b[0;32m--> 390\u001b[0m grad_list, hess_norm_list \u001b[38;5;241m=\u001b[39m \u001b[43mexp_get_lp_sm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m              \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mg_weight\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mg_times\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m              \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mg_epochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresult_root_dir\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m              \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresult_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq_reduce_by\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfreq_reduce_by\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m              \u001b[49m\u001b[43mfreq_reduce_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfreq_reduce_after\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mexp_get_lp_sm\u001b[0;34m(train_data_all, test_data_all, op_features, weight, times, epochs, root_dir, path, clear_file, freq_reduce_by, freq_reduce_after)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    377\u001b[0m train_model \u001b[38;5;241m=\u001b[39m Train_nn(\u001b[38;5;241m784\u001b[39m, weight, op_features, lr\u001b[38;5;241m=\u001b[39m details[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha_0\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 378\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_grads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_hessian\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbook_keep_freq\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mstore_gen_err\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_pt_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq_reduce_by\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfreq_reduce_by\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq_reduce_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreq_reduce_after\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(grad_file_path,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    381\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(grad) \u001b[38;5;28;01mfor\u001b[39;00m grad \u001b[38;5;129;01min\u001b[39;00m train_model\u001b[38;5;241m.\u001b[39mgrads_norms]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mTrain_nn.fit\u001b[0;34m(self, train_loader, test_loader, epochs, store_grads, store_hessian, store_gen_err, store_weights, store_pt_loss, store_freq, freq_reduce_by, freq_reduce_after)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m store_hessian \u001b[38;5;129;01mand\u001b[39;00m (batch\u001b[38;5;241m%\u001b[39m store_freq\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;66;03m#assert False, \"Not implemented\"\u001b[39;00m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhess_norms\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_hessianv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    317\u001b[0m     store_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m store_count\u001b[38;5;241m%\u001b[39mfreq_reduce_after\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mTrain_nn.get_hessianv2\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    246\u001b[0m     local_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(out, y)\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m local_loss\n\u001b[0;32m--> 248\u001b[0m hess_mat \u001b[38;5;241m=\u001b[39m \u001b[43mhessian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_fun_hess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# print(f'len of hess mat:{len(hess_mat)}')\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# print(f'hess_mat[0] shape:{len(hess_mat[0])}')\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# print(f'hess_mat[0][0] shape:{hess_mat[0][0].shape}')\u001b[39;00m\n\u001b[1;32m    252\u001b[0m hess_norm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/functional.py:808\u001b[0m, in \u001b[0;36mhessian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, outer_jacobian_strategy)\u001b[0m\n\u001b[1;32m    805\u001b[0m     _check_requires_grad(jac, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacobian\u001b[39m\u001b[38;5;124m\"\u001b[39m, strict\u001b[38;5;241m=\u001b[39mstrict)\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jac\n\u001b[0;32m--> 808\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mjacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjac_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvectorize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m               \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mouter_jacobian_strategy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _tuple_postprocess(res, (is_inputs_tuple, is_inputs_tuple))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/functional.py:670\u001b[0m, in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[1;32m    668\u001b[0m jac_i: Tuple[List[torch\u001b[38;5;241m.\u001b[39mTensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m([] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs)))  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(out\u001b[38;5;241m.\u001b[39mnelement()):\n\u001b[0;32m--> 670\u001b[0m     vj \u001b[38;5;241m=\u001b[39m \u001b[43m_autograd_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m el_idx, (jac_i_el, vj_el, inp_el) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(jac_i, vj, inputs)):\n\u001b[1;32m    674\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m vj_el \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/functional.py:159\u001b[0m, in \u001b[0;36m_autograd_grad\u001b[0;34m(outputs, inputs, grad_outputs, create_graph, retain_graph, is_grads_batched)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m,) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_grad_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_grads_batched\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:276\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd.functional import hessian\n",
    "import torch.utils.data as data_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from tqdm.notebook import tqdm \n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import seaborn as sns\n",
    "import os\n",
    "import copy\n",
    "from torch.nn.utils import _stateless\n",
    "\n",
    "batch_size = 1\n",
    "num_workers = 1\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "# device\n",
    "\n",
    "details = {}\n",
    "details['use_db'] = 'mnist'\n",
    "details['result_root_dir']='results/t0/'\n",
    "details['result_path']='try1_t6_r15'\n",
    "details['ratio'] = 15\n",
    "details['book_keep_freq'] = 20\n",
    "details['g_times'] = 8\n",
    "details['g_epochs'] = 1\n",
    "details['alpha_0']= 0.001\n",
    "details['freq_reduce_by'] = 20\n",
    "details['freq_reduce_after'] = 100\n",
    "\n",
    "details['training_step_limit'] = 100000 ## this is to train for max updates per epochs\n",
    "details['stop_hess_computation'] = 20000 ## Stop computing hessian after calculated these many times\n",
    "\n",
    "details['g_weight'] = [int(details['ratio']*1000/(784+10)),int(details['ratio']*1000/(784+10))] \n",
    "print(f'selected weight:{details[\"g_weight\"]}')\n",
    "\n",
    "with open(details['result_root_dir']+'details_'+details['result_path']+'.txt', 'w+') as f:\n",
    "    for key, val in details.items():\n",
    "        content = key + ' : '+str(val) + '\\n'\n",
    "        f.write(content)\n",
    "        \n",
    "torch.manual_seed(3407)\n",
    "np.random.seed(3407)\n",
    "torch.cuda.manual_seed_all(3407)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "# os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":4096:2\"\n",
    "\n",
    "train_data_all = torchvision.datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "test_data_all = torchvision.datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "# print(f'train data:{train_data}')\n",
    "# print(f'test data:{test_data}')\n",
    "\n",
    "def get_random_subset(train_data_all, test_data_all):    \n",
    "    # train_indices = torch.arange(20000)\n",
    "    test_indices = torch.arange(256)\n",
    "    train_indices = torch.randint(60000-1, (2000,))\n",
    "    # print(f'train indices:{train_indices[:10]}')\n",
    "    train_data = data_utils.Subset(train_data_all, train_indices)\n",
    "    test_data = data_utils.Subset(test_data_all, test_indices)\n",
    "    # print(f'train data:{train_data}')\n",
    "    # print(f'test data:{test_data}')\n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(0)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g,\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_data,\n",
    "        batch_size=256,\n",
    "        num_workers=num_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g,\n",
    "    )\n",
    "    print(f'train data size:{len(train_loader.dataset)}')\n",
    "    print(f'test data size:{len(test_loader.dataset)}')\n",
    "    # X_mat, y_mat = torch.Tensor(len(train_loader.dataset),784), torch.Tensor(len(train_loader.dataset)).long()\n",
    "    # for i, (data, label) in enumerate(train_loader):\n",
    "    #     X_mat[i] = data.flatten()\n",
    "    #     y_mat[i] = label.flatten()\n",
    "    # print(f'X_mat shape:{X_mat.shape}, y_mat shape:{y_mat.shape}')\n",
    "    return train_loader, test_loader #, X_mat, y_mat\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_features, hidden_layers, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = len(hidden_layers) + 1\n",
    "        self.total_params_len = 0\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        prev_weight = input_features\n",
    "        \n",
    "        for i, weight in enumerate(hidden_layers):\n",
    "            self.fc_layers.append(nn.Linear(prev_weight, weight))\n",
    "            self.total_params_len += prev_weight*weight + weight\n",
    "            prev_weight = weight\n",
    "        \n",
    "        self.fc_last = nn.Linear(hidden_layers[-1], output_size)\n",
    "        self.total_params_len += hidden_layers[-1]*output_size + output_size\n",
    "        \n",
    "        ### Others required params\n",
    "        self.param_list = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        # print('x shape in forward',x.shape)\n",
    "        for fc_layer in self.fc_layers:\n",
    "            x = F.relu(fc_layer(x))\n",
    "        x = self.fc_last(x)\n",
    "        return x\n",
    "    \n",
    "    def fit(self, X, Y, X_val, Y_val, epochs, batch_size=1, **kwargs):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.SGD(self.parameters())\n",
    "        \n",
    "\n",
    "class Train_nn:\n",
    "    \n",
    "    def __init__(self, input_features, hidden_layers, output_size, lr):\n",
    "        self.model = Net(input_features, hidden_layers=hidden_layers, output_size=output_size)\n",
    "        self.model.to(device)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda= lambda it: 1/(it+1))\n",
    "        \n",
    "    def get_loss(self, X, y, params=None):\n",
    "        # if params is not None:\n",
    "        assert False, \"Model not initialized with given params\"\n",
    "        op = self.model(X)\n",
    "        loss = self.loss_fn(pred, y)\n",
    "        return loss\n",
    "    \n",
    "    def get_gradient(self):\n",
    "        params = (self.model.parameters())\n",
    "        grad_norm_sq = torch.tensor(0, dtype=float).to(device)\n",
    "        # print('grad Norm init:', grad_norm_sq)\n",
    "        for param in self.model.parameters():\n",
    "            temp = param.grad.data.pow(2).sum()\n",
    "            # print(f'param grad norm \\n\\tsum:{temp.data}')#,\\n\\tshape:{param.shape}')\n",
    "            grad_norm_sq += temp\n",
    "            \n",
    "        return grad_norm_sq.sqrt().cpu()\n",
    "    \n",
    "    def get_gradientv2(self, X, y):\n",
    "        names = list(n for n, _ in self.model.named_parameters())\n",
    "        def loss_fun_grad(*params):\n",
    "            out: torch.Tensor = _stateless.functional_call(self.model, {n: p for n, p in zip(names, params)}, X)\n",
    "            local_loss = self.loss_fn(out, y)\n",
    "            return local_loss\n",
    "        grad_mat = torch.autograd.grad(loss_fun_grad, tuple(self.model.parameters()))\n",
    "        # print(f'len of hess mat:{len(hess_mat)}')\n",
    "        # print(f'hess_mat[0] shape:{len(hess_mat[0])}')\n",
    "        # print(f'hess_mat[0][0] shape:{hess_mat[0][0].shape}')\n",
    "        grad_norm = torch.tensor(0.).to(device)\n",
    "        for i in range(len(grad_mat)):\n",
    "            for j in range(len(grad_mat[0])):\n",
    "                grad_norm+= grad_mat[i][j].pow(2).sum()\n",
    "        grad_norm = grad_norm.sqrt()\n",
    "        # print(f'v2 hess norm{hess_norm}')\n",
    "        return grad_norm.cpu()\n",
    "    \n",
    "    def try_operator_norm(self, hess_mat):\n",
    "        for i in len(hess_mat):\n",
    "            for j in len(hess_mat[0]):\n",
    "                torch.unsqueeze(hess_mat[i][i],0)\n",
    "        hess_tensor_dim = list(hess_mat[0][0].shape)\n",
    "        hess_tensor_dim += [n*2,n*2]\n",
    "        hess_mat_np = np.zeros(shape=hess_tensor_dim)\n",
    "        hess_tensor = torch.tensor(hess_mat_np)\n",
    "        torch.cat(hess_mat, out=hess_tensor)\n",
    "        \n",
    "        hess_mat.reshpe(n*2,n*2)\n",
    "        hess_norm = torch.linalg.norm(hess_mat, 2)\n",
    "        assert False, \"Not working\"\n",
    "    \n",
    "    def get_hessian(self, X, y):\n",
    "        prev_params = copy.deepcopy(list(self.model.parameters()))\n",
    "        n = self.model.layers\n",
    "        def local_model(*params):\n",
    "            # print(f'len of params:{len(params)}')\n",
    "            # print(f'shape of params[0]:{params[0].shape}')\n",
    "            # with torch.no_grad():\n",
    "            #initialize model with given params\n",
    "            i = 0\n",
    "            for i, param in enumerate(self.model.parameters()):\n",
    "                param.data = params[i]\n",
    "            pred = self.model(X)\n",
    "            loss = self.loss_fn(pred, y)\n",
    "            # print(f'loss type:{type(loss)}')\n",
    "            return loss\n",
    "        p =list(self.model.parameters())\n",
    "        hess_mat = hessian(local_model, tuple(p))\n",
    "        hess_norm = torch.tensor(0.).to(device)\n",
    "        for i in range(len(hess_mat)):\n",
    "            for j in range(len(hess_mat[0])):\n",
    "                hess_norm+= hess_mat[i][j].pow(2).sum()\n",
    "        \n",
    "        # print(f'Hess mat len:{len(hess_mat)}')\n",
    "        # print(f'Hess mat[0] len:{len(hess_mat[0])}')\n",
    "        # print(f'Hess mat[0][0] shape:{hess_mat[0][0].shape}')\n",
    "        \n",
    "        hess_norm = hess_norm.sqrt()\n",
    "        # print(f'hess norm:{hess_norm}')\n",
    "        \n",
    "        # Reinitialize the original params to model\n",
    "        for i, param in enumerate(self.model.parameters()):\n",
    "                param.data = prev_params[i]\n",
    "        \n",
    "        return hess_norm\n",
    "    \n",
    "    def get_hessianv2(self, X,y):\n",
    "        names = list(n for n, _ in self.model.named_parameters())\n",
    "        def loss_fun_hess(*params):\n",
    "            out: torch.Tensor = _stateless.functional_call(self.model, {n: p for n, p in zip(names, params)}, X)\n",
    "            local_loss = self.loss_fn(out, y)\n",
    "            return local_loss\n",
    "        hess_mat = hessian(loss_fun_hess, tuple(self.model.parameters()))\n",
    "        # print(f'len of hess mat:{len(hess_mat)}')\n",
    "        # print(f'hess_mat[0] shape:{len(hess_mat[0])}')\n",
    "        # print(f'hess_mat[0][0] shape:{hess_mat[0][0].shape}')\n",
    "        hess_norm = torch.tensor(0.).to(device)\n",
    "        for i in range(len(hess_mat)):\n",
    "            for j in range(len(hess_mat[0])):\n",
    "                hess_norm+= hess_mat[i][j].pow(2).sum()\n",
    "        hess_norm = hess_norm.sqrt()\n",
    "        # print(f'v2 hess norm{hess_norm}')\n",
    "        return hess_norm.cpu()\n",
    "        \n",
    "    def fit(self, train_loader, test_loader, epochs, store_grads=False, store_hessian=False, store_gen_err=False, store_weights=False, store_pt_loss=True, store_freq = 20, freq_reduce_by=None, freq_reduce_after=None):\n",
    "        \n",
    "        ## For Book keeping results ##\n",
    "        self.grads_norms = []\n",
    "        self.grads_normsv2 = []\n",
    "        self.param_list = []\n",
    "        self.hess_norms = []\n",
    "        self.gen_err = []\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        self.point_loss = []\n",
    "        ## Initializing values ##\n",
    "        terminate_training = False\n",
    "        store_count = 0\n",
    "        \n",
    "        for epoch in tqdm(range(epochs), total=epochs, unit=\"epoch\", disable=True):\n",
    "            if terminate_training == True:\n",
    "                break\n",
    "            for batch, (X, y) in tqdm(enumerate(train_loader), total=len(train_loader), unit='batch'):\n",
    "                # if batch>300:\n",
    "                #     terminate_training = True\n",
    "                #     break\n",
    "                \n",
    "                X, y =X.to(device), y.to(device)\n",
    "                pred = self.model(X)\n",
    "                loss = self.loss_fn(pred, y)\n",
    "\n",
    "                # Backpropagation\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                ## Saving point loss\n",
    "                if store_pt_loss and (batch%store_freq==0):\n",
    "                    self.point_loss.append(loss.item())\n",
    "                    \n",
    "                ## Saving the weights\n",
    "                if store_weights and (batch%store_freq==0):\n",
    "                    current_params = tuple(self.model.parameters())\n",
    "                    self.param_list.append(current_params)\n",
    "                \n",
    "                ## computing and saving the gradient\n",
    "                if store_grads and (batch% store_freq == 0):\n",
    "                    # store_count += 1\n",
    "                    # # print(f'\\tstore_freq:{store_freq}, batch:{batch}')\n",
    "                    # if store_count%freq_reduce_after==0:\n",
    "                    #     store_freq += freq_reduce_by\n",
    "                    #     # print(f'store freq:{store_freq}, batch:{batch}')\n",
    "                    grad_norm_per_update = self.get_gradient()\n",
    "                    # print('\\tgrad norm:', grad_norm_per_update)\n",
    "                    self.grads_norms.append(grad_norm_per_update)\n",
    "                    # self.grads_normsv2.append(self.get_gradientv2(X,y))\n",
    "                ## computing and saving hessian\n",
    "                if store_hessian and (batch% store_freq==0):\n",
    "                    #assert False, \"Not implemented\"\n",
    "                    self.optimizer.zero_grad()\n",
    "                    self.hess_norms.append(self.get_hessianv2(X,y))\n",
    "                    store_count += 1\n",
    "                    if store_count%freq_reduce_after==0:\n",
    "                        store_freq += freq_reduce_by\n",
    "                \n",
    "                ## computing and storing the generalization error\n",
    "                if store_gen_err and (batch% store_freq == 0):\n",
    "                    assert False, \"fix reducing freq to get it working\"\n",
    "                    train_loss, test_loss, point_loss=0, 0, 0\n",
    "                    with torch.no_grad():\n",
    "                        for sub_batch, (X_local,y_local) in enumerate(train_loader):\n",
    "                            # if sub_batch> batch: # only taking the encountered points to calculate train loss\n",
    "                            #     break\n",
    "                            X_local, y_local = X_local.to(device), y_local.to(device)\n",
    "                            pred_local = self.model(X_local)\n",
    "                            train_loss += self.loss_fn(pred_local, y_local).item()\n",
    "                    train_loss = train_loss/(batch+1)\n",
    "                    with torch.no_grad():\n",
    "                        for sub_batch, (X_local,y_local) in enumerate(test_loader):\n",
    "                            X_local, y_local = X_local.to(device), y_local.to(device)\n",
    "                            pred_local = self.model(X_local)\n",
    "                            test_loss += self.loss_fn(pred_local, y_local).item()\n",
    "                    test_batch_size = len(test_loader)\n",
    "                    # print(f\"Number of batches in test:{len(test_loader)}\")\n",
    "                    test_loss = test_loss/ len(test_loader)\n",
    "                    self.train_loss.append(train_loss)\n",
    "                    self.val_loss.append(test_loss)\n",
    "                \n",
    "                if batch % 10000 == 9999:\n",
    "                    loss, current = loss.item(), batch * len(X)\n",
    "                    correct = 0\n",
    "                    test_loss = 0\n",
    "                    with torch.no_grad():\n",
    "                        for X, y in test_loader:\n",
    "                            X, y = X.to(device), y.to(device)\n",
    "                            pred = self.model(X)\n",
    "                            test_loss += self.loss_fn(pred, y).item()\n",
    "                            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "                    acc = 100*correct/len(test_loader.dataset)\n",
    "                    print(f\"\\taccuracy:{acc}\")#, at batch:{batch}\")\n",
    "                    print(f\"\\tloss: {loss:>7f}\")\n",
    "                \n",
    "                    # print(f'Learning rate:{self.scheduler.get_last_lr()}')\n",
    "                self.scheduler.step()\n",
    "            \n",
    "def exp_get_lp_sm(train_data_all, test_data_all, op_features, weight = 10, times = 8, epochs = 1, root_dir='', path=None, clear_file = True, freq_reduce_by=10, freq_reduce_after=100):\n",
    "    grad_list        = []\n",
    "    hess_norm_list   = []\n",
    "    if path is not None:\n",
    "        grad_file_path = root_dir+'grad_'+path\n",
    "        hess_file_path = root_dir+'hess_'+path\n",
    "        # gen_file_path = root_dir+'gen_'+path\n",
    "        if clear_file:\n",
    "            with open(grad_file_path, 'w+') as f:\n",
    "                f.write('')\n",
    "            with open(hess_file_path, 'w+') as f:\n",
    "                f.write('')\n",
    "    \n",
    "    train_loader, test_loader = get_random_subset(train_data_all, test_data_all)\n",
    "    for t in range(times):\n",
    "        print(f'Time:{t}')\n",
    "        train_model = Train_nn(784, weight, op_features, lr= details['alpha_0'])\n",
    "        train_model.fit(train_loader, test_loader, epochs=epochs, store_grads=True, store_hessian=True, store_freq=details['book_keep_freq'],  store_gen_err=False, store_pt_loss=False, store_weights=False, freq_reduce_by = freq_reduce_by, freq_reduce_after=freq_reduce_after)\n",
    "        \n",
    "        with open(grad_file_path,'a+') as f:\n",
    "            f.write(' '.join([str(grad) for grad in train_model.grads_norms]) + '\\n')\n",
    "        with open(hess_file_path,'a+') as f:\n",
    "            f.write(' '.join([str(hess) for hess in train_model.hess_norms]) + '\\n')\n",
    "        \n",
    "        hess_norm_list.append(train_model.hess_norms)\n",
    "        grad_list.append(train_model.grads_norms)\n",
    "         \n",
    "    return grad_list, hess_norm_list\n",
    "\n",
    "grad_list, hess_norm_list = exp_get_lp_sm(train_data_all, test_data_all, op_features=10, \n",
    "              weight=details['g_weight'], times=details['g_times'], \n",
    "              epochs=details['g_epochs'], root_dir=details['result_root_dir'], \n",
    "              path=details['result_path'], freq_reduce_by=details['freq_reduce_by'], \n",
    "              freq_reduce_after=details['freq_reduce_after'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1ab45c-8063-43f6-92a0-32eb0333b5aa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## For fixed w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c81819-717a-4f07-ad62-c8122c7831b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd.functional import hessian\n",
    "import torch.utils.data as data_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from tqdm.notebook import tqdm \n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import seaborn as sns\n",
    "import os\n",
    "import copy\n",
    "from torch.nn.utils import _stateless\n",
    "\n",
    "batch_size = 1\n",
    "num_workers = 1\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "# device\n",
    "\n",
    "details = {}\n",
    "details['use_db'] = 'mnist'\n",
    "details['result_root_dir']='results/t0/'\n",
    "details['result_path']='try1_t6_r1.0'\n",
    "details['ratio'] = 1.0\n",
    "details['book_keep_freq'] = 20\n",
    "details['g_times'] = 8\n",
    "details['g_epochs'] = 1\n",
    "details['alpha_0']= 0.001\n",
    "details['freq_reduce_by'] = 20\n",
    "details['freq_reduce_after'] = 100\n",
    "\n",
    "details['training_step_limit'] = 100000 ## this is to train for max updates per epochs\n",
    "details['stop_hess_computation'] = 20000 ## Stop computing hessian after calculated these many times\n",
    "\n",
    "details['g_weight'] = int(details['ratio']*20000/(784+10))\n",
    "print(f'selected weight:{details[\"g_weight\"]}')\n",
    "\n",
    "with open(details['result_root_dir']+'details_'+details['result_path']+'.txt', 'w+') as f:\n",
    "    for key, val in details.items():\n",
    "        content = key + ' : '+str(val) + '\\n'\n",
    "        f.write(content)\n",
    "        \n",
    "torch.manual_seed(3407)\n",
    "np.random.seed(3407)\n",
    "torch.cuda.manual_seed_all(3407)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "# os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":4096:2\"\n",
    "\n",
    "train_data_all = torchvision.datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "test_data_all = torchvision.datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "# print(f'train data:{train_data}')\n",
    "# print(f'test data:{test_data}')\n",
    "\n",
    "def get_random_subset(train_data_all, test_data_all):    \n",
    "    # train_indices = torch.arange(20000)\n",
    "    test_indices = torch.arange(1000)\n",
    "    train_indices = torch.randint(60000-1, (20000,))\n",
    "    # print(f'train indices:{train_indices[:10]}')\n",
    "    train_data = data_utils.Subset(train_data_all, train_indices)\n",
    "    test_data = data_utils.Subset(test_data_all, test_indices)\n",
    "    # print(f'train data:{train_data}')\n",
    "    # print(f'test data:{test_data}')\n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(0)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g,\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_data,\n",
    "        batch_size=256,\n",
    "        num_workers=num_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g,\n",
    "    )\n",
    "    print(f'train data size:{len(train_loader.dataset)}')\n",
    "    print(f'test data size:{len(test_loader.dataset)}')\n",
    "    # X_mat, y_mat = torch.Tensor(len(train_loader.dataset),784), torch.Tensor(len(train_loader.dataset)).long()\n",
    "    # for i, (data, label) in enumerate(train_loader):\n",
    "    #     X_mat[i] = data.flatten()\n",
    "    #     y_mat[i] = label.flatten()\n",
    "    # print(f'X_mat shape:{X_mat.shape}, y_mat shape:{y_mat.shape}')\n",
    "    return train_loader, test_loader #, X_mat, y_mat\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_features, hidden_layers, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = len(hidden_layers) + 1\n",
    "        self.total_params_len = 0\n",
    "        self.fc1 = nn.Linear(input_features, hidden_layers[0])\n",
    "        self.total_params_len += input_features*hidden_layers[0] + hidden_layers[0]\n",
    "        self.fc2 = nn.Linear(hidden_layers[0], output_size)\n",
    "        self.total_params_len += hidden_layers[0]*output_size + output_size\n",
    "        \n",
    "        ### Others required params\n",
    "        self.param_list = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        # print('x shape in forward',x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def fit(self, X, Y, X_val, Y_val, epochs, batch_size=1, **kwargs):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.SGD(self.parameters())\n",
    "        \n",
    "\n",
    "class Train_nn:\n",
    "    \n",
    "    def __init__(self, input_features, hidden_layers, output_size, lr):\n",
    "        self.model = Net(input_features, hidden_layers=hidden_layers, output_size=output_size)\n",
    "        self.model.to(device)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda= lambda it: 1/(it+1))\n",
    "        \n",
    "    def get_loss(self, X, y, params=None):\n",
    "        # if params is not None:\n",
    "        assert False, \"Model not initialized with given params\"\n",
    "        op = self.model(X)\n",
    "        loss = self.loss_fn(pred, y)\n",
    "        return loss\n",
    "    \n",
    "    def get_gradient(self):\n",
    "        params = (self.model.parameters())\n",
    "        grad_norm_sq = torch.tensor(0, dtype=float).to(device)\n",
    "        # print('grad Norm init:', grad_norm_sq)\n",
    "        for param in self.model.parameters():\n",
    "            temp = param.grad.data.pow(2).sum()\n",
    "            # print(f'param grad norm \\n\\tsum:{temp.data}')#,\\n\\tshape:{param.shape}')\n",
    "            grad_norm_sq += temp\n",
    "            \n",
    "        return grad_norm_sq.sqrt().cpu()\n",
    "    \n",
    "    def try_operator_norm(self, hess_mat):\n",
    "        for i in len(hess_mat):\n",
    "            for j in len(hess_mat[0]):\n",
    "                torch.unsqueeze(hess_mat[i][i],0)\n",
    "        hess_tensor_dim = list(hess_mat[0][0].shape)\n",
    "        hess_tensor_dim += [n*2,n*2]\n",
    "        hess_mat_np = np.zeros(shape=hess_tensor_dim)\n",
    "        hess_tensor = torch.tensor(hess_mat_np)\n",
    "        torch.cat(hess_mat, out=hess_tensor)\n",
    "        \n",
    "        hess_mat.reshpe(n*2,n*2)\n",
    "        hess_norm = torch.linalg.norm(hess_mat, 2)\n",
    "        assert False, \"Not working\"\n",
    "    \n",
    "    def get_hessian(self, X, y):\n",
    "        prev_params = copy.deepcopy(list(self.model.parameters()))\n",
    "        n = self.model.layers\n",
    "        def local_model(*params):\n",
    "            # print(f'len of params:{len(params)}')\n",
    "            # print(f'shape of params[0]:{params[0].shape}')\n",
    "            # with torch.no_grad():\n",
    "            #initialize model with given params\n",
    "            i = 0\n",
    "            for i, param in enumerate(self.model.parameters()):\n",
    "                param.data = params[i]\n",
    "            pred = self.model(X)\n",
    "            loss = self.loss_fn(pred, y)\n",
    "            # print(f'loss type:{type(loss)}')\n",
    "            return loss\n",
    "        p =list(self.model.parameters())\n",
    "        hess_mat = hessian(local_model, tuple(p))\n",
    "        hess_norm = torch.tensor(0.).to(device)\n",
    "        for i in range(len(hess_mat)):\n",
    "            for j in range(len(hess_mat[0])):\n",
    "                hess_norm+= hess_mat[i][j].pow(2).sum()\n",
    "        \n",
    "        # print(f'Hess mat len:{len(hess_mat)}')\n",
    "        # print(f'Hess mat[0] len:{len(hess_mat[0])}')\n",
    "        # print(f'Hess mat[0][0] shape:{hess_mat[0][0].shape}')\n",
    "        \n",
    "        hess_norm = hess_norm.sqrt()\n",
    "        # print(f'hess norm:{hess_norm}')\n",
    "        \n",
    "        # Reinitialize the original params to model\n",
    "        for i, param in enumerate(self.model.parameters()):\n",
    "                param.data = prev_params[i]\n",
    "        \n",
    "        return hess_norm\n",
    "    \n",
    "    def get_hessianv2(self, X,y):\n",
    "        names = list(n for n, _ in self.model.named_parameters())\n",
    "        def loss_fun_hess(*params):\n",
    "            out: torch.Tensor = _stateless.functional_call(self.model, {n: p for n, p in zip(names, params)}, X)\n",
    "            local_loss = self.loss_fn(out, y)\n",
    "            return local_loss\n",
    "        hess_mat = hessian(loss_fun_hess, tuple(self.model.parameters()))\n",
    "        # print(f'len of hess mat:{len(hess_mat)}')\n",
    "        # print(f'hess_mat[0] shape:{len(hess_mat[0])}')\n",
    "        # print(f'hess_mat[0][0] shape:{hess_mat[0][0].shape}')\n",
    "        hess_norm = torch.tensor(0.).to(device)\n",
    "        for i in range(len(hess_mat)):\n",
    "            for j in range(len(hess_mat[0])):\n",
    "                hess_norm+= hess_mat[i][j].pow(2).sum()\n",
    "        hess_norm = hess_norm.sqrt()\n",
    "        # print(f'v2 hess norm{hess_norm}')\n",
    "        return hess_norm.cpu()\n",
    "        \n",
    "    def fit(self, train_loader, test_loader, epochs, store_grads=False, store_hessian=False, store_gen_err=False, store_weights=False, store_pt_loss=True, store_freq = 20, freq_reduce_by=None, freq_reduce_after=None):\n",
    "        \n",
    "        ## For Book keeping results ##\n",
    "        self.grads_norms = []\n",
    "        self.param_list = []\n",
    "        self.hess_norms = []\n",
    "        self.gen_err = []\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        self.point_loss = []\n",
    "        ## Initializing values ##\n",
    "        terminate_training = False\n",
    "        store_count = 0\n",
    "        \n",
    "        for epoch in tqdm(range(epochs), total=epochs, unit=\"epoch\", disable=True):\n",
    "            if terminate_training == True:\n",
    "                break\n",
    "            for batch, (X, y) in tqdm(enumerate(train_loader), total=len(train_loader), unit='batch'):\n",
    "                # if batch>300:\n",
    "                #     terminate_training = True\n",
    "                #     break\n",
    "                \n",
    "                X, y =X.to(device), y.to(device)\n",
    "                pred = self.model(X)\n",
    "                loss = self.loss_fn(pred, y)\n",
    "\n",
    "                # Backpropagation\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                ## Saving point loss\n",
    "                if store_pt_loss and (batch%store_freq==0):\n",
    "                    self.point_loss.append(loss.item())\n",
    "                    \n",
    "                ## Saving the weights\n",
    "                if store_weights and (batch%store_freq==0):\n",
    "                    current_params = tuple(self.model.parameters())\n",
    "                    self.param_list.append(current_params)\n",
    "                \n",
    "                ## computing and saving the gradient\n",
    "                if store_grads and (batch% store_freq == 0):\n",
    "                    # store_count += 1\n",
    "                    # # print(f'\\tstore_freq:{store_freq}, batch:{batch}')\n",
    "                    # if store_count%freq_reduce_after==0:\n",
    "                    #     store_freq += freq_reduce_by\n",
    "                    #     # print(f'store freq:{store_freq}, batch:{batch}')\n",
    "                    grad_norm_per_update = self.get_gradient()\n",
    "                    # print('\\tgrad norm:', grad_norm_per_update)\n",
    "                    self.grads_norms.append(grad_norm_per_update)\n",
    "                    \n",
    "                ## computing and saving hessian\n",
    "                if store_hessian and (batch% store_freq==0):\n",
    "                    #assert False, \"Not implemented\"\n",
    "                    self.optimizer.zero_grad()\n",
    "                    self.hess_norms.append(self.get_hessianv2(X,y))\n",
    "                    store_count += 1\n",
    "                    if store_count%freq_reduce_after==0:\n",
    "                        store_freq += freq_reduce_by\n",
    "                \n",
    "                ## computing and storing the generalization error\n",
    "                if store_gen_err and (batch% store_freq == 0):\n",
    "                    assert False, \"fix reducing freq to get it working\"\n",
    "                    train_loss, test_loss, point_loss=0, 0, 0\n",
    "                    with torch.no_grad():\n",
    "                        for sub_batch, (X_local,y_local) in enumerate(train_loader):\n",
    "                            # if sub_batch> batch: # only taking the encountered points to calculate train loss\n",
    "                            #     break\n",
    "                            X_local, y_local = X_local.to(device), y_local.to(device)\n",
    "                            pred_local = self.model(X_local)\n",
    "                            train_loss += self.loss_fn(pred_local, y_local).item()\n",
    "                    train_loss = train_loss/(batch+1)\n",
    "                    with torch.no_grad():\n",
    "                        for sub_batch, (X_local,y_local) in enumerate(test_loader):\n",
    "                            X_local, y_local = X_local.to(device), y_local.to(device)\n",
    "                            pred_local = self.model(X_local)\n",
    "                            test_loss += self.loss_fn(pred_local, y_local).item()\n",
    "                    test_batch_size = len(test_loader)\n",
    "                    # print(f\"Number of batches in test:{len(test_loader)}\")\n",
    "                    test_loss = test_loss/ len(test_loader)\n",
    "                    self.train_loss.append(train_loss)\n",
    "                    self.val_loss.append(test_loss)\n",
    "                \n",
    "                if batch % 10000 == 0:\n",
    "                    loss, current = loss.item(), batch * len(X)\n",
    "                    correct = 0\n",
    "                    test_loss = 0\n",
    "                    with torch.no_grad():\n",
    "                        for X, y in test_loader:\n",
    "                            X, y = X.to(device), y.to(device)\n",
    "                            pred = self.model(X)\n",
    "                            test_loss += self.loss_fn(pred, y).item()\n",
    "                            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "                    acc = 100*correct/len(test_loader.dataset)\n",
    "                    print(f\"\\taccuracy:{acc}\")#, at batch:{batch}\")\n",
    "                    print(f\"\\tloss: {loss:>7f}\")\n",
    "                \n",
    "                    # print(f'Learning rate:{self.scheduler.get_last_lr()}')\n",
    "                self.scheduler.step()\n",
    "            \n",
    "def exp_get_lp_sm(train_data_all, test_data_all, op_features, weight = 10, times = 8, epochs = 1, root_dir='', path=None, clear_file = True, freq_reduce_by=10, freq_reduce_after=100):\n",
    "    grad_list        = []\n",
    "    hess_norm_list   = []\n",
    "    if path is not None:\n",
    "        grad_file_path = root_dir+'grad_'+path\n",
    "        hess_file_path = root_dir+'hess_'+path\n",
    "        # gen_file_path = root_dir+'gen_'+path\n",
    "        if clear_file:\n",
    "            with open(grad_file_path, 'w+') as f:\n",
    "                f.write('')\n",
    "            with open(hess_file_path, 'w+') as f:\n",
    "                f.write('')\n",
    "            \n",
    "    for t in range(times):\n",
    "        train_loader, test_loader = get_random_subset(train_data_all, test_data_all)\n",
    "        print(f'Time:{t}')\n",
    "        train_model = Train_nn(784, [weight], op_features, lr= details['alpha_0'])\n",
    "        train_model.fit(train_loader, test_loader, epochs=epochs, store_grads=True, store_hessian=True, store_freq=details['book_keep_freq'],  store_gen_err=False, store_pt_loss=False, store_weights=False, freq_reduce_by = freq_reduce_by, freq_reduce_after=freq_reduce_after)\n",
    "        \n",
    "        with open(grad_file_path,'a+') as f:\n",
    "            f.write(' '.join([str(grad) for grad in train_model.grads_norms]) + '\\n')\n",
    "        with open(hess_file_path,'a+') as f:\n",
    "            f.write(' '.join([str(hess) for hess in train_model.hess_norms]) + '\\n')\n",
    "        \n",
    "        hess_norm_list.append(train_model.hess_norms)\n",
    "        grad_list.append(train_model.grads_norms)\n",
    "         \n",
    "    return grad_list, hess_norm_list\n",
    "\n",
    "grad_list, hess_norm_list = exp_get_lp_sm(train_data_all, test_data_all, op_features=10, \n",
    "              weight=details['g_weight'], times=details['g_times'], \n",
    "              epochs=details['g_epochs'], root_dir=details['result_root_dir'], \n",
    "              path=details['result_path'], freq_reduce_by=details['freq_reduce_by'], \n",
    "              freq_reduce_after=details['freq_reduce_after'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abd4422-8114-4987-9480-f49161c8c43e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## For ratio 1.5 , W =37 so approx time = 6.5*6 hrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c8bc33ae-b553-4b2e-a488-08a498840eba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected weight:37\n",
      "train data size:20000\n",
      "test data size:1000\n",
      "Time:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df5978aca8e745a6b9d64e342bd863d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\taccuracy:8.3\n",
      "\tloss: 2.136471\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [62]\u001b[0m, in \u001b[0;36m<cell line: 357>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    353\u001b[0m         grad_list\u001b[38;5;241m.\u001b[39mappend(train_model\u001b[38;5;241m.\u001b[39mgrads_norms)\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grad_list, hess_norm_list\n\u001b[0;32m--> 357\u001b[0m \u001b[43mexp_get_lp_sm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m              \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mg_weight\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mg_times\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m              \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mg_epochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresult_root_dir\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m              \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresult_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq_reduce_by\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfreq_reduce_by\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m              \u001b[49m\u001b[43mfreq_reduce_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfreq_reduce_after\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [62]\u001b[0m, in \u001b[0;36mexp_get_lp_sm\u001b[0;34m(train_loader, test_loader, op_features, weight, times, epochs, root_dir, path, clear_file, freq_reduce_by, freq_reduce_after)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    344\u001b[0m train_model \u001b[38;5;241m=\u001b[39m Train_nn(\u001b[38;5;241m784\u001b[39m, [weight], op_features, lr\u001b[38;5;241m=\u001b[39m details[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha_0\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 345\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_grads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_hessian\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbook_keep_freq\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mstore_gen_err\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_pt_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq_reduce_by\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfreq_reduce_by\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq_reduce_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreq_reduce_after\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(grad_file_path,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    348\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(grad) \u001b[38;5;28;01mfor\u001b[39;00m grad \u001b[38;5;129;01min\u001b[39;00m train_model\u001b[38;5;241m.\u001b[39mgrads_norms]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [62]\u001b[0m, in \u001b[0;36mTrain_nn.fit\u001b[0;34m(self, train_loader, test_loader, epochs, store_grads, store_hessian, store_gen_err, store_weights, store_pt_loss, store_freq, freq_reduce_by, freq_reduce_after)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m store_hessian \u001b[38;5;129;01mand\u001b[39;00m (batch\u001b[38;5;241m%\u001b[39m store_freq\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;66;03m#assert False, \"Not implemented\"\u001b[39;00m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhess_norms\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_hessianv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;66;03m#self.hess_norms.append(self.get_hessian(X,y))\u001b[39;00m\n\u001b[1;32m    281\u001b[0m     \n\u001b[1;32m    282\u001b[0m \n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m## computing and storing the generalization error\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m store_gen_err \u001b[38;5;129;01mand\u001b[39;00m (batch\u001b[38;5;241m%\u001b[39m store_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
      "Input \u001b[0;32mIn [62]\u001b[0m, in \u001b[0;36mTrain_nn.get_hessianv2\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    210\u001b[0m     local_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(out, y)\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m local_loss\n\u001b[0;32m--> 212\u001b[0m hess_mat \u001b[38;5;241m=\u001b[39m \u001b[43mhessian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_fun_hess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# print(f'len of hess mat:{len(hess_mat)}')\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# print(f'hess_mat[0] shape:{len(hess_mat[0])}')\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# print(f'hess_mat[0][0] shape:{hess_mat[0][0].shape}')\u001b[39;00m\n\u001b[1;32m    216\u001b[0m hess_norm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/functional.py:808\u001b[0m, in \u001b[0;36mhessian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, outer_jacobian_strategy)\u001b[0m\n\u001b[1;32m    805\u001b[0m     _check_requires_grad(jac, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacobian\u001b[39m\u001b[38;5;124m\"\u001b[39m, strict\u001b[38;5;241m=\u001b[39mstrict)\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jac\n\u001b[0;32m--> 808\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mjacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjac_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvectorize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m               \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mouter_jacobian_strategy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _tuple_postprocess(res, (is_inputs_tuple, is_inputs_tuple))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/functional.py:670\u001b[0m, in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[1;32m    668\u001b[0m jac_i: Tuple[List[torch\u001b[38;5;241m.\u001b[39mTensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m([] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs)))  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(out\u001b[38;5;241m.\u001b[39mnelement()):\n\u001b[0;32m--> 670\u001b[0m     vj \u001b[38;5;241m=\u001b[39m \u001b[43m_autograd_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m el_idx, (jac_i_el, vj_el, inp_el) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(jac_i, vj, inputs)):\n\u001b[1;32m    674\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m vj_el \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/functional.py:159\u001b[0m, in \u001b[0;36m_autograd_grad\u001b[0;34m(outputs, inputs, grad_outputs, create_graph, retain_graph, is_grads_batched)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m,) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_grad_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_grads_batched\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:276\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd.functional import hessian\n",
    "import torch.utils.data as data_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from tqdm.notebook import tqdm \n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import seaborn as sns\n",
    "import os\n",
    "import copy\n",
    "from torch.nn.utils import _stateless\n",
    "\n",
    "batch_size = 1\n",
    "num_workers = 1\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "# device\n",
    "\n",
    "details = {}\n",
    "details['use_db'] = 'mnist'\n",
    "details['result_root_dir']='results/t0/'\n",
    "details['result_path']='try1_t6_r1.5'\n",
    "details['ratio'] = 1.5\n",
    "details['book_keep_freq'] = 20\n",
    "details['g_times'] = 6\n",
    "details['g_epochs'] = 1\n",
    "details['alpha_0']= 0.001\n",
    "details['freq_reduce_by'] = 20\n",
    "details['freq_reduce_after'] = 100\n",
    "\n",
    "details['training_step_limit'] = 100000 ## this is to train for max updates per epochs\n",
    "details['stop_hess_computation'] = 20000 ## Stop computing hessian after calculated these many times\n",
    "\n",
    "details['g_weight'] = int(details['ratio']*20000/(784+10))\n",
    "print(f'selected weight:{details[\"g_weight\"]}')\n",
    "\n",
    "with open(details['result_root_dir']+'details_'+details['result_path']+'.txt', 'w+') as f:\n",
    "    for key, val in details.items():\n",
    "        content = key + ' : '+str(val) + '\\n'\n",
    "        f.write(content)\n",
    "        \n",
    "torch.manual_seed(3407)\n",
    "np.random.seed(3407)\n",
    "torch.cuda.manual_seed_all(3407)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "# os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":4096:2\"\n",
    "\n",
    "train_data = torchvision.datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "test_data = torchvision.datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "# print(f'train data:{train_data}')\n",
    "# print(f'test data:{test_data}')\n",
    "train_indices = torch.arange(20000)\n",
    "test_indices = torch.arange(1000)\n",
    "train_data = data_utils.Subset(train_data, train_indices)\n",
    "test_data = data_utils.Subset(test_data, test_indices)\n",
    "# print(f'train data:{train_data}')\n",
    "# print(f'test data:{test_data}')\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=g,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    batch_size=256,\n",
    "    num_workers=num_workers,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=g,\n",
    ")\n",
    "print(f'train data size:{len(train_loader.dataset)}')\n",
    "print(f'test data size:{len(test_loader.dataset)}')\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_features, hidden_layers, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = len(hidden_layers) + 1\n",
    "        self.total_params_len = 0\n",
    "        self.fc1 = nn.Linear(input_features, hidden_layers[0])\n",
    "        self.total_params_len += input_features*hidden_layers[0] + hidden_layers[0]\n",
    "        self.fc2 = nn.Linear(hidden_layers[0], output_size)\n",
    "        self.total_params_len += hidden_layers[0]*output_size + output_size\n",
    "        \n",
    "        ### Others required params\n",
    "        self.param_list = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        # print('x shape in forward',x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def fit(self, X, Y, X_val, Y_val, epochs, batch_size=1, **kwargs):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.SGD(self.parameters())\n",
    "        \n",
    "\n",
    "class Train_nn:\n",
    "    \n",
    "    def __init__(self, input_features, hidden_layers, output_size, lr):\n",
    "        self.model = Net(input_features, hidden_layers=hidden_layers, output_size=output_size)\n",
    "        self.model.to(device)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda= lambda it: 1/(it+1))\n",
    "        \n",
    "    def get_loss(self, X, y, params=None):\n",
    "        # if params is not None:\n",
    "        assert False, \"Model not initialized with given params\"\n",
    "        op = self.model(X)\n",
    "        loss = self.loss_fn(pred, y)\n",
    "        return loss\n",
    "    \n",
    "    def get_gradient(self):\n",
    "        params = (self.model.parameters())\n",
    "        grad_norm_sq = torch.tensor(0, dtype=float).to(device)\n",
    "        # print('grad Norm init:', grad_norm_sq)\n",
    "        for param in self.model.parameters():\n",
    "            temp = param.grad.data.pow(2).sum()\n",
    "            # print(f'param grad norm \\n\\tsum:{temp.data}')#,\\n\\tshape:{param.shape}')\n",
    "            grad_norm_sq += temp\n",
    "            \n",
    "        return grad_norm_sq.sqrt().cpu()\n",
    "    \n",
    "    def try_operator_norm(self, hess_mat):\n",
    "        for i in len(hess_mat):\n",
    "            for j in len(hess_mat[0]):\n",
    "                torch.unsqueeze(hess_mat[i][i],0)\n",
    "        hess_tensor_dim = list(hess_mat[0][0].shape)\n",
    "        hess_tensor_dim += [n*2,n*2]\n",
    "        hess_mat_np = np.zeros(shape=hess_tensor_dim)\n",
    "        hess_tensor = torch.tensor(hess_mat_np)\n",
    "        torch.cat(hess_mat, out=hess_tensor)\n",
    "        \n",
    "        hess_mat.reshpe(n*2,n*2)\n",
    "        hess_norm = torch.linalg.norm(hess_mat, 2)\n",
    "        assert False, \"Not working\"\n",
    "    \n",
    "    def get_hessian(self, X, y):\n",
    "        prev_params = copy.deepcopy(list(self.model.parameters()))\n",
    "        n = self.model.layers\n",
    "        def local_model(*params):\n",
    "            # print(f'len of params:{len(params)}')\n",
    "            # print(f'shape of params[0]:{params[0].shape}')\n",
    "            # with torch.no_grad():\n",
    "            #initialize model with given params\n",
    "            i = 0\n",
    "            for i, param in enumerate(self.model.parameters()):\n",
    "                param.data = params[i]\n",
    "            pred = self.model(X)\n",
    "            loss = self.loss_fn(pred, y)\n",
    "            # print(f'loss type:{type(loss)}')\n",
    "            return loss\n",
    "        p =list(self.model.parameters())\n",
    "        hess_mat = hessian(local_model, tuple(p))\n",
    "        hess_norm = torch.tensor(0.).to(device)\n",
    "        for i in range(len(hess_mat)):\n",
    "            for j in range(len(hess_mat[0])):\n",
    "                hess_norm+= hess_mat[i][j].pow(2).sum()\n",
    "        \n",
    "        # print(f'Hess mat len:{len(hess_mat)}')\n",
    "        # print(f'Hess mat[0] len:{len(hess_mat[0])}')\n",
    "        # print(f'Hess mat[0][0] shape:{hess_mat[0][0].shape}')\n",
    "        \n",
    "        hess_norm = hess_norm.sqrt()\n",
    "        # print(f'hess norm:{hess_norm}')\n",
    "        \n",
    "        # Reinitialize the original params to model\n",
    "        for i, param in enumerate(self.model.parameters()):\n",
    "                param.data = prev_params[i]\n",
    "        \n",
    "        return hess_norm\n",
    "    \n",
    "    def get_hessianv2(self, X,y):\n",
    "        names = list(n for n, _ in self.model.named_parameters())\n",
    "        def loss_fun_hess(*params):\n",
    "            out: torch.Tensor = _stateless.functional_call(self.model, {n: p for n, p in zip(names, params)}, X)\n",
    "            local_loss = self.loss_fn(out, y)\n",
    "            return local_loss\n",
    "        hess_mat = hessian(loss_fun_hess, tuple(self.model.parameters()))\n",
    "        # print(f'len of hess mat:{len(hess_mat)}')\n",
    "        # print(f'hess_mat[0] shape:{len(hess_mat[0])}')\n",
    "        # print(f'hess_mat[0][0] shape:{hess_mat[0][0].shape}')\n",
    "        hess_norm = torch.tensor(0.).to(device)\n",
    "        for i in range(len(hess_mat)):\n",
    "            for j in range(len(hess_mat[0])):\n",
    "                hess_norm+= hess_mat[i][j].pow(2).sum()\n",
    "        hess_norm = hess_norm.sqrt()\n",
    "        # print(f'v2 hess norm{hess_norm}')\n",
    "        return hess_norm.cpu()\n",
    "        \n",
    "    def fit(self, train_loader, test_loader, epochs, store_grads=False, store_hessian=False, store_gen_err=False, store_weights=False, store_pt_loss=True, store_freq = 10, freq_reduce_by=None, freq_reduce_after=None):\n",
    "        \n",
    "        ## For Book keeping results ##\n",
    "        self.grads_norms = []\n",
    "        self.param_list = []\n",
    "        self.hess_norms = []\n",
    "        self.gen_err = []\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        self.point_loss = []\n",
    "        ## Initializing values ##\n",
    "        terminate_training = False\n",
    "        store_count = 0\n",
    "        \n",
    "        for epoch in tqdm(range(epochs), total=epochs, unit=\"epoch\", disable=True):\n",
    "            if terminate_training == True:\n",
    "                break\n",
    "            for batch, (X, y) in tqdm(enumerate(train_loader), total=len(train_loader), unit='batch'):\n",
    "                # if batch>300:\n",
    "                #     terminate_training = True\n",
    "                #     break\n",
    "                \n",
    "                X, y =X.to(device), y.to(device)\n",
    "                pred = self.model(X)\n",
    "                loss = self.loss_fn(pred, y)\n",
    "\n",
    "                # Backpropagation\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                ## Saving point loss\n",
    "                if store_pt_loss and (batch%store_freq==0):\n",
    "                    self.point_loss.append(loss.item())\n",
    "                    \n",
    "                ## Saving the weights\n",
    "                if store_weights and (batch%store_freq==0):\n",
    "                    current_params = tuple(self.model.parameters())\n",
    "                    self.param_list.append(current_params)\n",
    "                \n",
    "                ## computing and saving the gradient\n",
    "                if store_grads and (batch% store_freq == 0):\n",
    "                    store_count += 1\n",
    "                    # print(f'\\tstore_freq:{store_freq}, batch:{batch}')\n",
    "                    if store_count%freq_reduce_after==0:\n",
    "                        store_freq += freq_reduce_by\n",
    "                        # print(f'store freq:{store_freq}, batch:{batch}')\n",
    "                    grad_norm_per_update = self.get_gradient()\n",
    "                    # print('\\tgrad norm:', grad_norm_per_update)\n",
    "                    self.grads_norms.append(grad_norm_per_update)\n",
    "                    \n",
    "                ## computing and saving hessian\n",
    "                if store_hessian and (batch% store_freq==0):\n",
    "                    #assert False, \"Not implemented\"\n",
    "                    self.optimizer.zero_grad()\n",
    "                    self.hess_norms.append(self.get_hessianv2(X,y))\n",
    "                    #self.hess_norms.append(self.get_hessian(X,y))\n",
    "                    \n",
    "                \n",
    "                ## computing and storing the generalization error\n",
    "                if store_gen_err and (batch% store_freq == 0):\n",
    "                    train_loss, test_loss, point_loss=0, 0, 0\n",
    "                    with torch.no_grad():\n",
    "                        for sub_batch, (X_local,y_local) in enumerate(train_loader):\n",
    "                            # if sub_batch> batch: # only taking the encountered points to calculate train loss\n",
    "                            #     break\n",
    "                            X_local, y_local = X_local.to(device), y_local.to(device)\n",
    "                            pred_local = self.model(X_local)\n",
    "                            train_loss += self.loss_fn(pred_local, y_local).item()\n",
    "                    train_loss = train_loss/(batch+1)\n",
    "                    with torch.no_grad():\n",
    "                        for sub_batch, (X_local,y_local) in enumerate(test_loader):\n",
    "                            X_local, y_local = X_local.to(device), y_local.to(device)\n",
    "                            pred_local = self.model(X_local)\n",
    "                            test_loss += self.loss_fn(pred_local, y_local).item()\n",
    "                    test_batch_size = len(test_loader)\n",
    "                    # print(f\"Number of batches in test:{len(test_loader)}\")\n",
    "                    test_loss = test_loss/ len(test_loader)\n",
    "                    self.train_loss.append(train_loss)\n",
    "                    self.val_loss.append(test_loss)\n",
    "                \n",
    "                if batch % 10000 == 0:\n",
    "                    loss, current = loss.item(), batch * len(X)\n",
    "                    correct = 0\n",
    "                    test_loss = 0\n",
    "                    with torch.no_grad():\n",
    "                        for X, y in test_loader:\n",
    "                            X, y = X.to(device), y.to(device)\n",
    "                            pred = self.model(X)\n",
    "                            test_loss += self.loss_fn(pred, y).item()\n",
    "                            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "                    acc = 100*correct/len(test_loader.dataset)\n",
    "                    print(f\"\\taccuracy:{acc}\")#, at batch:{batch}\")\n",
    "                    print(f\"\\tloss: {loss:>7f}\")\n",
    "                \n",
    "                    # print(f'Learning rate:{self.scheduler.get_last_lr()}')\n",
    "                self.scheduler.step()\n",
    "            \n",
    "def exp_get_lp_sm(train_loader, test_loader, op_features, weight = 10, times = 8, epochs = 1, root_dir='', path=None, clear_file = True, freq_reduce_by=10, freq_reduce_after=100):\n",
    "    grad_list        = []\n",
    "    hess_norm_list   = []\n",
    "    # order_of_permute = []\n",
    "    # input_features = X.shape[1]\n",
    "    # for epoch in range(epochs):\n",
    "    #     ind = np.arange(X.shape[0])\n",
    "    #     np.random.shuffle(ind)\n",
    "    #     #print('indexes:', ind)\n",
    "    #     order_of_permute.append(ind)\n",
    "    if path is not None:\n",
    "        grad_file_path = root_dir+'grad_'+path\n",
    "        hess_file_path = root_dir+'hess_'+path\n",
    "        # gen_file_path = root_dir+'gen_'+path\n",
    "        if clear_file:\n",
    "            with open(grad_file_path, 'w+') as f:\n",
    "                f.write('')\n",
    "            with open(hess_file_path, 'w+') as f:\n",
    "                f.write('')\n",
    "            \n",
    "    for t in range(times):\n",
    "        print(f'Time:{t}')\n",
    "        train_model = Train_nn(784, [weight], op_features, lr= details['alpha_0'])\n",
    "        train_model.fit(train_loader, test_loader, epochs=epochs, store_grads=True, store_hessian=True, store_freq=details['book_keep_freq'],  store_gen_err=False, store_pt_loss=False, store_weights=False, freq_reduce_by = freq_reduce_by, freq_reduce_after=freq_reduce_after)\n",
    "        \n",
    "        with open(grad_file_path,'a+') as f:\n",
    "            f.write(' '.join([str(grad) for grad in train_model.grads_norms]) + '\\n')\n",
    "        with open(hess_file_path,'a+') as f:\n",
    "            f.write(' '.join([str(hess) for hess in train_model.hess_norms]) + '\\n')\n",
    "        \n",
    "        hess_norm_list.append(train_model.hess_norms)\n",
    "        grad_list.append(train_model.grads_norms)\n",
    "         \n",
    "    return grad_list, hess_norm_list\n",
    "\n",
    "exp_get_lp_sm(train_loader, test_loader, op_features=10, \n",
    "              weight=details['g_weight'], times=details['g_times'], \n",
    "              epochs=details['g_epochs'], root_dir=details['result_root_dir'], \n",
    "              path=details['result_path'], freq_reduce_by=details['freq_reduce_by'], \n",
    "              freq_reduce_after=details['freq_reduce_after'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cad64d-688d-4346-9dd6-4f705dbc5fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
