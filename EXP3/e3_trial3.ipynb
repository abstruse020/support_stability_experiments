{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb277049-b700-4107-8ee6-089a11ef000f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Over parameterized network Non decreasing lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8fe1ba3-4d2f-4131-a53e-40ca1bd0c78f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected weight:[25]\n",
      "train data size:2000\n",
      "test data size:256\n",
      "Time:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6efbfcb0549a499dac4c574754bbb02f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(4.5351, dtype=torch.float64)\n",
      "hess: tensor(58.4720)\n",
      "\taccuracy:5.859375\n",
      "\tloss: 2.299834\n",
      "grad: tensor(3.5895, dtype=torch.float64)\n",
      "hess: tensor(39.1836)\n",
      "grad: tensor(2.2147, dtype=torch.float64)\n",
      "hess: tensor(26.5113)\n",
      "grad: tensor(3.9376, dtype=torch.float64)\n",
      "hess: tensor(50.1822)\n",
      "grad: tensor(2.2974, dtype=torch.float64)\n",
      "hess: tensor(26.8003)\n",
      "grad: tensor(2.9156, dtype=torch.float64)\n",
      "hess: tensor(33.6059)\n",
      "grad: tensor(5.0878, dtype=torch.float64)\n",
      "hess: tensor(49.7428)\n",
      "grad: tensor(3.5337, dtype=torch.float64)\n",
      "hess: tensor(44.7176)\n",
      "grad: tensor(3.6666, dtype=torch.float64)\n",
      "hess: tensor(45.3936)\n",
      "grad: tensor(2.7725, dtype=torch.float64)\n",
      "hess: tensor(30.5925)\n",
      "grad: tensor(4.3674, dtype=torch.float64)\n",
      "hess: tensor(45.4752)\n",
      "grad: tensor(2.2625, dtype=torch.float64)\n",
      "hess: tensor(44.0066)\n",
      "grad: tensor(3.6647, dtype=torch.float64)\n",
      "hess: tensor(35.5010)\n",
      "grad: tensor(3.1759, dtype=torch.float64)\n",
      "hess: tensor(45.3310)\n",
      "grad: tensor(3.1716, dtype=torch.float64)\n",
      "hess: tensor(33.1775)\n",
      "grad: tensor(4.8570, dtype=torch.float64)\n",
      "hess: tensor(56.0333)\n",
      "grad: tensor(4.9467, dtype=torch.float64)\n",
      "hess: tensor(57.8018)\n",
      "grad: tensor(5.0399, dtype=torch.float64)\n",
      "hess: tensor(56.5528)\n",
      "grad: tensor(4.0378, dtype=torch.float64)\n",
      "hess: tensor(46.2501)\n",
      "grad: tensor(4.6844, dtype=torch.float64)\n",
      "hess: tensor(56.3163)\n",
      "grad: tensor(3.9652, dtype=torch.float64)\n",
      "hess: tensor(50.4929)\n",
      "grad: tensor(3.1648, dtype=torch.float64)\n",
      "hess: tensor(36.7852)\n",
      "grad: tensor(5.1084, dtype=torch.float64)\n",
      "hess: tensor(58.0015)\n",
      "grad: tensor(3.4524, dtype=torch.float64)\n",
      "hess: tensor(36.0247)\n",
      "grad: tensor(5.1051, dtype=torch.float64)\n",
      "hess: tensor(52.9772)\n",
      "grad: tensor(6.1598, dtype=torch.float64)\n",
      "hess: tensor(64.3546)\n",
      "grad: tensor(4.0186, dtype=torch.float64)\n",
      "hess: tensor(38.4855)\n",
      "grad: tensor(5.4234, dtype=torch.float64)\n",
      "hess: tensor(55.1801)\n",
      "grad: tensor(5.5463, dtype=torch.float64)\n",
      "hess: tensor(56.4653)\n",
      "grad: tensor(4.3600, dtype=torch.float64)\n",
      "hess: tensor(44.7293)\n",
      "grad: tensor(6.0048, dtype=torch.float64)\n",
      "hess: tensor(60.8682)\n",
      "grad: tensor(3.7457, dtype=torch.float64)\n",
      "hess: tensor(43.8938)\n",
      "grad: tensor(5.6667, dtype=torch.float64)\n",
      "hess: tensor(61.0803)\n",
      "grad: tensor(4.2639, dtype=torch.float64)\n",
      "hess: tensor(49.2840)\n",
      "grad: tensor(3.9520, dtype=torch.float64)\n",
      "hess: tensor(34.7003)\n",
      "grad: tensor(5.3011, dtype=torch.float64)\n",
      "hess: tensor(51.7623)\n",
      "grad: tensor(4.1385, dtype=torch.float64)\n",
      "hess: tensor(36.9484)\n",
      "grad: tensor(4.2525, dtype=torch.float64)\n",
      "hess: tensor(47.8437)\n",
      "grad: tensor(5.3473, dtype=torch.float64)\n",
      "hess: tensor(47.5007)\n",
      "grad: tensor(3.7694, dtype=torch.float64)\n",
      "hess: tensor(38.1034)\n",
      "grad: tensor(2.9397, dtype=torch.float64)\n",
      "hess: tensor(29.2781)\n",
      "grad: tensor(5.1610, dtype=torch.float64)\n",
      "hess: tensor(52.2872)\n",
      "grad: tensor(4.5182, dtype=torch.float64)\n",
      "hess: tensor(47.1436)\n",
      "grad: tensor(5.7072, dtype=torch.float64)\n",
      "hess: tensor(47.8656)\n",
      "grad: tensor(5.7161, dtype=torch.float64)\n",
      "hess: tensor(59.1077)\n",
      "grad: tensor(4.0412, dtype=torch.float64)\n",
      "hess: tensor(39.7667)\n",
      "grad: tensor(3.9967, dtype=torch.float64)\n",
      "hess: tensor(36.0439)\n",
      "grad: tensor(5.5356, dtype=torch.float64)\n",
      "hess: tensor(50.8321)\n",
      "grad: tensor(5.3536, dtype=torch.float64)\n",
      "hess: tensor(44.3781)\n",
      "grad: tensor(5.3427, dtype=torch.float64)\n",
      "hess: tensor(49.4518)\n",
      "grad: tensor(4.4555, dtype=torch.float64)\n",
      "hess: tensor(37.6500)\n",
      "\taccuracy:51.953125\n",
      "\tloss: 1.591884\n",
      "grad: tensor(8.2292, dtype=torch.float64)\n",
      "hess: tensor(66.7275)\n",
      "grad: tensor(5.0788, dtype=torch.float64)\n",
      "hess: tensor(43.4298)\n",
      "grad: tensor(5.8687, dtype=torch.float64)\n",
      "hess: tensor(48.3151)\n",
      "grad: tensor(3.2213, dtype=torch.float64)\n",
      "hess: tensor(31.9682)\n",
      "grad: tensor(4.0263, dtype=torch.float64)\n",
      "hess: tensor(38.8580)\n",
      "grad: tensor(5.3915, dtype=torch.float64)\n",
      "hess: tensor(45.3173)\n",
      "grad: tensor(5.1139, dtype=torch.float64)\n",
      "hess: tensor(48.5479)\n",
      "grad: tensor(5.0439, dtype=torch.float64)\n",
      "hess: tensor(39.7279)\n",
      "grad: tensor(6.0991, dtype=torch.float64)\n",
      "hess: tensor(53.8938)\n",
      "grad: tensor(6.0745, dtype=torch.float64)\n",
      "hess: tensor(49.9786)\n",
      "grad: tensor(5.3986, dtype=torch.float64)\n",
      "hess: tensor(40.1549)\n",
      "grad: tensor(8.9449, dtype=torch.float64)\n",
      "hess: tensor(81.8818)\n",
      "grad: tensor(7.3779, dtype=torch.float64)\n",
      "hess: tensor(64.2045)\n",
      "grad: tensor(6.5478, dtype=torch.float64)\n",
      "hess: tensor(50.7677)\n",
      "grad: tensor(4.8531, dtype=torch.float64)\n",
      "hess: tensor(42.0552)\n",
      "grad: tensor(5.7866, dtype=torch.float64)\n",
      "hess: tensor(54.1849)\n",
      "grad: tensor(5.6263, dtype=torch.float64)\n",
      "hess: tensor(47.6062)\n",
      "grad: tensor(7.0133, dtype=torch.float64)\n",
      "hess: tensor(56.0380)\n",
      "grad: tensor(5.7019, dtype=torch.float64)\n",
      "hess: tensor(47.2814)\n",
      "grad: tensor(5.5310, dtype=torch.float64)\n",
      "hess: tensor(55.2885)\n",
      "grad: tensor(3.9412, dtype=torch.float64)\n",
      "hess: tensor(25.7461)\n",
      "grad: tensor(6.4616, dtype=torch.float64)\n",
      "hess: tensor(53.9897)\n",
      "grad: tensor(5.1826, dtype=torch.float64)\n",
      "hess: tensor(40.7744)\n",
      "grad: tensor(4.0569, dtype=torch.float64)\n",
      "hess: tensor(31.1820)\n",
      "grad: tensor(3.8481, dtype=torch.float64)\n",
      "hess: tensor(27.9155)\n",
      "grad: tensor(6.0742, dtype=torch.float64)\n",
      "hess: tensor(41.9299)\n",
      "grad: tensor(8.6926, dtype=torch.float64)\n",
      "hess: tensor(73.8821)\n",
      "grad: tensor(10.3343, dtype=torch.float64)\n",
      "hess: tensor(87.1541)\n",
      "grad: tensor(4.4803, dtype=torch.float64)\n",
      "hess: tensor(36.3356)\n",
      "grad: tensor(4.6057, dtype=torch.float64)\n",
      "hess: tensor(33.9280)\n",
      "grad: tensor(6.3544, dtype=torch.float64)\n",
      "hess: tensor(50.4457)\n",
      "grad: tensor(4.2573, dtype=torch.float64)\n",
      "hess: tensor(28.3907)\n",
      "grad: tensor(5.6662, dtype=torch.float64)\n",
      "hess: tensor(43.0883)\n",
      "grad: tensor(6.5472, dtype=torch.float64)\n",
      "hess: tensor(54.6978)\n",
      "grad: tensor(8.5008, dtype=torch.float64)\n",
      "hess: tensor(74.7182)\n",
      "grad: tensor(4.8714, dtype=torch.float64)\n",
      "hess: tensor(37.2842)\n",
      "grad: tensor(5.7162, dtype=torch.float64)\n",
      "hess: tensor(49.0728)\n",
      "grad: tensor(5.8092, dtype=torch.float64)\n",
      "hess: tensor(49.2678)\n",
      "grad: tensor(5.2452, dtype=torch.float64)\n",
      "hess: tensor(38.9926)\n",
      "grad: tensor(2.2522, dtype=torch.float64)\n",
      "hess: tensor(23.0808)\n",
      "grad: tensor(0.8110, dtype=torch.float64)\n",
      "hess: tensor(11.4347)\n",
      "grad: tensor(11.8269, dtype=torch.float64)\n",
      "hess: tensor(100.6751)\n",
      "grad: tensor(1.8427, dtype=torch.float64)\n",
      "hess: tensor(27.2666)\n",
      "grad: tensor(4.5401, dtype=torch.float64)\n",
      "hess: tensor(37.5580)\n",
      "grad: tensor(6.7541, dtype=torch.float64)\n",
      "hess: tensor(51.8741)\n",
      "grad: tensor(6.2606, dtype=torch.float64)\n",
      "hess: tensor(48.4304)\n",
      "grad: tensor(6.4610, dtype=torch.float64)\n",
      "hess: tensor(54.7978)\n",
      "grad: tensor(7.0644, dtype=torch.float64)\n",
      "hess: tensor(61.3066)\n",
      "grad: tensor(4.0628, dtype=torch.float64)\n",
      "hess: tensor(31.1785)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f44d3c1c586b4d6f8f5bc1672067f0df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(0.6732, dtype=torch.float64)\n",
      "hess: tensor(12.1683)\n",
      "\taccuracy:80.859375\n",
      "\tloss: 0.038415\n",
      "grad: tensor(2.7152, dtype=torch.float64)\n",
      "hess: tensor(16.9440)\n",
      "grad: tensor(3.7142, dtype=torch.float64)\n",
      "hess: tensor(25.6924)\n",
      "grad: tensor(4.2961, dtype=torch.float64)\n",
      "hess: tensor(53.0776)\n",
      "grad: tensor(10.2782, dtype=torch.float64)\n",
      "hess: tensor(77.2385)\n",
      "grad: tensor(9.0738, dtype=torch.float64)\n",
      "hess: tensor(63.0244)\n",
      "grad: tensor(5.6897, dtype=torch.float64)\n",
      "hess: tensor(43.7116)\n",
      "grad: tensor(6.2583, dtype=torch.float64)\n",
      "hess: tensor(37.6650)\n",
      "grad: tensor(5.0936, dtype=torch.float64)\n",
      "hess: tensor(52.7293)\n",
      "grad: tensor(6.3582, dtype=torch.float64)\n",
      "hess: tensor(48.2746)\n",
      "grad: tensor(0.9966, dtype=torch.float64)\n",
      "hess: tensor(14.3808)\n",
      "grad: tensor(11.2264, dtype=torch.float64)\n",
      "hess: tensor(73.9123)\n",
      "grad: tensor(0.9698, dtype=torch.float64)\n",
      "hess: tensor(15.8960)\n",
      "grad: tensor(5.6511, dtype=torch.float64)\n",
      "hess: tensor(40.1757)\n",
      "grad: tensor(2.9539, dtype=torch.float64)\n",
      "hess: tensor(32.3606)\n",
      "grad: tensor(2.3862, dtype=torch.float64)\n",
      "hess: tensor(30.4167)\n",
      "grad: tensor(2.8034, dtype=torch.float64)\n",
      "hess: tensor(37.1497)\n",
      "grad: tensor(5.1704, dtype=torch.float64)\n",
      "hess: tensor(40.5195)\n",
      "grad: tensor(1.5207, dtype=torch.float64)\n",
      "hess: tensor(15.5488)\n",
      "grad: tensor(6.6310, dtype=torch.float64)\n",
      "hess: tensor(54.0683)\n",
      "grad: tensor(1.8298, dtype=torch.float64)\n",
      "hess: tensor(12.6695)\n",
      "grad: tensor(1.1042, dtype=torch.float64)\n",
      "hess: tensor(14.1844)\n",
      "grad: tensor(1.7352, dtype=torch.float64)\n",
      "hess: tensor(24.1511)\n",
      "grad: tensor(5.1183, dtype=torch.float64)\n",
      "hess: tensor(40.3681)\n",
      "grad: tensor(6.5458, dtype=torch.float64)\n",
      "hess: tensor(55.9675)\n",
      "grad: tensor(1.8418, dtype=torch.float64)\n",
      "hess: tensor(17.5710)\n",
      "\taccuracy:76.171875\n",
      "\tloss: 0.198741\n",
      "grad: tensor(1.5601, dtype=torch.float64)\n",
      "hess: tensor(17.3768)\n",
      "grad: tensor(0.4523, dtype=torch.float64)\n",
      "hess: tensor(8.9686)\n",
      "grad: tensor(1.1411, dtype=torch.float64)\n",
      "hess: tensor(14.4671)\n",
      "grad: tensor(2.9017, dtype=torch.float64)\n",
      "hess: tensor(28.7358)\n",
      "grad: tensor(3.5156, dtype=torch.float64)\n",
      "hess: tensor(43.7061)\n",
      "grad: tensor(13.3566, dtype=torch.float64)\n",
      "hess: tensor(141.0961)\n",
      "grad: tensor(3.0483, dtype=torch.float64)\n",
      "hess: tensor(40.2324)\n",
      "grad: tensor(1.7781, dtype=torch.float64)\n",
      "hess: tensor(32.3335)\n",
      "grad: tensor(4.2170, dtype=torch.float64)\n",
      "hess: tensor(56.3942)\n",
      "grad: tensor(5.5172, dtype=torch.float64)\n",
      "hess: tensor(49.5862)\n",
      "grad: tensor(5.2276, dtype=torch.float64)\n",
      "hess: tensor(52.6354)\n",
      "grad: tensor(2.0413, dtype=torch.float64)\n",
      "hess: tensor(23.0597)\n",
      "grad: tensor(8.4597, dtype=torch.float64)\n",
      "hess: tensor(49.1130)\n",
      "grad: tensor(13.8598, dtype=torch.float64)\n",
      "hess: tensor(119.8293)\n",
      "grad: tensor(4.0212, dtype=torch.float64)\n",
      "hess: tensor(36.2995)\n",
      "grad: tensor(2.6371, dtype=torch.float64)\n",
      "hess: tensor(20.6784)\n",
      "grad: tensor(5.9623, dtype=torch.float64)\n",
      "hess: tensor(62.0820)\n",
      "grad: tensor(3.8744, dtype=torch.float64)\n",
      "hess: tensor(37.1777)\n",
      "grad: tensor(4.0680, dtype=torch.float64)\n",
      "hess: tensor(43.0606)\n",
      "grad: tensor(0.3998, dtype=torch.float64)\n",
      "hess: tensor(6.5080)\n",
      "grad: tensor(7.4677, dtype=torch.float64)\n",
      "hess: tensor(100.5193)\n",
      "grad: tensor(1.4418, dtype=torch.float64)\n",
      "hess: tensor(19.6652)\n",
      "grad: tensor(6.6720, dtype=torch.float64)\n",
      "hess: tensor(51.1854)\n",
      "grad: tensor(8.3729, dtype=torch.float64)\n",
      "hess: tensor(76.9746)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48027057ae3542ae8365e1b205050ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(0.0593, dtype=torch.float64)\n",
      "hess: tensor(1.4905)\n",
      "\taccuracy:87.109375\n",
      "\tloss: 0.002547\n",
      "grad: tensor(1.1570, dtype=torch.float64)\n",
      "hess: tensor(9.7434)\n",
      "grad: tensor(1.8118, dtype=torch.float64)\n",
      "hess: tensor(17.3874)\n",
      "grad: tensor(3.2804, dtype=torch.float64)\n",
      "hess: tensor(56.6832)\n",
      "grad: tensor(11.3960, dtype=torch.float64)\n",
      "hess: tensor(101.9069)\n",
      "grad: tensor(13.9606, dtype=torch.float64)\n",
      "hess: tensor(94.2925)\n",
      "grad: tensor(5.4576, dtype=torch.float64)\n",
      "hess: tensor(49.4632)\n",
      "grad: tensor(8.0192, dtype=torch.float64)\n",
      "hess: tensor(48.9318)\n",
      "grad: tensor(1.9218, dtype=torch.float64)\n",
      "hess: tensor(30.1544)\n",
      "grad: tensor(7.4399, dtype=torch.float64)\n",
      "hess: tensor(63.2250)\n",
      "grad: tensor(0.1346, dtype=torch.float64)\n",
      "hess: tensor(2.6641)\n",
      "grad: tensor(15.8370, dtype=torch.float64)\n",
      "hess: tensor(103.1075)\n",
      "grad: tensor(0.1872, dtype=torch.float64)\n",
      "hess: tensor(4.1127)\n",
      "grad: tensor(3.8975, dtype=torch.float64)\n",
      "hess: tensor(36.9925)\n",
      "grad: tensor(0.9333, dtype=torch.float64)\n",
      "hess: tensor(14.3537)\n",
      "grad: tensor(0.4708, dtype=torch.float64)\n",
      "hess: tensor(8.4158)\n",
      "grad: tensor(0.4806, dtype=torch.float64)\n",
      "hess: tensor(9.2773)\n",
      "grad: tensor(5.5443, dtype=torch.float64)\n",
      "hess: tensor(46.6875)\n",
      "grad: tensor(0.5060, dtype=torch.float64)\n",
      "hess: tensor(6.8171)\n",
      "grad: tensor(4.9260, dtype=torch.float64)\n",
      "hess: tensor(57.1836)\n",
      "grad: tensor(0.8986, dtype=torch.float64)\n",
      "hess: tensor(7.9334)\n",
      "grad: tensor(0.3557, dtype=torch.float64)\n",
      "hess: tensor(5.9588)\n",
      "grad: tensor(0.3695, dtype=torch.float64)\n",
      "hess: tensor(7.0970)\n",
      "grad: tensor(5.0969, dtype=torch.float64)\n",
      "hess: tensor(46.1631)\n",
      "grad: tensor(5.5107, dtype=torch.float64)\n",
      "hess: tensor(60.2157)\n",
      "grad: tensor(0.7828, dtype=torch.float64)\n",
      "hess: tensor(9.6754)\n",
      "\taccuracy:81.25\n",
      "\tloss: 0.064675\n",
      "grad: tensor(0.5303, dtype=torch.float64)\n",
      "hess: tensor(7.3885)\n",
      "grad: tensor(0.1669, dtype=torch.float64)\n",
      "hess: tensor(3.9785)\n",
      "grad: tensor(0.3055, dtype=torch.float64)\n",
      "hess: tensor(4.7368)\n",
      "grad: tensor(1.8776, dtype=torch.float64)\n",
      "hess: tensor(22.9330)\n",
      "grad: tensor(2.2966, dtype=torch.float64)\n",
      "hess: tensor(37.2396)\n",
      "grad: tensor(9.7195, dtype=torch.float64)\n",
      "hess: tensor(126.9585)\n",
      "grad: tensor(1.6346, dtype=torch.float64)\n",
      "hess: tensor(28.3617)\n",
      "grad: tensor(0.6643, dtype=torch.float64)\n",
      "hess: tensor(15.7623)\n",
      "grad: tensor(3.0138, dtype=torch.float64)\n",
      "hess: tensor(49.9182)\n",
      "grad: tensor(4.7588, dtype=torch.float64)\n",
      "hess: tensor(52.2421)\n",
      "grad: tensor(3.6112, dtype=torch.float64)\n",
      "hess: tensor(46.0706)\n",
      "grad: tensor(1.5184, dtype=torch.float64)\n",
      "hess: tensor(20.6837)\n",
      "grad: tensor(9.1037, dtype=torch.float64)\n",
      "hess: tensor(53.1886)\n",
      "grad: tensor(12.0640, dtype=torch.float64)\n",
      "hess: tensor(124.8238)\n",
      "grad: tensor(3.4814, dtype=torch.float64)\n",
      "hess: tensor(37.8125)\n",
      "grad: tensor(1.5459, dtype=torch.float64)\n",
      "hess: tensor(14.5146)\n",
      "grad: tensor(5.3734, dtype=torch.float64)\n",
      "hess: tensor(65.0783)\n",
      "grad: tensor(3.4590, dtype=torch.float64)\n",
      "hess: tensor(38.9393)\n",
      "grad: tensor(2.8438, dtype=torch.float64)\n",
      "hess: tensor(37.3204)\n",
      "grad: tensor(0.1739, dtype=torch.float64)\n",
      "hess: tensor(3.2784)\n",
      "grad: tensor(2.8694, dtype=torch.float64)\n",
      "hess: tensor(59.9038)\n",
      "grad: tensor(0.6559, dtype=torch.float64)\n",
      "hess: tensor(10.8444)\n",
      "grad: tensor(6.4578, dtype=torch.float64)\n",
      "hess: tensor(55.2285)\n",
      "grad: tensor(6.5938, dtype=torch.float64)\n",
      "hess: tensor(76.5551)\n",
      "grad: tensor(0.8281, dtype=torch.float64)\n",
      "hess: tensor(11.8819)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c8412d98b84992aa2994391998b33f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(0.0133, dtype=torch.float64)\n",
      "hess: tensor(0.3750)\n",
      "\taccuracy:90.234375\n",
      "\tloss: 0.000504\n",
      "grad: tensor(1.5034, dtype=torch.float64)\n",
      "hess: tensor(25.9389)\n",
      "grad: tensor(3.2425, dtype=torch.float64)\n",
      "hess: tensor(61.9475)\n",
      "grad: tensor(11.5457, dtype=torch.float64)\n",
      "hess: tensor(59.0236)\n",
      "grad: tensor(4.9679, dtype=torch.float64)\n",
      "hess: tensor(51.4653)\n",
      "grad: tensor(0.7729, dtype=torch.float64)\n",
      "hess: tensor(17.6038)\n",
      "grad: tensor(8.7204, dtype=torch.float64)\n",
      "hess: tensor(76.5834)\n",
      "grad: tensor(1.0467, dtype=torch.float64)\n",
      "hess: tensor(11.8190)\n",
      "grad: tensor(0.0759, dtype=torch.float64)\n",
      "hess: tensor(1.8382)\n",
      "grad: tensor(0.0308, dtype=torch.float64)\n",
      "hess: tensor(0.7116)\n",
      "grad: tensor(0.1770, dtype=torch.float64)\n",
      "hess: tensor(3.4407)\n",
      "grad: tensor(2.5265, dtype=torch.float64)\n",
      "hess: tensor(34.4412)\n",
      "grad: tensor(0.2842, dtype=torch.float64)\n",
      "hess: tensor(4.3476)\n",
      "grad: tensor(3.6161, dtype=torch.float64)\n",
      "hess: tensor(44.7855)\n",
      "grad: tensor(0.1913, dtype=torch.float64)\n",
      "hess: tensor(3.5110)\n",
      "grad: tensor(2.3468, dtype=torch.float64)\n",
      "hess: tensor(23.7553)\n",
      "grad: tensor(4.2519, dtype=torch.float64)\n",
      "hess: tensor(54.3397)\n",
      "\taccuracy:82.8125\n",
      "\tloss: 0.032073\n",
      "grad: tensor(14.8579, dtype=torch.float64)\n",
      "hess: tensor(149.5733)\n",
      "grad: tensor(0.0755, dtype=torch.float64)\n",
      "hess: tensor(1.9567)\n",
      "grad: tensor(1.9609, dtype=torch.float64)\n",
      "hess: tensor(28.6428)\n",
      "grad: tensor(1.7859, dtype=torch.float64)\n",
      "hess: tensor(32.0588)\n",
      "grad: tensor(5.8954, dtype=torch.float64)\n",
      "hess: tensor(77.2489)\n",
      "grad: tensor(0.4016, dtype=torch.float64)\n",
      "hess: tensor(10.7795)\n",
      "grad: tensor(1.4229, dtype=torch.float64)\n",
      "hess: tensor(20.9061)\n",
      "grad: tensor(2.7826, dtype=torch.float64)\n",
      "hess: tensor(41.2870)\n",
      "grad: tensor(0.1375, dtype=torch.float64)\n",
      "hess: tensor(2.2506)\n",
      "grad: tensor(8.4718, dtype=torch.float64)\n",
      "hess: tensor(109.9052)\n",
      "grad: tensor(7.0853, dtype=torch.float64)\n",
      "hess: tensor(65.5691)\n",
      "grad: tensor(4.6179, dtype=torch.float64)\n",
      "hess: tensor(62.8880)\n",
      "grad: tensor(1.0353, dtype=torch.float64)\n",
      "hess: tensor(19.4940)\n",
      "grad: tensor(0.1211, dtype=torch.float64)\n",
      "hess: tensor(2.5368)\n",
      "grad: tensor(0.0568, dtype=torch.float64)\n",
      "hess: tensor(1.6694)\n",
      "grad: tensor(6.2876, dtype=torch.float64)\n",
      "hess: tensor(58.4047)\n",
      "grad: tensor(0.5969, dtype=torch.float64)\n",
      "hess: tensor(9.3680)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4be38a11894d400d8e8b661852855a09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(0.0045, dtype=torch.float64)\n",
      "hess: tensor(0.1325)\n",
      "\taccuracy:91.015625\n",
      "\tloss: 0.000160\n",
      "grad: tensor(1.2360, dtype=torch.float64)\n",
      "hess: tensor(23.3221)\n",
      "grad: tensor(3.0189, dtype=torch.float64)\n",
      "hess: tensor(62.1040)\n",
      "grad: tensor(12.3685, dtype=torch.float64)\n",
      "hess: tensor(62.3671)\n",
      "grad: tensor(4.3480, dtype=torch.float64)\n",
      "hess: tensor(50.0266)\n",
      "grad: tensor(0.3639, dtype=torch.float64)\n",
      "hess: tensor(9.2279)\n",
      "grad: tensor(9.7236, dtype=torch.float64)\n",
      "hess: tensor(86.6660)\n",
      "grad: tensor(0.7911, dtype=torch.float64)\n",
      "hess: tensor(9.5734)\n",
      "grad: tensor(0.0434, dtype=torch.float64)\n",
      "hess: tensor(1.1162)\n",
      "grad: tensor(0.0132, dtype=torch.float64)\n",
      "hess: tensor(0.3217)\n",
      "grad: tensor(0.0991, dtype=torch.float64)\n",
      "hess: tensor(2.0703)\n",
      "grad: tensor(1.8367, dtype=torch.float64)\n",
      "hess: tensor(28.3656)\n",
      "grad: tensor(0.1925, dtype=torch.float64)\n",
      "hess: tensor(3.1722)\n",
      "grad: tensor(3.7434, dtype=torch.float64)\n",
      "hess: tensor(49.2356)\n",
      "grad: tensor(0.1275, dtype=torch.float64)\n",
      "hess: tensor(2.4867)\n",
      "grad: tensor(1.9554, dtype=torch.float64)\n",
      "hess: tensor(21.6951)\n",
      "grad: tensor(3.2298, dtype=torch.float64)\n",
      "hess: tensor(46.3077)\n",
      "\taccuracy:85.15625\n",
      "\tloss: 0.019747\n",
      "grad: tensor(12.4659, dtype=torch.float64)\n",
      "hess: tensor(139.7626)\n",
      "grad: tensor(0.0404, dtype=torch.float64)\n",
      "hess: tensor(1.1013)\n",
      "grad: tensor(1.6962, dtype=torch.float64)\n",
      "hess: tensor(27.0850)\n",
      "grad: tensor(1.5069, dtype=torch.float64)\n",
      "hess: tensor(29.0401)\n",
      "grad: tensor(5.1628, dtype=torch.float64)\n",
      "hess: tensor(75.5433)\n",
      "grad: tensor(0.3100, dtype=torch.float64)\n",
      "hess: tensor(8.9054)\n",
      "grad: tensor(0.8645, dtype=torch.float64)\n",
      "hess: tensor(13.6357)\n",
      "grad: tensor(2.3162, dtype=torch.float64)\n",
      "hess: tensor(37.9760)\n",
      "grad: tensor(0.0941, dtype=torch.float64)\n",
      "hess: tensor(1.6070)\n",
      "grad: tensor(5.2210, dtype=torch.float64)\n",
      "hess: tensor(86.9181)\n",
      "grad: tensor(6.4393, dtype=torch.float64)\n",
      "hess: tensor(65.5528)\n",
      "grad: tensor(3.7952, dtype=torch.float64)\n",
      "hess: tensor(57.3350)\n",
      "grad: tensor(0.8875, dtype=torch.float64)\n",
      "hess: tensor(18.0191)\n",
      "grad: tensor(0.0998, dtype=torch.float64)\n",
      "hess: tensor(2.2464)\n",
      "grad: tensor(0.0281, dtype=torch.float64)\n",
      "hess: tensor(0.8710)\n",
      "grad: tensor(6.2639, dtype=torch.float64)\n",
      "hess: tensor(61.3190)\n",
      "grad: tensor(0.4703, dtype=torch.float64)\n",
      "hess: tensor(7.8448)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46879b41cfb4497fad49a394b03d04e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(0.0018, dtype=torch.float64)\n",
      "hess: tensor(0.0542)\n",
      "\taccuracy:91.40625\n",
      "\tloss: 0.000060\n",
      "grad: tensor(1.0255, dtype=torch.float64)\n",
      "hess: tensor(20.6394)\n",
      "grad: tensor(2.7201, dtype=torch.float64)\n",
      "hess: tensor(59.5393)\n",
      "grad: tensor(12.8666, dtype=torch.float64)\n",
      "hess: tensor(60.9015)\n",
      "grad: tensor(3.7952, dtype=torch.float64)\n",
      "hess: tensor(47.6078)\n",
      "grad: tensor(0.2062, dtype=torch.float64)\n",
      "hess: tensor(5.5783)\n",
      "grad: tensor(10.4202, dtype=torch.float64)\n",
      "hess: tensor(93.8042)\n",
      "grad: tensor(0.6474, dtype=torch.float64)\n",
      "hess: tensor(8.2042)\n",
      "grad: tensor(0.0286, dtype=torch.float64)\n",
      "hess: tensor(0.7663)\n",
      "grad: tensor(0.0074, dtype=torch.float64)\n",
      "hess: tensor(0.1874)\n",
      "grad: tensor(0.0651, dtype=torch.float64)\n",
      "hess: tensor(1.4286)\n",
      "grad: tensor(1.3419, dtype=torch.float64)\n",
      "hess: tensor(22.2155)\n",
      "grad: tensor(0.1406, dtype=torch.float64)\n",
      "hess: tensor(2.4419)\n",
      "grad: tensor(3.6830, dtype=torch.float64)\n",
      "hess: tensor(50.2570)\n",
      "grad: tensor(0.1014, dtype=torch.float64)\n",
      "hess: tensor(2.1399)\n",
      "grad: tensor(1.7312, dtype=torch.float64)\n",
      "hess: tensor(20.5745)\n",
      "grad: tensor(2.4644, dtype=torch.float64)\n",
      "hess: tensor(38.6358)\n",
      "\taccuracy:88.28125\n",
      "\tloss: 0.013452\n",
      "grad: tensor(9.8831, dtype=torch.float64)\n",
      "hess: tensor(128.4751)\n",
      "grad: tensor(0.0251, dtype=torch.float64)\n",
      "hess: tensor(0.7089)\n",
      "grad: tensor(1.4806, dtype=torch.float64)\n",
      "hess: tensor(25.3745)\n",
      "grad: tensor(1.2873, dtype=torch.float64)\n",
      "hess: tensor(26.2519)\n",
      "grad: tensor(4.8450, dtype=torch.float64)\n",
      "hess: tensor(75.6145)\n",
      "grad: tensor(0.2641, dtype=torch.float64)\n",
      "hess: tensor(7.9789)\n",
      "grad: tensor(0.5876, dtype=torch.float64)\n",
      "hess: tensor(9.9000)\n",
      "grad: tensor(2.0070, dtype=torch.float64)\n",
      "hess: tensor(35.3918)\n",
      "grad: tensor(0.0731, dtype=torch.float64)\n",
      "hess: tensor(1.2826)\n",
      "grad: tensor(3.0850, dtype=torch.float64)\n",
      "hess: tensor(63.4568)\n",
      "grad: tensor(5.8195, dtype=torch.float64)\n",
      "hess: tensor(64.5183)\n",
      "grad: tensor(3.0404, dtype=torch.float64)\n",
      "hess: tensor(50.3198)\n",
      "grad: tensor(0.8019, dtype=torch.float64)\n",
      "hess: tensor(17.2495)\n",
      "grad: tensor(0.0895, dtype=torch.float64)\n",
      "hess: tensor(2.1213)\n",
      "grad: tensor(0.2961, dtype=torch.float64)\n",
      "hess: tensor(9.1076)\n",
      "grad: tensor(6.2787, dtype=torch.float64)\n",
      "hess: tensor(64.0508)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13f9e0a22f50458aa2afb6644f7e0ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(0.0008, dtype=torch.float64)\n",
      "hess: tensor(0.0247)\n",
      "\taccuracy:91.40625\n",
      "\tloss: 0.000026\n",
      "grad: tensor(0.4831, dtype=torch.float64)\n",
      "hess: tensor(6.6571)\n",
      "grad: tensor(13.6404, dtype=torch.float64)\n",
      "hess: tensor(142.7451)\n",
      "grad: tensor(3.2497, dtype=torch.float64)\n",
      "hess: tensor(44.0412)\n",
      "grad: tensor(0.6178, dtype=torch.float64)\n",
      "hess: tensor(14.1735)\n",
      "grad: tensor(0.0111, dtype=torch.float64)\n",
      "hess: tensor(0.2798)\n",
      "grad: tensor(0.0200, dtype=torch.float64)\n",
      "hess: tensor(0.5536)\n",
      "grad: tensor(0.1219, dtype=torch.float64)\n",
      "hess: tensor(2.6078)\n",
      "grad: tensor(0.0449, dtype=torch.float64)\n",
      "hess: tensor(1.1378)\n",
      "grad: tensor(0.1054, dtype=torch.float64)\n",
      "hess: tensor(1.9113)\n",
      "grad: tensor(0.3723, dtype=torch.float64)\n",
      "hess: tensor(4.2067)\n",
      "grad: tensor(0.0292, dtype=torch.float64)\n",
      "hess: tensor(0.7262)\n",
      "grad: tensor(1.9577, dtype=torch.float64)\n",
      "hess: tensor(32.8240)\n",
      "\taccuracy:88.28125\n",
      "\tloss: 0.009677\n",
      "grad: tensor(0.0703, dtype=torch.float64)\n",
      "hess: tensor(1.2370)\n",
      "grad: tensor(0.0319, dtype=torch.float64)\n",
      "hess: tensor(0.5939)\n",
      "grad: tensor(1.1429, dtype=torch.float64)\n",
      "hess: tensor(24.4313)\n",
      "grad: tensor(0.4286, dtype=torch.float64)\n",
      "hess: tensor(10.3059)\n",
      "grad: tensor(2.7056, dtype=torch.float64)\n",
      "hess: tensor(55.7673)\n",
      "grad: tensor(1.8722, dtype=torch.float64)\n",
      "hess: tensor(34.5755)\n",
      "grad: tensor(8.6108, dtype=torch.float64)\n",
      "hess: tensor(63.2413)\n",
      "grad: tensor(2.0860, dtype=torch.float64)\n",
      "hess: tensor(32.5744)\n",
      "grad: tensor(2.4468, dtype=torch.float64)\n",
      "hess: tensor(43.6329)\n",
      "grad: tensor(1.9029, dtype=torch.float64)\n",
      "hess: tensor(32.9760)\n",
      "grad: tensor(0.1721, dtype=torch.float64)\n",
      "hess: tensor(5.4643)\n",
      "grad: tensor(6.3173, dtype=torch.float64)\n",
      "hess: tensor(66.6454)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "241fe0504d05476296254a62e2967705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(0.0004, dtype=torch.float64)\n",
      "hess: tensor(0.0118)\n",
      "\taccuracy:91.40625\n",
      "\tloss: 0.000011\n",
      "grad: tensor(0.4200, dtype=torch.float64)\n",
      "hess: tensor(6.0446)\n",
      "grad: tensor(13.9863, dtype=torch.float64)\n",
      "hess: tensor(146.5914)\n",
      "grad: tensor(2.7982, dtype=torch.float64)\n",
      "hess: tensor(40.5312)\n",
      "grad: tensor(0.5318, dtype=torch.float64)\n",
      "hess: tensor(12.7566)\n",
      "grad: tensor(0.0079, dtype=torch.float64)\n",
      "hess: tensor(0.2057)\n",
      "grad: tensor(0.0151, dtype=torch.float64)\n",
      "hess: tensor(0.4284)\n",
      "grad: tensor(0.0830, dtype=torch.float64)\n",
      "hess: tensor(1.8437)\n",
      "grad: tensor(0.0363, dtype=torch.float64)\n",
      "hess: tensor(0.9505)\n",
      "grad: tensor(0.0816, dtype=torch.float64)\n",
      "hess: tensor(1.5351)\n",
      "grad: tensor(0.3337, dtype=torch.float64)\n",
      "hess: tensor(3.8528)\n",
      "grad: tensor(0.0226, dtype=torch.float64)\n",
      "hess: tensor(0.5808)\n",
      "grad: tensor(1.5938, dtype=torch.float64)\n",
      "hess: tensor(28.2442)\n",
      "\taccuracy:88.671875\n",
      "\tloss: 0.007423\n",
      "grad: tensor(0.0529, dtype=torch.float64)\n",
      "hess: tensor(0.9614)\n",
      "grad: tensor(0.0260, dtype=torch.float64)\n",
      "hess: tensor(0.4987)\n",
      "grad: tensor(1.0445, dtype=torch.float64)\n",
      "hess: tensor(23.2942)\n",
      "grad: tensor(0.3297, dtype=torch.float64)\n",
      "hess: tensor(8.2547)\n",
      "grad: tensor(2.6725, dtype=torch.float64)\n",
      "hess: tensor(56.3734)\n",
      "grad: tensor(1.7594, dtype=torch.float64)\n",
      "hess: tensor(33.9315)\n",
      "grad: tensor(7.9528, dtype=torch.float64)\n",
      "hess: tensor(60.8149)\n",
      "grad: tensor(1.9194, dtype=torch.float64)\n",
      "hess: tensor(31.3656)\n",
      "grad: tensor(1.9713, dtype=torch.float64)\n",
      "hess: tensor(37.0179)\n",
      "grad: tensor(1.9629, dtype=torch.float64)\n",
      "hess: tensor(34.9594)\n",
      "grad: tensor(0.1088, dtype=torch.float64)\n",
      "hess: tensor(3.5271)\n",
      "grad: tensor(6.3871, dtype=torch.float64)\n",
      "hess: tensor(69.2768)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d3af08b6154c309b46fe3b9b2d7ef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(0.0002, dtype=torch.float64)\n",
      "hess: tensor(0.0060)\n",
      "\taccuracy:91.40625\n",
      "\tloss: 0.000005\n",
      "grad: tensor(0.3816, dtype=torch.float64)\n",
      "hess: tensor(5.6503)\n",
      "grad: tensor(14.4766, dtype=torch.float64)\n",
      "hess: tensor(150.0987)\n",
      "grad: tensor(2.3487, dtype=torch.float64)\n",
      "hess: tensor(33.1273)\n",
      "grad: tensor(0.4653, dtype=torch.float64)\n",
      "hess: tensor(11.5811)\n",
      "grad: tensor(0.0056, dtype=torch.float64)\n",
      "hess: tensor(0.1508)\n",
      "grad: tensor(0.0120, dtype=torch.float64)\n",
      "hess: tensor(0.3529)\n",
      "grad: tensor(0.0580, dtype=torch.float64)\n",
      "hess: tensor(1.3285)\n",
      "grad: tensor(0.0297, dtype=torch.float64)\n",
      "hess: tensor(0.8041)\n",
      "grad: tensor(0.0633, dtype=torch.float64)\n",
      "hess: tensor(1.2279)\n",
      "grad: tensor(0.2950, dtype=torch.float64)\n",
      "hess: tensor(3.4906)\n",
      "grad: tensor(0.0187, dtype=torch.float64)\n",
      "hess: tensor(0.4921)\n",
      "grad: tensor(1.3132, dtype=torch.float64)\n",
      "hess: tensor(24.4141)\n",
      "\taccuracy:88.671875\n",
      "\tloss: 0.005933\n",
      "grad: tensor(0.0424, dtype=torch.float64)\n",
      "hess: tensor(0.7906)\n",
      "grad: tensor(0.0230, dtype=torch.float64)\n",
      "hess: tensor(0.4514)\n",
      "grad: tensor(0.9740, dtype=torch.float64)\n",
      "hess: tensor(22.5126)\n",
      "grad: tensor(0.2586, dtype=torch.float64)\n",
      "hess: tensor(6.6931)\n",
      "grad: tensor(2.4723, dtype=torch.float64)\n",
      "hess: tensor(54.4386)\n",
      "grad: tensor(1.6666, dtype=torch.float64)\n",
      "hess: tensor(33.3114)\n",
      "grad: tensor(7.2084, dtype=torch.float64)\n",
      "hess: tensor(60.9644)\n",
      "grad: tensor(1.8019, dtype=torch.float64)\n",
      "hess: tensor(30.6124)\n",
      "grad: tensor(1.6106, dtype=torch.float64)\n",
      "hess: tensor(31.7182)\n",
      "grad: tensor(2.0122, dtype=torch.float64)\n",
      "hess: tensor(36.7413)\n",
      "grad: tensor(0.0707, dtype=torch.float64)\n",
      "hess: tensor(2.3250)\n",
      "grad: tensor(6.3832, dtype=torch.float64)\n",
      "hess: tensor(71.3731)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f597b0e07d0f4818a62df41cc7766748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(9.3450e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0032)\n",
      "\taccuracy:91.015625\n",
      "\tloss: 0.000003\n",
      "grad: tensor(0.3601, dtype=torch.float64)\n",
      "hess: tensor(5.5104)\n",
      "grad: tensor(14.5520, dtype=torch.float64)\n",
      "hess: tensor(151.3354)\n",
      "grad: tensor(1.9719, dtype=torch.float64)\n",
      "hess: tensor(30.9321)\n",
      "grad: tensor(0.3937, dtype=torch.float64)\n",
      "hess: tensor(10.1300)\n",
      "grad: tensor(0.0042, dtype=torch.float64)\n",
      "hess: tensor(0.1165)\n",
      "grad: tensor(0.0097, dtype=torch.float64)\n",
      "hess: tensor(0.2934)\n",
      "grad: tensor(0.0405, dtype=torch.float64)\n",
      "hess: tensor(0.9538)\n",
      "grad: tensor(0.0248, dtype=torch.float64)\n",
      "hess: tensor(0.6884)\n",
      "grad: tensor(0.0505, dtype=torch.float64)\n",
      "hess: tensor(1.0069)\n",
      "grad: tensor(0.2689, dtype=torch.float64)\n",
      "hess: tensor(3.2494)\n",
      "grad: tensor(0.0157, dtype=torch.float64)\n",
      "hess: tensor(0.4235)\n",
      "grad: tensor(1.1412, dtype=torch.float64)\n",
      "hess: tensor(21.9109)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.004795\n",
      "grad: tensor(0.0338, dtype=torch.float64)\n",
      "hess: tensor(0.6368)\n",
      "grad: tensor(0.0207, dtype=torch.float64)\n",
      "hess: tensor(0.4189)\n",
      "grad: tensor(0.9347, dtype=torch.float64)\n",
      "hess: tensor(22.4092)\n",
      "grad: tensor(0.2097, dtype=torch.float64)\n",
      "hess: tensor(5.5736)\n",
      "grad: tensor(2.2757, dtype=torch.float64)\n",
      "hess: tensor(52.1601)\n",
      "grad: tensor(1.6345, dtype=torch.float64)\n",
      "hess: tensor(33.5546)\n",
      "grad: tensor(6.6669, dtype=torch.float64)\n",
      "hess: tensor(58.5960)\n",
      "grad: tensor(1.6990, dtype=torch.float64)\n",
      "hess: tensor(29.8641)\n",
      "grad: tensor(1.3320, dtype=torch.float64)\n",
      "hess: tensor(27.2087)\n",
      "grad: tensor(2.0895, dtype=torch.float64)\n",
      "hess: tensor(38.8653)\n",
      "grad: tensor(0.0587, dtype=torch.float64)\n",
      "hess: tensor(1.5705)\n",
      "grad: tensor(1.8825, dtype=torch.float64)\n",
      "hess: tensor(30.3132)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bc299a8d1af42f08be808f13bf26c7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(5.1343e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0018)\n",
      "\taccuracy:91.015625\n",
      "\tloss: 0.000002\n",
      "grad: tensor(0.6678, dtype=torch.float64)\n",
      "hess: tensor(9.2663)\n",
      "grad: tensor(13.9366, dtype=torch.float64)\n",
      "hess: tensor(138.0523)\n",
      "grad: tensor(0.0281, dtype=torch.float64)\n",
      "hess: tensor(0.8808)\n",
      "grad: tensor(0.0033, dtype=torch.float64)\n",
      "hess: tensor(0.0923)\n",
      "grad: tensor(9.6263, dtype=torch.float64)\n",
      "hess: tensor(124.9718)\n",
      "grad: tensor(0.0170, dtype=torch.float64)\n",
      "hess: tensor(0.4313)\n",
      "grad: tensor(0.3928, dtype=torch.float64)\n",
      "hess: tensor(9.8151)\n",
      "grad: tensor(0.2424, dtype=torch.float64)\n",
      "hess: tensor(2.9879)\n",
      "grad: tensor(1.3539, dtype=torch.float64)\n",
      "hess: tensor(19.3193)\n",
      "grad: tensor(0.0629, dtype=torch.float64)\n",
      "hess: tensor(1.0726)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.003833\n",
      "grad: tensor(6.7324, dtype=torch.float64)\n",
      "hess: tensor(65.4687)\n",
      "grad: tensor(0.9122, dtype=torch.float64)\n",
      "hess: tensor(22.6086)\n",
      "grad: tensor(11.1125, dtype=torch.float64)\n",
      "hess: tensor(80.0655)\n",
      "grad: tensor(1.2082, dtype=torch.float64)\n",
      "hess: tensor(26.2629)\n",
      "grad: tensor(0.0448, dtype=torch.float64)\n",
      "hess: tensor(0.8691)\n",
      "grad: tensor(1.6169, dtype=torch.float64)\n",
      "hess: tensor(29.3494)\n",
      "grad: tensor(9.9810, dtype=torch.float64)\n",
      "hess: tensor(127.8179)\n",
      "grad: tensor(0.0532, dtype=torch.float64)\n",
      "hess: tensor(1.4488)\n",
      "grad: tensor(1.7944, dtype=torch.float64)\n",
      "hess: tensor(29.8556)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff8bc97d674a4e8fbec592a59a1979f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(3.1098e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0011)\n",
      "\taccuracy:91.40625\n",
      "\tloss: 0.000001\n",
      "grad: tensor(0.6163, dtype=torch.float64)\n",
      "hess: tensor(8.8203)\n",
      "grad: tensor(12.9525, dtype=torch.float64)\n",
      "hess: tensor(135.8791)\n",
      "grad: tensor(0.0206, dtype=torch.float64)\n",
      "hess: tensor(0.6577)\n",
      "grad: tensor(0.0028, dtype=torch.float64)\n",
      "hess: tensor(0.0808)\n",
      "grad: tensor(8.6227, dtype=torch.float64)\n",
      "hess: tensor(120.5818)\n",
      "grad: tensor(0.0137, dtype=torch.float64)\n",
      "hess: tensor(0.3553)\n",
      "grad: tensor(0.3619, dtype=torch.float64)\n",
      "hess: tensor(9.2467)\n",
      "grad: tensor(0.2230, dtype=torch.float64)\n",
      "hess: tensor(2.8169)\n",
      "grad: tensor(1.2879, dtype=torch.float64)\n",
      "hess: tensor(18.8604)\n",
      "grad: tensor(0.0560, dtype=torch.float64)\n",
      "hess: tensor(0.9954)\n",
      "\taccuracy:89.0625\n",
      "\tloss: 0.003297\n",
      "grad: tensor(6.7860, dtype=torch.float64)\n",
      "hess: tensor(67.7225)\n",
      "grad: tensor(0.9038, dtype=torch.float64)\n",
      "hess: tensor(23.0043)\n",
      "grad: tensor(10.8327, dtype=torch.float64)\n",
      "hess: tensor(82.2995)\n",
      "grad: tensor(1.1045, dtype=torch.float64)\n",
      "hess: tensor(24.8494)\n",
      "grad: tensor(0.0430, dtype=torch.float64)\n",
      "hess: tensor(0.8458)\n",
      "grad: tensor(1.5812, dtype=torch.float64)\n",
      "hess: tensor(29.4257)\n",
      "grad: tensor(9.9051, dtype=torch.float64)\n",
      "hess: tensor(128.3806)\n",
      "grad: tensor(0.0468, dtype=torch.float64)\n",
      "hess: tensor(1.3045)\n",
      "grad: tensor(1.6612, dtype=torch.float64)\n",
      "hess: tensor(28.5634)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5633504c370d4f4a9a77aa81c9fbb955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(2.0783e-05, dtype=torch.float64)\n",
      "hess: tensor(0.0007)\n",
      "\taccuracy:91.40625\n",
      "\tloss: 0.000001\n",
      "grad: tensor(0.5768, dtype=torch.float64)\n",
      "hess: tensor(8.4897)\n",
      "grad: tensor(12.2112, dtype=torch.float64)\n",
      "hess: tensor(134.4156)\n",
      "grad: tensor(0.0149, dtype=torch.float64)\n",
      "hess: tensor(0.4835)\n",
      "grad: tensor(0.0023, dtype=torch.float64)\n",
      "hess: tensor(0.0687)\n",
      "grad: tensor(8.0286, dtype=torch.float64)\n",
      "hess: tensor(108.7518)\n",
      "grad: tensor(0.0103, dtype=torch.float64)\n",
      "hess: tensor(0.2723)\n",
      "grad: tensor(0.3258, dtype=torch.float64)\n",
      "hess: tensor(8.5074)\n",
      "grad: tensor(0.2042, dtype=torch.float64)\n",
      "hess: tensor(2.6030)\n",
      "grad: tensor(1.3462, dtype=torch.float64)\n",
      "hess: tensor(21.1110)\n",
      "grad: tensor(0.0482, dtype=torch.float64)\n",
      "hess: tensor(0.8773)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.002776\n",
      "grad: tensor(6.8510, dtype=torch.float64)\n",
      "hess: tensor(70.0010)\n",
      "grad: tensor(0.8788, dtype=torch.float64)\n",
      "hess: tensor(23.0609)\n",
      "grad: tensor(11.3279, dtype=torch.float64)\n",
      "hess: tensor(84.8255)\n",
      "grad: tensor(1.0248, dtype=torch.float64)\n",
      "hess: tensor(23.7965)\n",
      "grad: tensor(0.0405, dtype=torch.float64)\n",
      "hess: tensor(0.8087)\n",
      "grad: tensor(1.5275, dtype=torch.float64)\n",
      "hess: tensor(29.1898)\n",
      "grad: tensor(9.4202, dtype=torch.float64)\n",
      "hess: tensor(126.8930)\n",
      "grad: tensor(0.0407, dtype=torch.float64)\n",
      "hess: tensor(1.1652)\n",
      "grad: tensor(1.6165, dtype=torch.float64)\n",
      "hess: tensor(28.5803)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e5fda926c794f01a3c57ce7c84ddbd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(8.5815e-06, dtype=torch.float64)\n",
      "hess: tensor(0.0003)\n",
      "\taccuracy:91.796875\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.5393, dtype=torch.float64)\n",
      "hess: tensor(8.1592)\n",
      "grad: tensor(11.7546, dtype=torch.float64)\n",
      "hess: tensor(134.2470)\n",
      "grad: tensor(0.0116, dtype=torch.float64)\n",
      "hess: tensor(0.3821)\n",
      "grad: tensor(0.0020, dtype=torch.float64)\n",
      "hess: tensor(0.0592)\n",
      "grad: tensor(7.5289, dtype=torch.float64)\n",
      "hess: tensor(108.7723)\n",
      "grad: tensor(0.0079, dtype=torch.float64)\n",
      "hess: tensor(0.2150)\n",
      "grad: tensor(0.3054, dtype=torch.float64)\n",
      "hess: tensor(8.1306)\n",
      "grad: tensor(0.1915, dtype=torch.float64)\n",
      "hess: tensor(2.4973)\n",
      "grad: tensor(1.3151, dtype=torch.float64)\n",
      "hess: tensor(21.1770)\n",
      "grad: tensor(0.0415, dtype=torch.float64)\n",
      "hess: tensor(0.7715)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.002340\n",
      "grad: tensor(6.7879, dtype=torch.float64)\n",
      "hess: tensor(67.9265)\n",
      "grad: tensor(0.8550, dtype=torch.float64)\n",
      "hess: tensor(22.9148)\n",
      "grad: tensor(11.2584, dtype=torch.float64)\n",
      "hess: tensor(87.0411)\n",
      "grad: tensor(0.9431, dtype=torch.float64)\n",
      "hess: tensor(22.5710)\n",
      "grad: tensor(0.0393, dtype=torch.float64)\n",
      "hess: tensor(0.7960)\n",
      "grad: tensor(1.5139, dtype=torch.float64)\n",
      "hess: tensor(29.5593)\n",
      "grad: tensor(8.7098, dtype=torch.float64)\n",
      "hess: tensor(123.3727)\n",
      "grad: tensor(0.0358, dtype=torch.float64)\n",
      "hess: tensor(1.0464)\n",
      "grad: tensor(1.5984, dtype=torch.float64)\n",
      "hess: tensor(28.9582)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0647738387f445ab52ddf9dbf4b879e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(7.6855e-06, dtype=torch.float64)\n",
      "hess: tensor(0.0003)\n",
      "\taccuracy:91.796875\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.4965, dtype=torch.float64)\n",
      "hess: tensor(7.4739)\n",
      "grad: tensor(11.3509, dtype=torch.float64)\n",
      "hess: tensor(132.5065)\n",
      "grad: tensor(0.0084, dtype=torch.float64)\n",
      "hess: tensor(0.2831)\n",
      "grad: tensor(0.0017, dtype=torch.float64)\n",
      "hess: tensor(0.0513)\n",
      "grad: tensor(6.6930, dtype=torch.float64)\n",
      "hess: tensor(107.9810)\n",
      "grad: tensor(0.0060, dtype=torch.float64)\n",
      "hess: tensor(0.1670)\n",
      "grad: tensor(0.2884, dtype=torch.float64)\n",
      "hess: tensor(7.8232)\n",
      "grad: tensor(0.1749, dtype=torch.float64)\n",
      "hess: tensor(2.2721)\n",
      "grad: tensor(1.2384, dtype=torch.float64)\n",
      "hess: tensor(20.5200)\n",
      "grad: tensor(0.0360, dtype=torch.float64)\n",
      "hess: tensor(0.6825)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.001986\n",
      "grad: tensor(6.7627, dtype=torch.float64)\n",
      "hess: tensor(69.5947)\n",
      "grad: tensor(0.8208, dtype=torch.float64)\n",
      "hess: tensor(22.4855)\n",
      "grad: tensor(10.8996, dtype=torch.float64)\n",
      "hess: tensor(89.2255)\n",
      "grad: tensor(0.8356, dtype=torch.float64)\n",
      "hess: tensor(20.6378)\n",
      "grad: tensor(0.0369, dtype=torch.float64)\n",
      "hess: tensor(0.7588)\n",
      "grad: tensor(1.5071, dtype=torch.float64)\n",
      "hess: tensor(30.0033)\n",
      "grad: tensor(8.2728, dtype=torch.float64)\n",
      "hess: tensor(121.7455)\n",
      "grad: tensor(0.0294, dtype=torch.float64)\n",
      "hess: tensor(0.8790)\n",
      "grad: tensor(5.4170, dtype=torch.float64)\n",
      "hess: tensor(71.5841)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "143b9b14818c49308dedb440ac367f7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(1.9132e-06, dtype=torch.float64)\n",
      "hess: tensor(0.0001)\n",
      "\taccuracy:91.796875\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.4447, dtype=torch.float64)\n",
      "hess: tensor(15.5862)\n",
      "grad: tensor(0.9078, dtype=torch.float64)\n",
      "hess: tensor(17.4332)\n",
      "grad: tensor(9.2988, dtype=torch.float64)\n",
      "hess: tensor(114.4373)\n",
      "grad: tensor(0.0046, dtype=torch.float64)\n",
      "hess: tensor(0.1539)\n",
      "grad: tensor(0.0046, dtype=torch.float64)\n",
      "hess: tensor(0.1290)\n",
      "grad: tensor(0.0135, dtype=torch.float64)\n",
      "hess: tensor(0.3096)\n",
      "grad: tensor(0.0471, dtype=torch.float64)\n",
      "hess: tensor(1.2938)\n",
      "grad: tensor(0.4094, dtype=torch.float64)\n",
      "hess: tensor(9.2054)\n",
      "\taccuracy:89.453125\n",
      "\tloss: 0.001617\n",
      "grad: tensor(0.0021, dtype=torch.float64)\n",
      "hess: tensor(0.0728)\n",
      "grad: tensor(0.7774, dtype=torch.float64)\n",
      "hess: tensor(21.8480)\n",
      "grad: tensor(0.1803, dtype=torch.float64)\n",
      "hess: tensor(6.9330)\n",
      "grad: tensor(1.3064, dtype=torch.float64)\n",
      "hess: tensor(31.2436)\n",
      "grad: tensor(0.0236, dtype=torch.float64)\n",
      "hess: tensor(0.8529)\n",
      "grad: tensor(0.6996, dtype=torch.float64)\n",
      "hess: tensor(16.7736)\n",
      "grad: tensor(0.0264, dtype=torch.float64)\n",
      "hess: tensor(0.8026)\n",
      "grad: tensor(5.1083, dtype=torch.float64)\n",
      "hess: tensor(70.0668)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7a95381a2264fd3a12a98cdd33b9ffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(1.2634e-06, dtype=torch.float64)\n",
      "hess: tensor(7.1804e-05)\n",
      "\taccuracy:92.1875\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.3638, dtype=torch.float64)\n",
      "hess: tensor(13.1110)\n",
      "grad: tensor(0.7871, dtype=torch.float64)\n",
      "hess: tensor(15.5545)\n",
      "grad: tensor(9.0772, dtype=torch.float64)\n",
      "hess: tensor(114.6154)\n",
      "grad: tensor(0.0042, dtype=torch.float64)\n",
      "hess: tensor(0.1448)\n",
      "grad: tensor(0.0036, dtype=torch.float64)\n",
      "hess: tensor(0.1028)\n",
      "grad: tensor(0.0111, dtype=torch.float64)\n",
      "hess: tensor(0.2594)\n",
      "grad: tensor(0.0442, dtype=torch.float64)\n",
      "hess: tensor(1.2379)\n",
      "grad: tensor(0.3568, dtype=torch.float64)\n",
      "hess: tensor(8.1640)\n",
      "\taccuracy:90.625\n",
      "\tloss: 0.001420\n",
      "grad: tensor(0.0017, dtype=torch.float64)\n",
      "hess: tensor(0.0615)\n",
      "grad: tensor(0.7572, dtype=torch.float64)\n",
      "hess: tensor(21.6653)\n",
      "grad: tensor(0.1719, dtype=torch.float64)\n",
      "hess: tensor(6.7210)\n",
      "grad: tensor(1.2539, dtype=torch.float64)\n",
      "hess: tensor(30.5877)\n",
      "grad: tensor(0.0163, dtype=torch.float64)\n",
      "hess: tensor(0.6013)\n",
      "grad: tensor(0.6620, dtype=torch.float64)\n",
      "hess: tensor(16.1768)\n",
      "grad: tensor(0.0221, dtype=torch.float64)\n",
      "hess: tensor(0.6869)\n",
      "grad: tensor(4.8092, dtype=torch.float64)\n",
      "hess: tensor(68.4233)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17950c348729416d95f9b5a30db4356e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(8.8151e-07, dtype=torch.float64)\n",
      "hess: tensor(4.6367e-05)\n",
      "\taccuracy:91.796875\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.2996, dtype=torch.float64)\n",
      "hess: tensor(11.0666)\n",
      "grad: tensor(0.6958, dtype=torch.float64)\n",
      "hess: tensor(14.0876)\n",
      "grad: tensor(9.1151, dtype=torch.float64)\n",
      "hess: tensor(116.2242)\n",
      "grad: tensor(0.0038, dtype=torch.float64)\n",
      "hess: tensor(0.1311)\n",
      "grad: tensor(0.0028, dtype=torch.float64)\n",
      "hess: tensor(0.0826)\n",
      "grad: tensor(0.0090, dtype=torch.float64)\n",
      "hess: tensor(0.2150)\n",
      "grad: tensor(0.0404, dtype=torch.float64)\n",
      "hess: tensor(1.1515)\n",
      "grad: tensor(0.3027, dtype=torch.float64)\n",
      "hess: tensor(7.0692)\n",
      "\taccuracy:90.234375\n",
      "\tloss: 0.001174\n",
      "grad: tensor(0.0015, dtype=torch.float64)\n",
      "hess: tensor(0.0532)\n",
      "grad: tensor(0.7281, dtype=torch.float64)\n",
      "hess: tensor(21.2986)\n",
      "grad: tensor(0.1747, dtype=torch.float64)\n",
      "hess: tensor(6.9358)\n",
      "grad: tensor(1.2467, dtype=torch.float64)\n",
      "hess: tensor(30.9175)\n",
      "grad: tensor(0.0111, dtype=torch.float64)\n",
      "hess: tensor(0.4165)\n",
      "grad: tensor(0.6092, dtype=torch.float64)\n",
      "hess: tensor(15.2041)\n",
      "grad: tensor(0.0199, dtype=torch.float64)\n",
      "hess: tensor(0.6271)\n",
      "grad: tensor(4.4919, dtype=torch.float64)\n",
      "hess: tensor(66.3722)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2436666e970040858b470abf4ff445af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(6.7558e-07, dtype=torch.float64)\n",
      "hess: tensor(3.5862e-05)\n",
      "\taccuracy:91.796875\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.2556, dtype=torch.float64)\n",
      "hess: tensor(9.6597)\n",
      "grad: tensor(0.6043, dtype=torch.float64)\n",
      "hess: tensor(12.5468)\n",
      "grad: tensor(8.0874, dtype=torch.float64)\n",
      "hess: tensor(111.0664)\n",
      "grad: tensor(0.0034, dtype=torch.float64)\n",
      "hess: tensor(0.1190)\n",
      "grad: tensor(0.0022, dtype=torch.float64)\n",
      "hess: tensor(0.0644)\n",
      "grad: tensor(0.0072, dtype=torch.float64)\n",
      "hess: tensor(0.1748)\n",
      "grad: tensor(0.0394, dtype=torch.float64)\n",
      "hess: tensor(1.1435)\n",
      "grad: tensor(0.2555, dtype=torch.float64)\n",
      "hess: tensor(6.0478)\n",
      "\taccuracy:90.234375\n",
      "\tloss: 0.001045\n",
      "grad: tensor(0.0013, dtype=torch.float64)\n",
      "hess: tensor(0.0458)\n",
      "grad: tensor(0.7111, dtype=torch.float64)\n",
      "hess: tensor(21.1711)\n",
      "grad: tensor(0.1683, dtype=torch.float64)\n",
      "hess: tensor(6.7851)\n",
      "grad: tensor(1.2062, dtype=torch.float64)\n",
      "hess: tensor(30.4094)\n",
      "grad: tensor(0.0082, dtype=torch.float64)\n",
      "hess: tensor(0.3116)\n",
      "grad: tensor(0.5777, dtype=torch.float64)\n",
      "hess: tensor(14.6887)\n",
      "grad: tensor(0.0172, dtype=torch.float64)\n",
      "hess: tensor(0.5517)\n",
      "grad: tensor(4.2316, dtype=torch.float64)\n",
      "hess: tensor(64.6923)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a895144d458f438ba365a06e868c47dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(4.7744e-07, dtype=torch.float64)\n",
      "hess: tensor(2.6116e-05)\n",
      "\taccuracy:91.796875\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.2472, dtype=torch.float64)\n",
      "hess: tensor(9.4988)\n",
      "grad: tensor(0.5164, dtype=torch.float64)\n",
      "hess: tensor(10.9845)\n",
      "grad: tensor(8.0734, dtype=torch.float64)\n",
      "hess: tensor(112.1919)\n",
      "grad: tensor(0.0032, dtype=torch.float64)\n",
      "hess: tensor(0.1133)\n",
      "grad: tensor(0.0016, dtype=torch.float64)\n",
      "hess: tensor(0.0482)\n",
      "grad: tensor(0.0061, dtype=torch.float64)\n",
      "hess: tensor(0.1502)\n",
      "grad: tensor(0.0368, dtype=torch.float64)\n",
      "hess: tensor(1.0856)\n",
      "grad: tensor(0.2255, dtype=torch.float64)\n",
      "hess: tensor(5.4345)\n",
      "\taccuracy:90.234375\n",
      "\tloss: 0.000865\n",
      "grad: tensor(0.0011, dtype=torch.float64)\n",
      "hess: tensor(0.0419)\n",
      "grad: tensor(0.6813, dtype=torch.float64)\n",
      "hess: tensor(20.6979)\n",
      "grad: tensor(0.1784, dtype=torch.float64)\n",
      "hess: tensor(7.2838)\n",
      "grad: tensor(1.1186, dtype=torch.float64)\n",
      "hess: tensor(28.9008)\n",
      "grad: tensor(0.0056, dtype=torch.float64)\n",
      "hess: tensor(0.2165)\n",
      "grad: tensor(0.5561, dtype=torch.float64)\n",
      "hess: tensor(14.3910)\n",
      "grad: tensor(0.0157, dtype=torch.float64)\n",
      "hess: tensor(0.5130)\n",
      "grad: tensor(3.9786, dtype=torch.float64)\n",
      "hess: tensor(62.8358)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc16f558b0bb4ffdbeeb62c3e13d9e25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(3.4903e-07, dtype=torch.float64)\n",
      "hess: tensor(1.9080e-05)\n",
      "\taccuracy:92.1875\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.1911, dtype=torch.float64)\n",
      "hess: tensor(7.5057)\n",
      "grad: tensor(0.4617, dtype=torch.float64)\n",
      "hess: tensor(10.0191)\n",
      "grad: tensor(7.5235, dtype=torch.float64)\n",
      "hess: tensor(109.4864)\n",
      "grad: tensor(0.0029, dtype=torch.float64)\n",
      "hess: tensor(0.1030)\n",
      "grad: tensor(0.0012, dtype=torch.float64)\n",
      "hess: tensor(0.0382)\n",
      "grad: tensor(0.0051, dtype=torch.float64)\n",
      "hess: tensor(0.1262)\n",
      "grad: tensor(0.0356, dtype=torch.float64)\n",
      "hess: tensor(1.0678)\n",
      "grad: tensor(0.1887, dtype=torch.float64)\n",
      "hess: tensor(4.6226)\n",
      "\taccuracy:90.234375\n",
      "\tloss: 0.000753\n",
      "grad: tensor(0.0009, dtype=torch.float64)\n",
      "hess: tensor(0.0359)\n",
      "grad: tensor(0.6167, dtype=torch.float64)\n",
      "hess: tensor(19.1527)\n",
      "grad: tensor(0.1579, dtype=torch.float64)\n",
      "hess: tensor(6.5558)\n",
      "grad: tensor(1.0948, dtype=torch.float64)\n",
      "hess: tensor(28.7319)\n",
      "grad: tensor(0.5935, dtype=torch.float64)\n",
      "hess: tensor(21.2442)\n",
      "grad: tensor(0.5436, dtype=torch.float64)\n",
      "hess: tensor(14.2765)\n",
      "grad: tensor(0.0078, dtype=torch.float64)\n",
      "hess: tensor(0.2795)\n",
      "grad: tensor(0.4484, dtype=torch.float64)\n",
      "hess: tensor(12.6730)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5860558001b549d3a70d1bec172863d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(3.0002e-07, dtype=torch.float64)\n",
      "hess: tensor(1.6662e-05)\n",
      "\taccuracy:92.1875\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.0746, dtype=torch.float64)\n",
      "hess: tensor(1.8584)\n",
      "grad: tensor(5.9774, dtype=torch.float64)\n",
      "hess: tensor(70.8735)\n",
      "grad: tensor(0.4380, dtype=torch.float64)\n",
      "hess: tensor(7.2381)\n",
      "grad: tensor(0.0024, dtype=torch.float64)\n",
      "hess: tensor(0.0691)\n",
      "grad: tensor(0.2084, dtype=torch.float64)\n",
      "hess: tensor(6.2951)\n",
      "grad: tensor(0.0343, dtype=torch.float64)\n",
      "hess: tensor(1.0461)\n",
      "grad: tensor(0.0070, dtype=torch.float64)\n",
      "hess: tensor(0.2170)\n",
      "\taccuracy:90.234375\n",
      "\tloss: 0.000646\n",
      "grad: tensor(0.0077, dtype=torch.float64)\n",
      "hess: tensor(0.1944)\n",
      "grad: tensor(2.9026, dtype=torch.float64)\n",
      "hess: tensor(73.4673)\n",
      "grad: tensor(0.3881, dtype=torch.float64)\n",
      "hess: tensor(11.0089)\n",
      "grad: tensor(0.5685, dtype=torch.float64)\n",
      "hess: tensor(20.7445)\n",
      "grad: tensor(0.5137, dtype=torch.float64)\n",
      "hess: tensor(13.7284)\n",
      "grad: tensor(0.0073, dtype=torch.float64)\n",
      "hess: tensor(0.2658)\n",
      "grad: tensor(0.4202, dtype=torch.float64)\n",
      "hess: tensor(12.0724)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6bf1630e8e44c68af845e99048313a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(2.2303e-07, dtype=torch.float64)\n",
      "hess: tensor(1.1958e-05)\n",
      "\taccuracy:92.1875\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.0672, dtype=torch.float64)\n",
      "hess: tensor(1.6964)\n",
      "grad: tensor(5.7417, dtype=torch.float64)\n",
      "hess: tensor(69.9043)\n",
      "grad: tensor(0.4245, dtype=torch.float64)\n",
      "hess: tensor(7.1069)\n",
      "grad: tensor(0.0019, dtype=torch.float64)\n",
      "hess: tensor(0.0576)\n",
      "grad: tensor(0.2029, dtype=torch.float64)\n",
      "hess: tensor(6.2087)\n",
      "grad: tensor(0.0330, dtype=torch.float64)\n",
      "hess: tensor(1.0197)\n",
      "grad: tensor(0.0057, dtype=torch.float64)\n",
      "hess: tensor(0.1789)\n",
      "\taccuracy:90.234375\n",
      "\tloss: 0.000538\n",
      "grad: tensor(0.0070, dtype=torch.float64)\n",
      "hess: tensor(0.1791)\n",
      "grad: tensor(2.6474, dtype=torch.float64)\n",
      "hess: tensor(69.7256)\n",
      "grad: tensor(0.3531, dtype=torch.float64)\n",
      "hess: tensor(10.2007)\n",
      "grad: tensor(0.5442, dtype=torch.float64)\n",
      "hess: tensor(20.1694)\n",
      "grad: tensor(0.4856, dtype=torch.float64)\n",
      "hess: tensor(13.1930)\n",
      "grad: tensor(0.0070, dtype=torch.float64)\n",
      "hess: tensor(0.2565)\n",
      "grad: tensor(0.3964, dtype=torch.float64)\n",
      "hess: tensor(11.5804)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b222ea623a74861806a1feba49ed8b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(1.7944e-07, dtype=torch.float64)\n",
      "hess: tensor(9.6984e-06)\n",
      "\taccuracy:92.578125\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.0592, dtype=torch.float64)\n",
      "hess: tensor(1.5147)\n",
      "grad: tensor(5.4753, dtype=torch.float64)\n",
      "hess: tensor(68.5719)\n",
      "grad: tensor(0.4298, dtype=torch.float64)\n",
      "hess: tensor(7.2767)\n",
      "grad: tensor(0.0017, dtype=torch.float64)\n",
      "hess: tensor(0.0520)\n",
      "grad: tensor(0.1952, dtype=torch.float64)\n",
      "hess: tensor(6.0541)\n",
      "grad: tensor(0.0301, dtype=torch.float64)\n",
      "hess: tensor(0.9424)\n",
      "grad: tensor(0.0049, dtype=torch.float64)\n",
      "hess: tensor(0.1578)\n",
      "\taccuracy:90.625\n",
      "\tloss: 0.000483\n",
      "grad: tensor(0.0063, dtype=torch.float64)\n",
      "hess: tensor(0.1649)\n",
      "grad: tensor(2.6253, dtype=torch.float64)\n",
      "hess: tensor(70.1268)\n",
      "grad: tensor(0.3123, dtype=torch.float64)\n",
      "hess: tensor(9.1292)\n",
      "grad: tensor(0.5310, dtype=torch.float64)\n",
      "hess: tensor(20.0129)\n",
      "grad: tensor(0.4595, dtype=torch.float64)\n",
      "hess: tensor(12.6706)\n",
      "grad: tensor(0.0067, dtype=torch.float64)\n",
      "hess: tensor(0.2506)\n",
      "grad: tensor(0.3595, dtype=torch.float64)\n",
      "hess: tensor(10.6869)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71fc907d56ec4cc386f3930ab6ed32e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(1.4719e-07, dtype=torch.float64)\n",
      "hess: tensor(8.1444e-06)\n",
      "\taccuracy:92.578125\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.0573, dtype=torch.float64)\n",
      "hess: tensor(1.4838)\n",
      "grad: tensor(5.3355, dtype=torch.float64)\n",
      "hess: tensor(67.9918)\n",
      "grad: tensor(0.4160, dtype=torch.float64)\n",
      "hess: tensor(7.1360)\n",
      "grad: tensor(0.0014, dtype=torch.float64)\n",
      "hess: tensor(0.0432)\n",
      "grad: tensor(0.1875, dtype=torch.float64)\n",
      "hess: tensor(5.8963)\n",
      "grad: tensor(0.0306, dtype=torch.float64)\n",
      "hess: tensor(0.9715)\n",
      "grad: tensor(0.0039, dtype=torch.float64)\n",
      "hess: tensor(0.1268)\n",
      "\taccuracy:90.625\n",
      "\tloss: 0.000418\n",
      "grad: tensor(0.0057, dtype=torch.float64)\n",
      "hess: tensor(0.1500)\n",
      "grad: tensor(2.4853, dtype=torch.float64)\n",
      "hess: tensor(68.1781)\n",
      "grad: tensor(0.2697, dtype=torch.float64)\n",
      "hess: tensor(8.0227)\n",
      "grad: tensor(0.5203, dtype=torch.float64)\n",
      "hess: tensor(19.9057)\n",
      "grad: tensor(0.4297, dtype=torch.float64)\n",
      "hess: tensor(12.0314)\n",
      "grad: tensor(0.0063, dtype=torch.float64)\n",
      "hess: tensor(0.2348)\n",
      "grad: tensor(0.3357, dtype=torch.float64)\n",
      "hess: tensor(10.1102)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "346522e5d490434baa9a4aff2ab4f00a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(1.1275e-07, dtype=torch.float64)\n",
      "hess: tensor(5.8590e-06)\n",
      "\taccuracy:92.1875\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.0523, dtype=torch.float64)\n",
      "hess: tensor(1.3696)\n",
      "grad: tensor(5.1632, dtype=torch.float64)\n",
      "hess: tensor(67.3815)\n",
      "grad: tensor(0.4196, dtype=torch.float64)\n",
      "hess: tensor(7.2689)\n",
      "grad: tensor(0.0012, dtype=torch.float64)\n",
      "hess: tensor(0.0370)\n",
      "grad: tensor(0.1814, dtype=torch.float64)\n",
      "hess: tensor(5.7617)\n",
      "grad: tensor(0.0283, dtype=torch.float64)\n",
      "hess: tensor(0.9108)\n",
      "grad: tensor(0.0034, dtype=torch.float64)\n",
      "hess: tensor(0.1116)\n",
      "\taccuracy:90.625\n",
      "\tloss: 0.000358\n",
      "grad: tensor(0.0052, dtype=torch.float64)\n",
      "hess: tensor(0.1383)\n",
      "grad: tensor(2.3603, dtype=torch.float64)\n",
      "hess: tensor(66.3696)\n",
      "grad: tensor(0.2577, dtype=torch.float64)\n",
      "hess: tensor(7.7683)\n",
      "grad: tensor(0.5093, dtype=torch.float64)\n",
      "hess: tensor(19.7856)\n",
      "grad: tensor(0.4108, dtype=torch.float64)\n",
      "hess: tensor(11.6494)\n",
      "grad: tensor(0.0063, dtype=torch.float64)\n",
      "hess: tensor(0.2374)\n",
      "grad: tensor(0.3199, dtype=torch.float64)\n",
      "hess: tensor(9.9101)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e61c0a1057a2464092f5d383b24a788f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(1.0061e-07, dtype=torch.float64)\n",
      "hess: tensor(5.2404e-06)\n",
      "\taccuracy:92.1875\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.0454, dtype=torch.float64)\n",
      "hess: tensor(1.2004)\n",
      "grad: tensor(4.8942, dtype=torch.float64)\n",
      "hess: tensor(65.6725)\n",
      "grad: tensor(0.4051, dtype=torch.float64)\n",
      "hess: tensor(7.1020)\n",
      "grad: tensor(0.0010, dtype=torch.float64)\n",
      "hess: tensor(11.3047)\n",
      "grad: tensor(0.0059, dtype=torch.float64)\n",
      "hess: tensor(0.2260)\n",
      "grad: tensor(0.2971, dtype=torch.float64)\n",
      "hess: tensor(9.1746)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4266e2ef40814c32b81f5f2462823bdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(8.2714e-08, dtype=torch.float64)\n",
      "hess: tensor(4.2832e-06)\n",
      "\taccuracy:92.1875\n",
      "\tloss: 0.000000\n",
      "grad: tensor(0.0447, dtype=torch.float64)\n",
      "hess: tensor(1.1983)\n",
      "grad: tensor(4.6963, dtype=torch.float64)\n",
      "hess: tensor(64.3478)\n",
      "grad: tensor(0.4031, dtype=torch.float64)\n",
      "hess: tensor(7.1403)\n",
      "grad: tensor(0.0009, dtype=torch.float64)\n",
      "hess: tensor(0.0279)\n",
      "grad: tensor(0.1641, dtype=torch.float64)\n",
      "hess: tensor(5.3287)\n",
      "grad: tensor(0.0720, dtype=torch.float64)\n",
      "hess: tensor(1.0781)\n",
      "grad: tensor(0.0746, dtype=torch.float64)\n",
      "hess: tensor(2.0099)\n",
      "\taccuracy:90.234375\n",
      "\tloss: 0.000284\n",
      "grad: tensor(0.0048, dtype=torch.float64)\n",
      "hess: tensor(0.1441)\n",
      "grad: tensor(0.0364, dtype=torch.float64)\n",
      "hess: tensor(1.2977)\n",
      "grad: tensor(0.8498, dtype=torch.float64)\n",
      "hess: tensor(24.6118)\n",
      "grad: tensor(1.1930, dtype=torch.float64)\n",
      "hess: tensor(29.5503)\n",
      "grad: tensor(3.1529, dtype=torch.float64)\n",
      "hess: tensor(67.8077)\n",
      "grad: tensor(2.4049, dtype=torch.float64)\n",
      "hess: tensor(47.6072)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c868dec9eaca40dd8d0512500a14d84b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(6.7558e-08, dtype=torch.float64)\n",
      "hess: tensor(3.5184e-06)\n",
      "\taccuracy:91.796875\n",
      "\tloss: 0.000000\n",
      "grad: tensor(5.4086, dtype=torch.float64)\n",
      "hess: tensor(111.3229)\n",
      "grad: tensor(0.0410, dtype=torch.float64)\n",
      "hess: tensor(1.4278)\n",
      "grad: tensor(0.0021, dtype=torch.float64)\n",
      "hess: tensor(0.0852)\n",
      "grad: tensor(0.0026, dtype=torch.float64)\n",
      "hess: tensor(0.1104)\n",
      "grad: tensor(0.0685, dtype=torch.float64)\n",
      "hess: tensor(1.0347)\n",
      "grad: tensor(0.0684, dtype=torch.float64)\n",
      "hess: tensor(1.8607)\n",
      "\taccuracy:90.234375\n",
      "\tloss: 0.000251\n",
      "grad: tensor(0.0042, dtype=torch.float64)\n",
      "hess: tensor(0.1245)\n",
      "grad: tensor(0.0352, dtype=torch.float64)\n",
      "hess: tensor(1.2685)\n",
      "grad: tensor(0.7994, dtype=torch.float64)\n",
      "hess: tensor(23.4442)\n",
      "grad: tensor(1.1545, dtype=torch.float64)\n",
      "hess: tensor(29.0213)\n",
      "grad: tensor(3.2148, dtype=torch.float64)\n",
      "hess: tensor(69.3204)\n",
      "grad: tensor(2.3322, dtype=torch.float64)\n",
      "hess: tensor(47.7333)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e181153d61748bf9ad334f87e7bf152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor(5.6885e-08, dtype=torch.float64)\n",
      "hess: tensor(2.9817e-06)\n",
      "\taccuracy:91.796875\n",
      "\tloss: 0.000000\n",
      "grad: tensor(5.0280, dtype=torch.float64)\n",
      "hess: tensor(108.1678)\n",
      "grad: tensor(0.0362, dtype=torch.float64)\n",
      "hess: tensor(1.2769)\n",
      "grad: tensor(0.0021, dtype=torch.float64)\n",
      "hess: tensor(0.0842)\n",
      "grad: tensor(0.0025, dtype=torch.float64)\n",
      "hess: tensor(0.1063)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 397>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    393\u001b[0m         grad_list\u001b[38;5;241m.\u001b[39mappend(train_model\u001b[38;5;241m.\u001b[39mgrads_norms)\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grad_list, hess_norm_list\n\u001b[0;32m--> 397\u001b[0m grad_list, hess_norm_list \u001b[38;5;241m=\u001b[39m \u001b[43mexp_get_lp_sm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m              \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mg_weight\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mg_times\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m              \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mg_epochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresult_root_dir\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m              \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresult_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq_reduce_by\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfreq_reduce_by\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m              \u001b[49m\u001b[43mfreq_reduce_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfreq_reduce_after\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mexp_get_lp_sm\u001b[0;34m(train_data_all, test_data_all, op_features, weight, times, epochs, root_dir, path, clear_file, freq_reduce_by, freq_reduce_after)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    384\u001b[0m train_model \u001b[38;5;241m=\u001b[39m Train_nn(\u001b[38;5;241m784\u001b[39m, weight, op_features, lr\u001b[38;5;241m=\u001b[39m details[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha_0\u001b[39m\u001b[38;5;124m'\u001b[39m], decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 385\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_grads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_hessian\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbook_keep_freq\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mstore_gen_err\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_pt_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq_reduce_by\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfreq_reduce_by\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq_reduce_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreq_reduce_after\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(grad_file_path,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    388\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(grad) \u001b[38;5;28;01mfor\u001b[39;00m grad \u001b[38;5;129;01min\u001b[39;00m train_model\u001b[38;5;241m.\u001b[39mgrads_norms]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mTrain_nn.fit\u001b[0;34m(self, train_loader, test_loader, epochs, store_grads, store_hessian, store_gen_err, store_weights, store_pt_loss, store_freq, freq_reduce_by, freq_reduce_after)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminate_training \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (X, y) \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(train_loader), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader), unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch\u001b[38;5;241m>\u001b[39mdetails[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_step_limit\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    284\u001b[0m         terminate_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tqdm/notebook.py:258\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 258\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1359\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1359\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1362\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1325\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1325\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1326\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1327\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1163\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1161\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1167\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1168\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/multiprocessing/queues.py:122\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rlock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# unserialize the data after having released the lock\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ForkingPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py:329\u001b[0m, in \u001b[0;36mrebuild_typed_storage\u001b[0;34m(storage, dtype)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrebuild_typed_storage\u001b[39m(storage, dtype):\n\u001b[0;32m--> 329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_TypedStorage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrap_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/storage.py:387\u001b[0m, in \u001b[0;36m_TypedStorage.__init__\u001b[0;34m(self, device, dtype, wrap_storage, *args)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    384\u001b[0m         arg_error_msg \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    385\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mArgument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be specified\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m(dtype, torch\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    389\u001b[0m         arg_error_msg \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    390\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mArgument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be torch.dtype, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(dtype)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd.functional import hessian\n",
    "import torch.utils.data as data_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from tqdm.notebook import tqdm \n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import seaborn as sns\n",
    "import os\n",
    "import copy\n",
    "from torch.nn.utils import _stateless\n",
    "\n",
    "batch_size = 1\n",
    "num_workers = 1\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "# device\n",
    "\n",
    "details = {}\n",
    "details['use_db'] = 'mnist'\n",
    "details['result_root_dir']='results/t0/'\n",
    "details['result_path']='try1_t8_w25'\n",
    "details['g_weight'] = [25]\n",
    "# details['ratio'] = 15\n",
    "details['book_keep_freq'] = 20\n",
    "details['g_times'] = 8\n",
    "details['g_epochs'] = 10000\n",
    "details['alpha_0']= 0.003\n",
    "details['freq_reduce_by'] = 20\n",
    "details['freq_reduce_after'] = 100\n",
    "\n",
    "details['training_step_limit'] = 2000000 ## this is to train for max updates per epochs\n",
    "details['stop_hess_computation'] = 1000000 ## Stop computing hessian after calculated these many times\n",
    "\n",
    "\n",
    "print(f'selected weight:{details[\"g_weight\"]}')\n",
    "\n",
    "with open(details['result_root_dir']+'details_'+details['result_path']+'.txt', 'w+') as f:\n",
    "    for key, val in details.items():\n",
    "        content = key + ' : '+str(val) + '\\n'\n",
    "        f.write(content)\n",
    "        \n",
    "torch.manual_seed(3407)\n",
    "np.random.seed(3407)\n",
    "torch.cuda.manual_seed_all(3407)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "# os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":4096:2\"\n",
    "\n",
    "train_data_all = torchvision.datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "test_data_all = torchvision.datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "# print(f'train data:{train_data}')\n",
    "# print(f'test data:{test_data}')\n",
    "\n",
    "def get_random_subset(train_data_all, test_data_all):    \n",
    "    # train_indices = torch.arange(20000)\n",
    "    test_indices = torch.arange(256)\n",
    "    train_indices = torch.randint(60000-1, (2000,))\n",
    "    # print(f'train indices:{train_indices[:10]}')\n",
    "    train_data = data_utils.Subset(train_data_all, train_indices)\n",
    "    test_data = data_utils.Subset(test_data_all, test_indices)\n",
    "    # print(f'train data:{train_data}')\n",
    "    # print(f'test data:{test_data}')\n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(0)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g,\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_data,\n",
    "        batch_size=256,\n",
    "        num_workers=num_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g,\n",
    "    )\n",
    "    print(f'train data size:{len(train_loader.dataset)}')\n",
    "    print(f'test data size:{len(test_loader.dataset)}')\n",
    "    # X_mat, y_mat = torch.Tensor(len(train_loader.dataset),784), torch.Tensor(len(train_loader.dataset)).long()\n",
    "    # for i, (data, label) in enumerate(train_loader):\n",
    "    #     X_mat[i] = data.flatten()\n",
    "    #     y_mat[i] = label.flatten()\n",
    "    # print(f'X_mat shape:{X_mat.shape}, y_mat shape:{y_mat.shape}')\n",
    "    return train_loader, test_loader #, X_mat, y_mat\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_features, hidden_layers, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = len(hidden_layers) + 1\n",
    "        self.total_params_len = 0\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        prev_weight = input_features\n",
    "        \n",
    "        for i, weight in enumerate(hidden_layers):\n",
    "            self.fc_layers.append(nn.Linear(prev_weight, weight))\n",
    "            self.total_params_len += prev_weight*weight + weight\n",
    "            prev_weight = weight\n",
    "        \n",
    "        self.fc_last = nn.Linear(hidden_layers[-1], output_size)\n",
    "        self.total_params_len += hidden_layers[-1]*output_size + output_size\n",
    "        \n",
    "        ### Others required params\n",
    "        self.param_list = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        # print('x shape in forward',x.shape)\n",
    "        for fc_layer in self.fc_layers:\n",
    "            x = F.relu(fc_layer(x))\n",
    "        x = self.fc_last(x)\n",
    "        return x\n",
    "    \n",
    "    def fit(self, X, Y, X_val, Y_val, epochs, batch_size=1, **kwargs):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.SGD(self.parameters())\n",
    "        \n",
    "\n",
    "class Train_nn:\n",
    "    \n",
    "    def __init__(self, input_features, hidden_layers, output_size, lr, decay=True):\n",
    "        self.model = Net(input_features, hidden_layers=hidden_layers, output_size=output_size)\n",
    "        self.model.to(device)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        if decay:\n",
    "            lr_lambda = lambda it: 1/(it+1)\n",
    "        else:\n",
    "            lr_lambda = lambda it: 1\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda= lr_lambda)\n",
    "        \n",
    "    def get_loss(self, X, y, params=None):\n",
    "        # if params is not None:\n",
    "        assert False, \"Model not initialized with given params\"\n",
    "        op = self.model(X)\n",
    "        loss = self.loss_fn(pred, y)\n",
    "        return loss\n",
    "    \n",
    "    def get_gradient(self):\n",
    "        params = (self.model.parameters())\n",
    "        grad_norm_sq = torch.tensor(0, dtype=float).to(device)\n",
    "        # print('grad Norm init:', grad_norm_sq)\n",
    "        for param in self.model.parameters():\n",
    "            temp = param.grad.data.pow(2).sum()\n",
    "            # print(f'param grad norm \\n\\tsum:{temp.data}')#,\\n\\tshape:{param.shape}')\n",
    "            grad_norm_sq += temp\n",
    "            \n",
    "        return grad_norm_sq.sqrt().cpu()\n",
    "    \n",
    "    def get_gradientv2(self, X, y):\n",
    "        names = list(n for n, _ in self.model.named_parameters())\n",
    "        def loss_fun_grad(*params):\n",
    "            out: torch.Tensor = _stateless.functional_call(self.model, {n: p for n, p in zip(names, params)}, X)\n",
    "            local_loss = self.loss_fn(out, y)\n",
    "            return local_loss\n",
    "        grad_mat = torch.autograd.grad(loss_fun_grad, tuple(self.model.parameters()))\n",
    "        # print(f'len of hess mat:{len(hess_mat)}')\n",
    "        # print(f'hess_mat[0] shape:{len(hess_mat[0])}')\n",
    "        # print(f'hess_mat[0][0] shape:{hess_mat[0][0].shape}')\n",
    "        grad_norm = torch.tensor(0.).to(device)\n",
    "        for i in range(len(grad_mat)):\n",
    "            for j in range(len(grad_mat[0])):\n",
    "                grad_norm+= grad_mat[i][j].pow(2).sum()\n",
    "        grad_norm = grad_norm.sqrt()\n",
    "        # print(f'v2 hess norm{hess_norm}')\n",
    "        return grad_norm.cpu()\n",
    "    \n",
    "    def try_operator_norm(self, hess_mat):\n",
    "        for i in len(hess_mat):\n",
    "            for j in len(hess_mat[0]):\n",
    "                torch.unsqueeze(hess_mat[i][i],0)\n",
    "        hess_tensor_dim = list(hess_mat[0][0].shape)\n",
    "        hess_tensor_dim += [n*2,n*2]\n",
    "        hess_mat_np = np.zeros(shape=hess_tensor_dim)\n",
    "        hess_tensor = torch.tensor(hess_mat_np)\n",
    "        torch.cat(hess_mat, out=hess_tensor)\n",
    "        \n",
    "        hess_mat.reshpe(n*2,n*2)\n",
    "        hess_norm = torch.linalg.norm(hess_mat, 2)\n",
    "        assert False, \"Not working\"\n",
    "    \n",
    "    def get_hessian(self, X, y):\n",
    "        prev_params = copy.deepcopy(list(self.model.parameters()))\n",
    "        n = self.model.layers\n",
    "        def local_model(*params):\n",
    "            # print(f'len of params:{len(params)}')\n",
    "            # print(f'shape of params[0]:{params[0].shape}')\n",
    "            # with torch.no_grad():\n",
    "            #initialize model with given params\n",
    "            i = 0\n",
    "            for i, param in enumerate(self.model.parameters()):\n",
    "                param.data = params[i]\n",
    "            pred = self.model(X)\n",
    "            loss = self.loss_fn(pred, y)\n",
    "            # print(f'loss type:{type(loss)}')\n",
    "            return loss\n",
    "        p =list(self.model.parameters())\n",
    "        hess_mat = hessian(local_model, tuple(p))\n",
    "        hess_norm = torch.tensor(0.).to(device)\n",
    "        for i in range(len(hess_mat)):\n",
    "            for j in range(len(hess_mat[0])):\n",
    "                hess_norm+= hess_mat[i][j].pow(2).sum()\n",
    "        \n",
    "        # print(f'Hess mat len:{len(hess_mat)}')\n",
    "        # print(f'Hess mat[0] len:{len(hess_mat[0])}')\n",
    "        # print(f'Hess mat[0][0] shape:{hess_mat[0][0].shape}')\n",
    "        \n",
    "        hess_norm = hess_norm.sqrt()\n",
    "        # print(f'hess norm:{hess_norm}')\n",
    "        \n",
    "        # Reinitialize the original params to model\n",
    "        for i, param in enumerate(self.model.parameters()):\n",
    "                param.data = prev_params[i]\n",
    "        \n",
    "        return hess_norm\n",
    "    \n",
    "    def get_hessianv2(self, X,y):\n",
    "        names = list(n for n, _ in self.model.named_parameters())\n",
    "        def loss_fun_hess(*params):\n",
    "            out: torch.Tensor = _stateless.functional_call(self.model, {n: p for n, p in zip(names, params)}, X)\n",
    "            local_loss = self.loss_fn(out, y)\n",
    "            return local_loss\n",
    "        hess_mat = hessian(loss_fun_hess, tuple(self.model.parameters()))\n",
    "        # print(f'len of hess mat:{len(hess_mat)}')\n",
    "        # print(f'hess_mat[0] shape:{len(hess_mat[0])}')\n",
    "        # print(f'hess_mat[0][0] shape:{hess_mat[0][0].shape}')\n",
    "        hess_norm = torch.tensor(0.).to(device)\n",
    "        for i in range(len(hess_mat)):\n",
    "            for j in range(len(hess_mat[0])):\n",
    "                hess_norm+= hess_mat[i][j].pow(2).sum()\n",
    "        hess_norm = hess_norm.sqrt()\n",
    "        # print(f'v2 hess norm{hess_norm}')\n",
    "        return hess_norm.cpu()\n",
    "        \n",
    "    def fit(self, train_loader, test_loader, epochs, store_grads=False, store_hessian=False, store_gen_err=False, store_weights=False, store_pt_loss=True, store_freq = 20, freq_reduce_by=None, freq_reduce_after=None):\n",
    "        \n",
    "        ## For Book keeping results ##\n",
    "        self.grads_norms = []\n",
    "        self.grads_normsv2 = []\n",
    "        self.param_list = []\n",
    "        self.hess_norms = []\n",
    "        self.gen_err = []\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        self.point_loss = []\n",
    "        ## Initializing values ##\n",
    "        terminate_training = False\n",
    "        store_count = 0\n",
    "        \n",
    "        for epoch in tqdm(range(epochs), total=epochs, unit=\"epoch\", disable=True):\n",
    "            if terminate_training == True:\n",
    "                break\n",
    "            for batch, (X, y) in tqdm(enumerate(train_loader), total=len(train_loader), unit='batch'):\n",
    "                if batch>details['training_step_limit']:\n",
    "                    terminate_training = True\n",
    "                    break\n",
    "                \n",
    "                X, y =X.to(device), y.to(device)\n",
    "                pred = self.model(X)\n",
    "                loss = self.loss_fn(pred, y)\n",
    "\n",
    "                # Backpropagation\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                ## Saving point loss\n",
    "                if store_pt_loss and (batch%store_freq==0):\n",
    "                    self.point_loss.append(loss.item())\n",
    "                    \n",
    "                ## Saving the weights\n",
    "                if store_weights and (batch%store_freq==0):\n",
    "                    current_params = tuple(self.model.parameters())\n",
    "                    self.param_list.append(current_params)\n",
    "                \n",
    "                ## computing and saving the gradient\n",
    "                if store_grads and (batch% store_freq == 0):\n",
    "                    # store_count += 1\n",
    "                    # # print(f'\\tstore_freq:{store_freq}, batch:{batch}')\n",
    "                    # if store_count%freq_reduce_after==0:\n",
    "                    #     store_freq += freq_reduce_by\n",
    "                    #     # print(f'store freq:{store_freq}, batch:{batch}')\n",
    "                    grad_norm_per_update = self.get_gradient()\n",
    "                    print('grad:', grad_norm_per_update)\n",
    "                    # print('\\tgrad norm:', grad_norm_per_update)\n",
    "                    self.grads_norms.append(grad_norm_per_update)\n",
    "                    # self.grads_normsv2.append(self.get_gradientv2(X,y))\n",
    "                ## computing and saving hessian\n",
    "                if store_hessian and (batch% store_freq==0):\n",
    "                    #assert False, \"Not implemented\"\n",
    "                    self.optimizer.zero_grad()\n",
    "                    hess_val = self.get_hessianv2(X,y)\n",
    "                    print('hess:',hess_val)\n",
    "                    self.hess_norms.append(hess_val)\n",
    "                    store_count += 1\n",
    "                    if store_count%freq_reduce_after==0:\n",
    "                        store_freq += freq_reduce_by\n",
    "                \n",
    "                ## computing and storing the generalization error\n",
    "                if store_gen_err and (batch% store_freq == 0):\n",
    "                    assert False, \"fix reducing freq to get it working and fastX, fasty\"\n",
    "                    train_loss, test_loss, point_loss=0, 0, 0\n",
    "                    with torch.no_grad():\n",
    "                        for sub_batch, (X_local,y_local) in enumerate(train_loader):\n",
    "                            # if sub_batch> batch: # only taking the encountered points to calculate train loss\n",
    "                            #     break\n",
    "                            X_local, y_local = X_local.to(device), y_local.to(device)\n",
    "                            pred_local = self.model(X_local)\n",
    "                            train_loss += self.loss_fn(pred_local, y_local).item()\n",
    "                    train_loss = train_loss/(batch+1)\n",
    "                    with torch.no_grad():\n",
    "                        for sub_batch, (X_local,y_local) in enumerate(test_loader):\n",
    "                            X_local, y_local = X_local.to(device), y_local.to(device)\n",
    "                            pred_local = self.model(X_local)\n",
    "                            test_loss += self.loss_fn(pred_local, y_local).item()\n",
    "                    test_batch_size = len(test_loader)\n",
    "                    # print(f\"Number of batches in test:{len(test_loader)}\")\n",
    "                    test_loss = test_loss/ len(test_loader)\n",
    "                    self.train_loss.append(train_loss)\n",
    "                    self.val_loss.append(test_loss)\n",
    "                \n",
    "                if batch % 1000 == 0:\n",
    "                    loss, current = loss.item(), batch * len(X)\n",
    "                    correct = 0\n",
    "                    test_loss = 0\n",
    "                    with torch.no_grad():\n",
    "                        for X, y in test_loader:\n",
    "                            X, y = X.to(device), y.to(device)\n",
    "                            pred = self.model(X)\n",
    "                            test_loss += self.loss_fn(pred, y).item()\n",
    "                            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "                    acc = 100*correct/len(test_loader.dataset)\n",
    "                    print(f\"\\taccuracy:{acc}\")#, at batch:{batch}\")\n",
    "                    print(f\"\\tloss: {loss:>7f}\")\n",
    "                \n",
    "                    # print(f'Learning rate:{self.scheduler.get_last_lr()}')\n",
    "                self.scheduler.step()\n",
    "            \n",
    "def exp_get_lp_sm(train_data_all, test_data_all, op_features, weight = 10, times = 8, epochs = 1, root_dir='', path=None, clear_file = True, freq_reduce_by=10, freq_reduce_after=100):\n",
    "    grad_list        = []\n",
    "    hess_norm_list   = []\n",
    "    if path is not None:\n",
    "        grad_file_path = root_dir+'grad_'+path\n",
    "        hess_file_path = root_dir+'hess_'+path\n",
    "        # gen_file_path = root_dir+'gen_'+path\n",
    "        if clear_file:\n",
    "            with open(grad_file_path, 'w+') as f:\n",
    "                f.write('')\n",
    "            with open(hess_file_path, 'w+') as f:\n",
    "                f.write('')\n",
    "    \n",
    "    train_loader, test_loader = get_random_subset(train_data_all, test_data_all)\n",
    "    for t in range(times):\n",
    "        print(f'Time:{t}')\n",
    "        train_model = Train_nn(784, weight, op_features, lr= details['alpha_0'], decay=False)\n",
    "        train_model.fit(train_loader, test_loader, epochs=epochs, store_grads=True, store_hessian=True, store_freq=details['book_keep_freq'],  store_gen_err=False, store_pt_loss=False, store_weights=False, freq_reduce_by = freq_reduce_by, freq_reduce_after=freq_reduce_after, )\n",
    "        \n",
    "        with open(grad_file_path,'a+') as f:\n",
    "            f.write(' '.join([str(grad) for grad in train_model.grads_norms]) + '\\n')\n",
    "        with open(hess_file_path,'a+') as f:\n",
    "            f.write(' '.join([str(hess) for hess in train_model.hess_norms]) + '\\n')\n",
    "        \n",
    "        hess_norm_list.append(train_model.hess_norms)\n",
    "        grad_list.append(train_model.grads_norms)\n",
    "         \n",
    "    return grad_list, hess_norm_list\n",
    "\n",
    "grad_list, hess_norm_list = exp_get_lp_sm(train_data_all, test_data_all, op_features=10, \n",
    "              weight=details['g_weight'], times=details['g_times'], \n",
    "              epochs=details['g_epochs'], root_dir=details['result_root_dir'], \n",
    "              path=details['result_path'], freq_reduce_by=details['freq_reduce_by'], \n",
    "              freq_reduce_after=details['freq_reduce_after'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d7d2e5-3dc2-455d-a521-99d7f16be538",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
