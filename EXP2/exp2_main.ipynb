{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29480076-eac2-411b-b0a9-f571071b3eda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected weight:10\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81b4ed2f84c746f2a01c284f0b18fb9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26421880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "956989c8591e4828b9fb082e81706ed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d9fcad0b56475dbb922052bb5c0d99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4422102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c0a90e957f4428eb47fe0cd2701a483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "train data size:20000\n",
      "test data size:1000\n",
      "X_mat shape:torch.Size([20000, 784]), y_mat shape:torch.Size([20000])\n",
      "Time:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c398e6f232624e6d813c9b342238d258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\taccuracy:11.4\n",
      "\tloss: 2.509024\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 380>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grad_list, hess_norm_list\n\u001b[1;32m    379\u001b[0m \u001b[38;5;66;03m# grad_list, hess_norm_list=[],[]\u001b[39;00m\n\u001b[0;32m--> 380\u001b[0m grad_list, hess_norm_list \u001b[38;5;241m=\u001b[39m \u001b[43mexp_get_lp_sm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m              \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mg_weight\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mg_times\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m              \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mg_epochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresult_root_dir\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m              \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresult_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq_reduce_by\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfreq_reduce_by\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m              \u001b[49m\u001b[43mfreq_reduce_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfreq_reduce_after\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mexp_get_lp_sm\u001b[0;34m(train_data_all, test_data_all, op_features, weight, times, epochs, root_dir, path, clear_file, freq_reduce_by, freq_reduce_after)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    365\u001b[0m train_model \u001b[38;5;241m=\u001b[39m Train_nn(\u001b[38;5;241m784\u001b[39m, [weight], op_features, lr\u001b[38;5;241m=\u001b[39m details[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha_0\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 366\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_grads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_hessian\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbook_keep_freq\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mstore_gen_err\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_pt_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq_reduce_by\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfreq_reduce_by\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq_reduce_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreq_reduce_after\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfast_X_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfast_X_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfast_y_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfast_y_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(grad_file_path,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    369\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(grad) \u001b[38;5;28;01mfor\u001b[39;00m grad \u001b[38;5;129;01min\u001b[39;00m train_model\u001b[38;5;241m.\u001b[39mgrads_norms]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mTrain_nn.fit\u001b[0;34m(self, train_loader, test_loader, epochs, store_grads, store_hessian, store_gen_err, store_weights, store_pt_loss, store_freq, freq_reduce_by, freq_reduce_after, fast_X_train, fast_y_train)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m store_hessian \u001b[38;5;129;01mand\u001b[39;00m (batch\u001b[38;5;241m%\u001b[39m store_freq\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 288\u001b[0m     hess_norms_per_update \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_hessianv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;66;03m# print(f'\\thess norm:{hess_norms_per_update}')\u001b[39;00m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhess_norms\u001b[38;5;241m.\u001b[39mappend(hess_norms_per_update)\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mTrain_nn.get_hessianv2\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    220\u001b[0m     local_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(out, y)\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m local_loss\n\u001b[0;32m--> 222\u001b[0m hess_mat \u001b[38;5;241m=\u001b[39m \u001b[43mhessian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_fun_hess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m hess_norm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(hess_mat)):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/functional.py:808\u001b[0m, in \u001b[0;36mhessian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, outer_jacobian_strategy)\u001b[0m\n\u001b[1;32m    805\u001b[0m     _check_requires_grad(jac, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacobian\u001b[39m\u001b[38;5;124m\"\u001b[39m, strict\u001b[38;5;241m=\u001b[39mstrict)\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jac\n\u001b[0;32m--> 808\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mjacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjac_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvectorize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m               \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mouter_jacobian_strategy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _tuple_postprocess(res, (is_inputs_tuple, is_inputs_tuple))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/functional.py:670\u001b[0m, in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[1;32m    668\u001b[0m jac_i: Tuple[List[torch\u001b[38;5;241m.\u001b[39mTensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m([] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs)))  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(out\u001b[38;5;241m.\u001b[39mnelement()):\n\u001b[0;32m--> 670\u001b[0m     vj \u001b[38;5;241m=\u001b[39m \u001b[43m_autograd_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m el_idx, (jac_i_el, vj_el, inp_el) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(jac_i, vj, inputs)):\n\u001b[1;32m    674\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m vj_el \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/functional.py:159\u001b[0m, in \u001b[0;36m_autograd_grad\u001b[0;34m(outputs, inputs, grad_outputs, create_graph, retain_graph, is_grads_batched)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m,) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_grad_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_grads_batched\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:261\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    256\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly_inputs argument is deprecated and is ignored now \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    257\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(defaults to True). To accumulate gradient for other \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    258\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparts of the graph, please use torch.autograd.backward.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    260\u001b[0m grad_outputs_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_outputs, \u001b[38;5;28mlen\u001b[39m(t_outputs))\n\u001b[0;32m--> 261\u001b[0m grad_outputs_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_grads_batched\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:68\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd.functional import hessian\n",
    "import torch.utils.data as data_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from tqdm.notebook import tqdm \n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import seaborn as sns\n",
    "import os\n",
    "import copy\n",
    "from torch.nn.utils import _stateless\n",
    "\n",
    "batch_size = 1\n",
    "num_workers = 1\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "# device\n",
    "\n",
    "details = {}\n",
    "details['use_db'] = 'fashion_mnist'\n",
    "details['result_root_dir']='results/t0/'\n",
    "details['result_path']='try2f_t10_r.4'\n",
    "details['ratio'] = 0.4\n",
    "details['book_keep_freq'] = 20\n",
    "details['g_times'] = 8\n",
    "details['g_epochs'] = 1\n",
    "details['alpha_0']= 0.001\n",
    "details['freq_reduce_by'] = 20\n",
    "details['freq_reduce_after'] = 100\n",
    "\n",
    "details['training_step_limit'] = 100000 ## this is to train for max updates per epochs\n",
    "details['stop_hess_computation'] = 20000 ## Stop computing hessian after calculated these many times\n",
    "\n",
    "details['g_weight'] = int(details['ratio']*20000/(784+10))\n",
    "print(f'selected weight:{details[\"g_weight\"]}')\n",
    "\n",
    "with open(details['result_root_dir']+'details_'+details['result_path']+'.txt', 'w+') as f:\n",
    "    for key, val in details.items():\n",
    "        content = key + ' : '+str(val) + '\\n'\n",
    "        f.write(content)\n",
    "        \n",
    "torch.manual_seed(3407)\n",
    "np.random.seed(3407)\n",
    "torch.cuda.manual_seed_all(3407)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "# os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":4096:2\"\n",
    "\n",
    "train_data_all = torchvision.datasets.FashionMNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "test_data_all = torchvision.datasets.FashionMNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "# print(f'train data:{train_data}')\n",
    "# print(f'test data:{test_data}')\n",
    "def get_random_subset(train_data_all, test_data_all):    \n",
    "    # train_indices = torch.arange(20000)\n",
    "    test_indices = torch.arange(1000)\n",
    "    train_indices = torch.randint(60000-1, (20000,))\n",
    "    # print(f'train indices:{train_indices[:10]}')\n",
    "    train_data = data_utils.Subset(train_data_all, train_indices)\n",
    "    test_data = data_utils.Subset(test_data_all, test_indices)\n",
    "    # print(f'train data:{train_data}')\n",
    "    # print(f'test data:{test_data}')\n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(0)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g,\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_data,\n",
    "        batch_size=256,\n",
    "        num_workers=num_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g,\n",
    "    )\n",
    "    print(f'train data size:{len(train_loader.dataset)}')\n",
    "    print(f'test data size:{len(test_loader.dataset)}')\n",
    "    X_mat, y_mat = torch.Tensor(len(train_loader.dataset),784), torch.Tensor(len(train_loader.dataset)).long()\n",
    "    for i, (data, label) in enumerate(train_loader):\n",
    "        X_mat[i] = data.flatten()\n",
    "        y_mat[i] = label.flatten()\n",
    "    print(f'X_mat shape:{X_mat.shape}, y_mat shape:{y_mat.shape}')\n",
    "    return train_loader, test_loader, X_mat, y_mat\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_features, hidden_layers, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = len(hidden_layers) + 1\n",
    "        self.total_params_len = 0\n",
    "        self.fc1 = nn.Linear(input_features, hidden_layers[0])\n",
    "        self.total_params_len += input_features*hidden_layers[0] + hidden_layers[0]\n",
    "        self.fc2 = nn.Linear(hidden_layers[0], output_size)\n",
    "        self.total_params_len += hidden_layers[0]*output_size + output_size\n",
    "        \n",
    "        ### Others required params\n",
    "        self.param_list = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        # print('x shape in forward',x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def fit(self, X, Y, X_val, Y_val, epochs, batch_size=1, **kwargs):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.SGD(self.parameters())\n",
    "        \n",
    "\n",
    "class Train_nn:\n",
    "    \n",
    "    def __init__(self, input_features, hidden_layers, output_size, lr):\n",
    "        self.model = Net(input_features, hidden_layers=hidden_layers, output_size=output_size)\n",
    "        self.model.to(device)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda= lambda it: 1/(it+1))\n",
    "        \n",
    "    def get_loss(self, X, y, params=None):\n",
    "        # if params is not None:\n",
    "        assert False, \"Model not initialized with given params\"\n",
    "        op = self.model(X)\n",
    "        loss = self.loss_fn(pred, y)\n",
    "        return loss\n",
    "    \n",
    "    def get_gradient(self):\n",
    "        params = (self.model.parameters())\n",
    "        grad_norm_sq = torch.tensor(0, dtype=float).to(device)\n",
    "        # print('grad Norm init:', grad_norm_sq)\n",
    "        for param in self.model.parameters():\n",
    "            temp = param.grad.data.pow(2).sum()\n",
    "            # print(f'param grad norm \\n\\tsum:{temp.data}')#,\\n\\tshape:{param.shape}')\n",
    "            grad_norm_sq += temp\n",
    "        grad_norm = grad_norm_sq.sqrt()\n",
    "        return grad_norm.cpu()\n",
    "    \n",
    "    def try_operator_norm(self, hess_mat):\n",
    "        for i in len(hess_mat):\n",
    "            for j in len(hess_mat[0]):\n",
    "                torch.unsqueeze(hess_mat[i][i],0)\n",
    "        hess_tensor_dim = list(hess_mat[0][0].shape)\n",
    "        hess_tensor_dim += [n*2,n*2]\n",
    "        hess_mat_np = np.zeros(shape=hess_tensor_dim)\n",
    "        hess_tensor = torch.tensor(hess_mat_np)\n",
    "        torch.cat(hess_mat, out=hess_tensor)\n",
    "        \n",
    "        hess_mat.reshpe(n*2,n*2)\n",
    "        hess_norm = torch.linalg.norm(hess_mat, 2)\n",
    "        assert False, \"Not working\"\n",
    "    \n",
    "    def get_hessian(self, X, y):\n",
    "        prev_params = copy.deepcopy(list(self.model.parameters()))\n",
    "        n = self.model.layers\n",
    "        def local_model(*params):\n",
    "            # print(f'len of params:{len(params)}')\n",
    "            # print(f'shape of params[0]:{params[0].shape}')\n",
    "            # with torch.no_grad():\n",
    "            #initialize model with given params\n",
    "            i = 0\n",
    "            for i, param in enumerate(self.model.parameters()):\n",
    "                param.data = params[i]\n",
    "            pred = self.model(X)\n",
    "            loss = self.loss_fn(pred, y)\n",
    "            # print(f'loss type:{type(loss)}')\n",
    "            return loss\n",
    "        p =list(self.model.parameters())\n",
    "        hess_mat = hessian(local_model, tuple(p))\n",
    "        hess_norm = torch.tensor(0.).to(device)\n",
    "        for i in range(len(hess_mat)):\n",
    "            for j in range(len(hess_mat[0])):\n",
    "                hess_norm+= hess_mat[i][j].pow(2).sum()\n",
    "        \n",
    "        # print(f'Hess mat len:{len(hess_mat)}')\n",
    "        # print(f'Hess mat[0] len:{len(hess_mat[0])}')\n",
    "        # print(f'Hess mat[0][0] shape:{hess_mat[0][0].shape}')\n",
    "        \n",
    "        hess_norm = hess_norm.sqrt()\n",
    "        # print(f'hess norm:{hess_norm}')\n",
    "        \n",
    "        # Reinitialize the original params to model\n",
    "        for i, param in enumerate(self.model.parameters()):\n",
    "                param.data = prev_params[i]\n",
    "        \n",
    "        return hess_norm\n",
    "    \n",
    "    def get_hessianv2(self, X,y):\n",
    "        names = list(n for n, _ in self.model.named_parameters())\n",
    "        def loss_fun_hess(*params):\n",
    "            out: torch.Tensor = _stateless.functional_call(self.model, {n: p for n, p in zip(names, params)}, X)\n",
    "            local_loss = self.loss_fn(out, y)\n",
    "            return local_loss\n",
    "        hess_mat = hessian(loss_fun_hess, tuple(self.model.parameters()))\n",
    "        \n",
    "        hess_norm = torch.tensor(0.).to(device)\n",
    "        for i in range(len(hess_mat)):\n",
    "            for j in range(len(hess_mat[0])):\n",
    "                hess_norm+= hess_mat[i][j].pow(2).sum()\n",
    "        hess_norm = hess_norm.sqrt()\n",
    "        # print(f'v2 hess norm{hess_norm}')\n",
    "        return hess_norm.cpu()\n",
    "        \n",
    "    def fit(self, train_loader, test_loader, epochs, store_grads=False, store_hessian=False, store_gen_err=False, store_weights=False, store_pt_loss=True, store_freq = 20, freq_reduce_by=None, freq_reduce_after=None, fast_X_train=None, fast_y_train=None):\n",
    "        \n",
    "        ## For Book keeping results ##\n",
    "        self.grads_norms = []\n",
    "        self.param_list = []\n",
    "        self.hess_norms = []\n",
    "        self.gen_err = []\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        self.point_loss = []\n",
    "        ## Initializing values ##\n",
    "        terminate_training = False\n",
    "        store_count = 0\n",
    "        ## Moving to gpu\n",
    "        fast_X_train = fast_X_train.to(device)\n",
    "        fast_y_train = fast_y_train.to(device)\n",
    "        \n",
    "        \n",
    "        for epoch in tqdm(range(epochs), total=epochs, unit=\"epoch\", disable=True):\n",
    "            if terminate_training == True:\n",
    "                break\n",
    "            for batch, (X, y) in tqdm(enumerate(train_loader), total=len(train_loader), unit='batch'):\n",
    "                # if batch>300:\n",
    "                #     terminate_training = True\n",
    "                #     break\n",
    "                \n",
    "                X, y =X.to(device), y.to(device)\n",
    "                pred = self.model(X)\n",
    "                loss = self.loss_fn(pred, y)\n",
    "\n",
    "                # Backpropagation\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                ## Saving point loss\n",
    "                if store_pt_loss and (batch%store_freq==0):\n",
    "                    self.point_loss.append(loss.item())\n",
    "                    \n",
    "                ## Saving the weights\n",
    "                if store_weights and (batch%store_freq==0):\n",
    "                    current_params = list(self.model.parameters())\n",
    "                    self.param_list.append(current_params)\n",
    "                \n",
    "                ## computing and saving the gradient\n",
    "                if store_grads and (batch% store_freq == 0):\n",
    "                    # print(f'store freq:{store_freq}, batch:{batch}')\n",
    "                    grad_norm_per_update = self.get_gradient()\n",
    "                    # print('\\tgrad norm:', grad_norm_per_update)\n",
    "                    # print('appended:',grad_norm_per_update)\n",
    "                    # print(f'store gen err:{store_gen_err}')\n",
    "                    self.grads_norms.append(grad_norm_per_update)\n",
    "                    \n",
    "                ## computing and saving hessian\n",
    "                if store_hessian and (batch% store_freq==0):\n",
    "                    self.optimizer.zero_grad()\n",
    "                    hess_norms_per_update = self.get_hessianv2(X,y)\n",
    "                    # print(f'\\thess norm:{hess_norms_per_update}')\n",
    "                    self.hess_norms.append(hess_norms_per_update)\n",
    "                    #self.hess_norms.append(self.get_hessian(X,y))\n",
    "                    \n",
    "                ## computing and storing the generalization error\n",
    "                if store_gen_err and (batch% store_freq == 0):\n",
    "                    train_loss, test_loss=0, 0\n",
    "                    if (fast_y_train is None) or (fast_X_train is None):\n",
    "                        with torch.no_grad():\n",
    "                            for sub_batch, (X_local,y_local) in enumerate(train_loader):\n",
    "                                if epoch==0 and sub_batch> batch: # only taking the visited points to calculate train loss\n",
    "                                    break\n",
    "                                X_local, y_local = X_local.to(device), y_local.to(device)\n",
    "                                pred_local = self.model(X_local)\n",
    "                                train_loss += self.loss_fn(pred_local, y_local).item()\n",
    "                        train_loss = train_loss/(batch+1)\n",
    "                    else:\n",
    "                        # print('using fast train loss, epoch', epoch)\n",
    "                        with torch.no_grad():\n",
    "                            if epoch==0:\n",
    "                                pred_local = self.model(fast_X_train[:batch+1])\n",
    "                                train_loss = self.loss_fn(pred_local, fast_y_train[:batch+1]).item()\n",
    "                                # print(f'train_loss:{train_loss}')\n",
    "                            else:\n",
    "                                pred_local = self.model(fast_X_train)\n",
    "                                train_loss = self.loss_fn(pred_local, fast_y_train).item()\n",
    "                    with torch.no_grad():\n",
    "                        for sub_batch, (X_local,y_local) in enumerate(test_loader):\n",
    "                            X_local, y_local = X_local.to(device), y_local.to(device)\n",
    "                            pred_local = self.model(X_local)\n",
    "                            test_loss += self.loss_fn(pred_local, y_local).item()\n",
    "                    test_batch_size = len(test_loader)\n",
    "                    # print(f\"Number of batches in test:{len(test_loader)}\")\n",
    "                    test_loss = test_loss/ len(test_loader)\n",
    "                    self.train_loss.append(train_loss)\n",
    "                    self.val_loss.append(test_loss)\n",
    "                    self.gen_err.append(train_loss - test_loss)\n",
    "                    \n",
    "                    store_count += 1\n",
    "                    if store_count%freq_reduce_after==0:\n",
    "                        store_freq += freq_reduce_by\n",
    "                        \n",
    "                if batch % 10000 == 0:\n",
    "                    loss, current = loss.item(), batch * len(X)\n",
    "                    correct = 0\n",
    "                    test_loss = 0\n",
    "                    with torch.no_grad():\n",
    "                        for X, y in test_loader:\n",
    "                            X, y = X.to(device), y.to(device)\n",
    "                            pred = self.model(X)\n",
    "                            test_loss += self.loss_fn(pred, y).item()\n",
    "                            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "                    acc = 100*correct/len(test_loader.dataset)\n",
    "                    print(f\"\\taccuracy:{acc}\")#, at batch:{batch}\")\n",
    "                    print(f\"\\tloss: {loss:>7f}\")\n",
    "                    # print(f'Learning rate:{self.scheduler.get_last_lr()}')\n",
    "                self.scheduler.step()\n",
    "            \n",
    "def exp_get_lp_sm(train_data_all, test_data_all, op_features, weight = 10, times = 8, epochs = 1, root_dir='', path=None, clear_file = True, freq_reduce_by=10, freq_reduce_after=100):\n",
    "    grad_list        = []\n",
    "    hess_norm_list   = []\n",
    "    if path is not None:\n",
    "        grad_file_path = root_dir+'grad_'+path\n",
    "        hess_file_path = root_dir+'hess_'+path\n",
    "        gen_file_path = root_dir+'gen_'+path\n",
    "        if clear_file:\n",
    "            with open(grad_file_path, 'w+') as f:\n",
    "                f.write('')\n",
    "            with open(hess_file_path, 'w+') as f:\n",
    "                f.write('')\n",
    "            with open(gen_file_path, 'w+') as f:\n",
    "                f.write('')\n",
    "            \n",
    "    for t in range(times):\n",
    "        train_loader, test_loader, fast_X_train, fast_y_train = get_random_subset(train_data_all, test_data_all)\n",
    "        print(f'Time:{t}')\n",
    "        train_model = Train_nn(784, [weight], op_features, lr= details['alpha_0'])\n",
    "        train_model.fit(train_loader, test_loader, epochs=epochs, store_grads=True, store_hessian=True, store_freq=details['book_keep_freq'],  store_gen_err=True, store_pt_loss=False, store_weights=False, freq_reduce_by = freq_reduce_by, freq_reduce_after=freq_reduce_after, fast_X_train=fast_X_train, fast_y_train=fast_y_train)\n",
    "        \n",
    "        with open(grad_file_path,'a+') as f:\n",
    "            f.write(' '.join([str(grad) for grad in train_model.grads_norms]) + '\\n')\n",
    "        with open(hess_file_path,'a+') as f:\n",
    "            f.write(' '.join([str(hess) for hess in train_model.hess_norms]) + '\\n')\n",
    "        with open(gen_file_path,'a+') as f:\n",
    "            f.write(' '.join([str(gen_e) for gen_e in train_model.gen_err]) + '\\n')\n",
    "        \n",
    "        hess_norm_list.append(train_model.hess_norms)\n",
    "        grad_list.append(train_model.grads_norms)\n",
    "         \n",
    "    return grad_list, hess_norm_list\n",
    "# grad_list, hess_norm_list=[],[]\n",
    "grad_list, hess_norm_list = exp_get_lp_sm(train_data_all, test_data_all, op_features=10, \n",
    "              weight=details['g_weight'], times=details['g_times'], \n",
    "              epochs=details['g_epochs'], root_dir=details['result_root_dir'], \n",
    "              path=details['result_path'], freq_reduce_by=details['freq_reduce_by'], \n",
    "              freq_reduce_after=details['freq_reduce_after'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc04dde0-6333-4b93-8cfb-72ec43a2774a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c95bbfb-2d8a-4729-a717-5f00775e6bba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size:2000\n",
      "test data size:100\n",
      "X_mat shape:torch.Size([2000, 784]), y_mat shape:torch.Size([2000])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(3407)\n",
    "np.random.seed(3407)\n",
    "torch.cuda.manual_seed_all(3407)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "# os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":4096:2\"\n",
    "\n",
    "train_data_all = torchvision.datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "test_data_all = torchvision.datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "def get_random_subset(train_data_all, test_data_all):    \n",
    "    # train_indices = torch.arange(20000)\n",
    "    test_indices = torch.arange(100)\n",
    "    train_indices = torch.randint(60000-1, (2000,))\n",
    "    # print(f'train indices:{train_indices[:10]}')\n",
    "    train_data = data_utils.Subset(train_data_all, train_indices)\n",
    "    test_data = data_utils.Subset(test_data_all, test_indices)\n",
    "    # print(f'train data:{train_data}')\n",
    "    # print(f'test data:{test_data}')\n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(0)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g,\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_data,\n",
    "        batch_size=256,\n",
    "        num_workers=num_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g,\n",
    "    )\n",
    "    print(f'train data size:{len(train_loader.dataset)}')\n",
    "    print(f'test data size:{len(test_loader.dataset)}')\n",
    "    X_mat, y_mat = torch.Tensor(len(train_loader.dataset),784), torch.Tensor(len(train_loader.dataset)).long()\n",
    "    for i, (data, label) in enumerate(train_loader):\n",
    "        X_mat[i] = data.flatten()\n",
    "        y_mat[i] = label.flatten()\n",
    "    print(f'X_mat shape:{X_mat.shape}, y_mat shape:{y_mat.shape}')\n",
    "    return train_loader, test_loader, X_mat, y_mat\n",
    "\n",
    "\n",
    "    # fig, ax = plt.subplots(2)\n",
    "    # for i, (data, label) in enumerate(train_loader):\n",
    "    #     # count_train+=1\n",
    "    #     if i< 2:\n",
    "    #         ax[i].imshow(data.reshape(28,28))\n",
    "    #     else:\n",
    "    #         break\n",
    "            \n",
    "# for i in range(5):\n",
    "#     get_random_subset(train_data_all, test_data_all)\n",
    "train_loader_temp, test_loader_temp, X_train_temp, y_train_temp = get_random_subset(train_data_all, test_data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420ab366-7631-4bd1-8a57-cb362c966983",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b04c519-a142-4ec4-a46f-f9cd598e7919",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d119499e3c4134bd7393c64494a3ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appended: tensor(3.9513, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:1.5174301862716675\n",
      "\taccuracy:11.0\n",
      "\tloss: 2.261410\n",
      "appended: tensor(4.5444, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.1730642318725586\n",
      "appended: tensor(3.0902, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.171642303466797\n",
      "appended: tensor(4.1102, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.199967622756958\n",
      "appended: tensor(3.0810, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.190607786178589\n",
      "appended: tensor(3.2957, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.194854497909546\n",
      "appended: tensor(2.8664, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.1917645931243896\n",
      "appended: tensor(5.8451, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.1967413425445557\n",
      "appended: tensor(3.7885, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.1958534717559814\n",
      "appended: tensor(3.4580, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.1932926177978516\n",
      "appended: tensor(2.9110, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.1921844482421875\n",
      "appended: tensor(2.1767, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.1889326572418213\n",
      "appended: tensor(5.6841, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.183725118637085\n",
      "appended: tensor(4.4847, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.184039354324341\n",
      "appended: tensor(4.0879, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.18345308303833\n",
      "appended: tensor(2.7532, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.1820905208587646\n",
      "appended: tensor(2.6794, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.1825339794158936\n",
      "appended: tensor(5.1305, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.180907964706421\n",
      "appended: tensor(3.9976, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.181661367416382\n",
      "appended: tensor(2.1202, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.183849811553955\n",
      "appended: tensor(4.0946, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.1817781925201416\n",
      "appended: tensor(4.4505, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.1811459064483643\n",
      "appended: tensor(3.6912, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.1762425899505615\n",
      "appended: tensor(4.6748, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.1771957874298096\n",
      "appended: tensor(5.1686, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.176264524459839\n",
      "appended: tensor(5.2282, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.174833297729492\n",
      "appended: tensor(4.4822, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.173912286758423\n",
      "appended: tensor(2.0621, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.1728594303131104\n",
      "appended: tensor(3.1282, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.171527862548828\n",
      "appended: tensor(4.4450, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.172854423522949\n",
      "appended: tensor(3.8410, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.1747260093688965\n",
      "appended: tensor(4.8950, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.1743640899658203\n",
      "appended: tensor(3.3376, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.173509359359741\n",
      "appended: tensor(3.9360, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.1724915504455566\n",
      "appended: tensor(2.7352, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.1711056232452393\n",
      "appended: tensor(3.3889, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.1717591285705566\n",
      "appended: tensor(2.5531, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.1734769344329834\n",
      "appended: tensor(3.7443, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.1740782260894775\n",
      "appended: tensor(6.5635, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.1720213890075684\n",
      "appended: tensor(3.0751, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.1741943359375\n",
      "appended: tensor(4.0606, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.1729395389556885\n",
      "appended: tensor(3.1254, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.1715948581695557\n",
      "appended: tensor(4.8117, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.170912742614746\n",
      "appended: tensor(4.2710, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.1712470054626465\n",
      "appended: tensor(2.9182, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.1710360050201416\n",
      "appended: tensor(4.8731, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.1682467460632324\n",
      "appended: tensor(5.4881, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.1678993701934814\n",
      "appended: tensor(2.4462, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.1678545475006104\n",
      "appended: tensor(3.6272, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.1687724590301514\n",
      "appended: tensor(2.6412, dtype=torch.float64)\n",
      "store gen err:True\n",
      "using fast train loss, epoch 0\n",
      "train_loss:2.167975664138794\n"
     ]
    }
   ],
   "source": [
    "train_model = Train_nn(784, [12], 10, lr= 0.1)\n",
    "train_model.fit(train_loader_temp, test_loader_temp, epochs=1, store_grads=True, store_hessian=True, store_freq=details['book_keep_freq'],  store_gen_err=True, store_pt_loss=False, store_weights=False, freq_reduce_by = 40, freq_reduce_after=1, fast_X_train=X_train_temp, fast_y_train=y_train_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a489a5-1697-4389-ad67-d38a80dcf79c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d96dcc6-796d-4c4a-b0f9-3d53a5b569f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bd769876-da78-4e33-a2bd-ba551692f163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20000, 784]) torch.Size([20000])\n"
     ]
    }
   ],
   "source": [
    "print(X_train_temp.shape, y_train_temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1d8a1c8c-f32d-4a7a-a7c0-3a54b8a65639",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_net = Net( 784, [12], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "07982975-e059-4df6-8845-35dce940142c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 10])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_out=temp_net(X_train_temp[:100])\n",
    "temp_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6531a076-14d2-45cf-bead-0ca7269ecd0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3111, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.CrossEntropyLoss()(temp_out, y_train_temp[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff46550-0f4c-4785-8ec7-e11024bb1159",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randint(100, (10,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b5e7b05-f3f0-4316-adca-c74b225dc4d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "count_train = 0\n",
    "fig, ax = plt.subplots(5)\n",
    "for i, (data, label) in enumerate(train_loader):\n",
    "    # count_train+=1\n",
    "    if i< 5:\n",
    "        ax[i].imshow(data.reshape(28,28))\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9bb35ea-b8d1-4aa3-82db-14ba729f0f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "source": [
    "print(count_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d251083-8332-4e21-8ab6-d281dd9c4655",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
