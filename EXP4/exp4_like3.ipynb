{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02c26523-0346-40af-b863-bdbe6731df37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 0, 9, 8, 1, 6, 2, 5, 0, 4, 9, 9, 2, 8, 5, 7, 3, 1, 0, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([6, 5, 3, 0, 8, 4, 7, 2, 5, 6, 3, 3, 7, 0, 2, 1, 9, 8, 5, 9])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randint(10, (20,))\n",
    "print(x)\n",
    "x = np.random.permutation(10)[x]\n",
    "torch.tensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "053e42b0-2e3e-4aa5-9a33-0e910aec9afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected weight:25\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd.functional import hessian\n",
    "import torch.utils.data as data_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from tqdm.notebook import tqdm \n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import seaborn as sns\n",
    "import os\n",
    "import copy\n",
    "from torch.nn.utils import _stateless\n",
    "\n",
    "batch_size = 1\n",
    "num_workers = 1\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# device\n",
    "\n",
    "details = {}\n",
    "details['use_db'] = 'mnist'\n",
    "details['result_root_dir']='results/t0/'\n",
    "details['result_path']='try1_t10_r1'\n",
    "details['ratio'] = 1\n",
    "details['book_keep_freq'] = 20\n",
    "details['g_times'] = 8\n",
    "details['g_epochs'] = 1\n",
    "details['alpha_0']= 0.001\n",
    "details['freq_reduce_by'] = 20\n",
    "details['freq_reduce_after'] = 100\n",
    "\n",
    "details['training_step_limit'] = 100000 ## this is to train for max updates per epochs\n",
    "details['stop_hess_computation'] = 20000 ## Stop computing hessian after calculated these many times\n",
    "\n",
    "details['g_weight'] = int(details['ratio']*20000/(784+10))\n",
    "print(f'selected weight:{details[\"g_weight\"]}')\n",
    "\n",
    "with open(details['result_root_dir']+'details_'+details['result_path']+'.txt', 'w+') as f:\n",
    "    for key, val in details.items():\n",
    "        content = key + ' : '+str(val) + '\\n'\n",
    "        f.write(content)\n",
    "        \n",
    "torch.manual_seed(3407)\n",
    "np.random.seed(3407)\n",
    "torch.cuda.manual_seed_all(3407)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "# os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":4096:2\"\n",
    "\n",
    "train_data_all = torchvision.datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "test_data_all = torchvision.datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "# print(f'train data:{train_data}')\n",
    "# print(f'test data:{test_data}')\n",
    "def get_random_subset(train_data_all, test_data_all, corrupt_label=False):    \n",
    "    # train_indices = torch.arange(20000)\n",
    "    test_indices = torch.arange(500)\n",
    "    train_indices = torch.randint(60000-1, (2000,))\n",
    "    # print(f'train indices:{train_indices[:10]}')\n",
    "    train_data = data_utils.Subset(train_data_all, train_indices)\n",
    "    test_data = data_utils.Subset(test_data_all, test_indices)\n",
    "    # print(f'train data:{train_data}')\n",
    "    # print(f'test data:{test_data}')\n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(0)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g,\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_data,\n",
    "        batch_size=256,\n",
    "        num_workers=num_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g,\n",
    "    )\n",
    "    print(f'train data size:{len(train_loader.dataset)}')\n",
    "    print(f'test data size:{len(test_loader.dataset)}')\n",
    "    X_mat, y_mat = torch.Tensor(len(train_loader.dataset),784), torch.Tensor(len(train_loader.dataset)).long()\n",
    "    for i, (data, label) in enumerate(train_loader):\n",
    "        X_mat[i] = data.flatten()\n",
    "        y_mat[i] = label.flatten()\n",
    "    if corrupt_label:\n",
    "        y_mat = torch.tensor(np.random.permutation(10)[y_mat])\n",
    "    print(f'X_mat shape:{X_mat.shape}, y_mat shape:{y_mat.shape}')\n",
    "    return train_loader, test_loader, X_mat, y_mat\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_features, hidden_layers, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = len(hidden_layers) + 1\n",
    "        self.total_params_len = 0\n",
    "        self.fc1 = nn.Linear(input_features, hidden_layers[0])\n",
    "        self.total_params_len += input_features*hidden_layers[0] + hidden_layers[0]\n",
    "        self.fc2 = nn.Linear(hidden_layers[0], output_size)\n",
    "        self.total_params_len += hidden_layers[0]*output_size + output_size\n",
    "        \n",
    "        ### Others required params\n",
    "        self.param_list = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        # print('x shape in forward',x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "class Train_nn:\n",
    "    \n",
    "    def __init__(self, input_features, hidden_layers, output_size, lr, dont_decay=False, l2_reg=0.1):\n",
    "        self.model = Net(input_features, hidden_layers=hidden_layers, output_size=output_size)\n",
    "        self.model.to(device)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=lr, weight_decay=l2_reg)\n",
    "        if dont_decay:\n",
    "            lambda_lr = lambda it: 1\n",
    "        else:\n",
    "            lambda_lr = lambda it: 1/(it+1)\n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, lambda_lr)\n",
    "        \n",
    "    def get_loss(self, X, y, params=None):\n",
    "        # if params is not None:\n",
    "        assert False, \"Model not initialized with given params\"\n",
    "        op = self.model(X)\n",
    "        loss = self.loss_fn(pred, y)\n",
    "        return loss\n",
    "    \n",
    "    def get_gradient(self):\n",
    "        params = (self.model.parameters())\n",
    "        grad_norm_sq = torch.tensor(0, dtype=float).to(device)\n",
    "        # print('grad Norm init:', grad_norm_sq)\n",
    "        for param in self.model.parameters():\n",
    "            temp = param.grad.data.pow(2).sum()\n",
    "            # print(f'param grad norm \\n\\tsum:{temp.data}')#,\\n\\tshape:{param.shape}')\n",
    "            grad_norm_sq += temp\n",
    "        grad_norm = grad_norm_sq.sqrt()\n",
    "        return grad_norm.cpu()\n",
    "    \n",
    "    def try_operator_norm(self, hess_mat):\n",
    "        for i in len(hess_mat):\n",
    "            for j in len(hess_mat[0]):\n",
    "                torch.unsqueeze(hess_mat[i][i],0)\n",
    "        hess_tensor_dim = list(hess_mat[0][0].shape)\n",
    "        hess_tensor_dim += [n*2,n*2]\n",
    "        hess_mat_np = np.zeros(shape=hess_tensor_dim)\n",
    "        hess_tensor = torch.tensor(hess_mat_np)\n",
    "        torch.cat(hess_mat, out=hess_tensor)\n",
    "        \n",
    "        hess_mat.reshpe(n*2,n*2)\n",
    "        hess_norm = torch.linalg.norm(hess_mat, 2)\n",
    "        assert False, \"Not working\"\n",
    "    \n",
    "    def get_hessian(self, X, y):\n",
    "        prev_params = copy.deepcopy(list(self.model.parameters()))\n",
    "        n = self.model.layers\n",
    "        def local_model(*params):\n",
    "            # print(f'len of params:{len(params)}')\n",
    "            # print(f'shape of params[0]:{params[0].shape}')\n",
    "            # with torch.no_grad():\n",
    "            #initialize model with given params\n",
    "            i = 0\n",
    "            for i, param in enumerate(self.model.parameters()):\n",
    "                param.data = params[i]\n",
    "            pred = self.model(X)\n",
    "            loss = self.loss_fn(pred, y)\n",
    "            # print(f'loss type:{type(loss)}')\n",
    "            return loss\n",
    "        p =list(self.model.parameters())\n",
    "        hess_mat = hessian(local_model, tuple(p))\n",
    "        hess_norm = torch.tensor(0.).to(device)\n",
    "        for i in range(len(hess_mat)):\n",
    "            for j in range(len(hess_mat[0])):\n",
    "                hess_norm+= hess_mat[i][j].pow(2).sum()\n",
    "        \n",
    "        # print(f'Hess mat len:{len(hess_mat)}')\n",
    "        # print(f'Hess mat[0] len:{len(hess_mat[0])}')\n",
    "        # print(f'Hess mat[0][0] shape:{hess_mat[0][0].shape}')\n",
    "        \n",
    "        hess_norm = hess_norm.sqrt()\n",
    "        # print(f'hess norm:{hess_norm}')\n",
    "        \n",
    "        # Reinitialize the original params to model\n",
    "        for i, param in enumerate(self.model.parameters()):\n",
    "                param.data = prev_params[i]\n",
    "        \n",
    "        return hess_norm\n",
    "    \n",
    "    def get_hessianv2(self, X,y):\n",
    "        names = list(n for n, _ in self.model.named_parameters())\n",
    "        def loss_fun_hess(*params):\n",
    "            out: torch.Tensor = _stateless.functional_call(self.model, {n: p for n, p in zip(names, params)}, X)\n",
    "            local_loss = self.loss_fn(out, y)\n",
    "            return local_loss\n",
    "        hess_mat = hessian(loss_fun_hess, tuple(self.model.parameters()))\n",
    "        \n",
    "        hess_norm = torch.tensor(0.).to(device)\n",
    "        for i in range(len(hess_mat)):\n",
    "            for j in range(len(hess_mat[0])):\n",
    "                hess_norm+= hess_mat[i][j].pow(2).sum()\n",
    "        hess_norm = hess_norm.sqrt()\n",
    "        # print(f'v2 hess norm{hess_norm}')\n",
    "        return hess_norm.cpu()\n",
    "        \n",
    "    def fit(self, train_loader, test_loader, epochs, store_grads=False, store_hessian=False, store_gen_err=False, store_weights=False, store_pt_loss=True, store_freq = 20, freq_reduce_by=None, freq_reduce_after=None, fast_X_train=None, fast_y_train=None):\n",
    "        \n",
    "        ## For Book keeping results ##\n",
    "        self.grads_norms = []\n",
    "        self.param_list = []\n",
    "        self.hess_norms = []\n",
    "        self.gen_err = []\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        self.point_loss = []\n",
    "        ## Initializing values ##\n",
    "        terminate_training = False\n",
    "        store_count = 0\n",
    "        ## Moving to gpu\n",
    "        fast_X_train = fast_X_train.to(device)\n",
    "        fast_y_train = fast_y_train.to(device)\n",
    "        \n",
    "        \n",
    "        for epoch in tqdm(range(epochs), total=epochs, unit=\"epoch\", disable=False):\n",
    "            if terminate_training == True:\n",
    "                break\n",
    "            for batch, (X, y) in tqdm(enumerate(train_loader), total=len(train_loader), unit='batch',disable=True):\n",
    "                # if batch>300:\n",
    "                #     terminate_training = True\n",
    "                #     break\n",
    "                # print('y shape:',y.shape)\n",
    "                batch_size = len(y)\n",
    "                X, y =X.to(device), y.to(device)\n",
    "                ## assigning the corrupted label\n",
    "                y = fast_y_train[batch*batch_size: (batch+1)*batch_size]\n",
    "                ####\n",
    "                pred = self.model(X)\n",
    "                loss = self.loss_fn(pred, y)\n",
    "                # Backpropagation\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                ## Saving point loss\n",
    "                if store_pt_loss and (batch%store_freq==0):\n",
    "                    self.point_loss.append(loss.item())\n",
    "                    \n",
    "                ## Saving the weights\n",
    "                if store_weights and (batch%store_freq==0):\n",
    "                    current_params = list(self.model.parameters())\n",
    "                    self.param_list.append(current_params)\n",
    "                \n",
    "                ## computing and saving the gradient\n",
    "                if store_grads and (batch% store_freq == 0):\n",
    "                    grad_norm_per_update = self.get_gradient()\n",
    "                    self.grads_norms.append(grad_norm_per_update)\n",
    "                    \n",
    "                ## computing and saving hessian\n",
    "                if store_hessian and (batch% store_freq==0):\n",
    "                    self.optimizer.zero_grad()\n",
    "                    hess_norms_per_update = self.get_hessianv2(X,y)\n",
    "                    # print(f'\\thess norm:{hess_norms_per_update}')\n",
    "                    self.hess_norms.append(hess_norms_per_update)\n",
    "                    \n",
    "                ## computing and storing the generalization error\n",
    "                if store_gen_err and (batch% store_freq == 0):\n",
    "                    train_loss, test_loss=0, 0\n",
    "                    if (fast_y_train is None) or (fast_X_train is None):\n",
    "                        assert False, \"not given fast_X_train and fast_y_train\"\n",
    "                        with torch.no_grad():\n",
    "                            for sub_batch, (X_local,y_local) in enumerate(train_loader):\n",
    "                                if epoch==0 and sub_batch> batch: # only taking the visited points to calculate train loss\n",
    "                                    break\n",
    "                                X_local, y_local = X_local.to(device), y_local.to(device)\n",
    "                                pred_local = self.model(X_local)\n",
    "                                train_loss += self.loss_fn(pred_local, y_local).item()\n",
    "                        train_loss = train_loss/(batch+1)\n",
    "                    else:\n",
    "                        # print('using fast train loss, epoch', epoch)\n",
    "                        with torch.no_grad():\n",
    "                            if epoch==0:\n",
    "                                pred_local = self.model(fast_X_train[:batch+1])\n",
    "                                train_loss = self.loss_fn(pred_local, fast_y_train[:batch+1]).item()\n",
    "                                # print(f'train_loss:{train_loss}')\n",
    "                            else:\n",
    "                                pred_local = self.model(fast_X_train)\n",
    "                                train_loss = self.loss_fn(pred_local, fast_y_train).item()\n",
    "                    with torch.no_grad():\n",
    "                        for sub_batch, (X_local,y_local) in enumerate(test_loader):\n",
    "                            X_local, y_local = X_local.to(device), y_local.to(device)\n",
    "                            pred_local = self.model(X_local)\n",
    "                            test_loss += self.loss_fn(pred_local, y_local).item()\n",
    "                    test_batch_size = len(test_loader)\n",
    "                    # print(f\"Number of batches in test:{len(test_loader)}\")\n",
    "                    test_loss = test_loss/ len(test_loader)\n",
    "                    self.train_loss.append(train_loss)\n",
    "                    self.val_loss.append(test_loss)\n",
    "                    self.gen_err.append(train_loss - test_loss)\n",
    "                    print(f'train loss:{train_loss}',end=' ')\n",
    "                    print(f'test loss :{test_loss}',end=' ')\n",
    "                    print(f'gen loss  :{self.gen_err[-1]}')\n",
    "                    store_count += 1\n",
    "                    if store_count%freq_reduce_after==0:\n",
    "                        store_freq += freq_reduce_by\n",
    "                        \n",
    "                if batch % 1000 == 0:\n",
    "                    loss, current = loss.item(), batch * len(X)\n",
    "                    correct = 0\n",
    "                    test_loss = 0\n",
    "                    with torch.no_grad():\n",
    "                        pred_local = self.model(fast_X_train)\n",
    "                        correct += (pred_local.argmax(1) == fast_y_train).type(torch.float).sum().item()\n",
    "                    # print('data points', fast_X_train.shape[0])\n",
    "                    train_acc = 100* correct/fast_X_train.shape[0]\n",
    "                    correct=0\n",
    "                    print(f'\\ttrain acc:{train_acc}')\n",
    "                    with torch.no_grad():\n",
    "                        for X, y in test_loader:\n",
    "                            X, y = X.to(device), y.to(device)\n",
    "                            pred = self.model(X)\n",
    "                            test_loss += self.loss_fn(pred, y).item()\n",
    "                            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "                    acc = 100*correct/len(test_loader.dataset)\n",
    "                    print(f\"\\ttest acc   :{acc}\")#, at batch:{batch}\")\n",
    "                    print(f\"\\tloss       : {loss:>7f}\")\n",
    "                    print(f'\\tlr rate:{self.scheduler.get_last_lr()}')\n",
    "                self.scheduler.step()\n",
    "            \n",
    "def exp_get_lp_sm(train_data_all, test_data_all, op_features, weight = 10, times = 8, epochs = 1, root_dir='', path=None, clear_file = True, freq_reduce_by=10, freq_reduce_after=100):\n",
    "    grad_list        = []\n",
    "    hess_norm_list   = []\n",
    "    if path is not None:\n",
    "        grad_file_path = root_dir+'grad_'+path\n",
    "        hess_file_path = root_dir+'hess_'+path\n",
    "        gen_file_path = root_dir+'gen_'+path\n",
    "        if clear_file:\n",
    "            with open(grad_file_path, 'w+') as f:\n",
    "                f.write('')\n",
    "            with open(hess_file_path, 'w+') as f:\n",
    "                f.write('')\n",
    "            with open(gen_file_path, 'w+') as f:\n",
    "                f.write('')\n",
    "            \n",
    "    for t in range(times):\n",
    "        train_loader, test_loader, fast_X_train, fast_y_train = get_random_subset(train_data_all, test_data_all)\n",
    "        print(f'Time:{t}')\n",
    "        train_model = Train_nn(784, [weight], op_features, lr= details['alpha_0'])\n",
    "        train_model.fit(train_loader, test_loader, epochs=epochs, store_grads=True, store_hessian=True, store_freq=details['book_keep_freq'],  store_gen_err=True, store_pt_loss=False, store_weights=False, freq_reduce_by = freq_reduce_by, freq_reduce_after=freq_reduce_after, fast_X_train=fast_X_train, fast_y_train=fast_y_train)\n",
    "        \n",
    "        with open(grad_file_path,'a+') as f:\n",
    "            f.write(' '.join([str(grad) for grad in train_model.grads_norms]) + '\\n')\n",
    "        with open(hess_file_path,'a+') as f:\n",
    "            f.write(' '.join([str(hess) for hess in train_model.hess_norms]) + '\\n')\n",
    "        with open(gen_file_path,'a+') as f:\n",
    "            f.write(' '.join([str(gen_e) for gen_e in train_model.gen_err]) + '\\n')\n",
    "        \n",
    "        hess_norm_list.append(train_model.hess_norms)\n",
    "        grad_list.append(train_model.grads_norms)\n",
    "         \n",
    "    return grad_list, hess_norm_list\n",
    "# grad_list, hess_norm_list=[],[]\n",
    "# grad_list, hess_norm_list = exp_get_lp_sm(train_data_all, test_data_all, op_features=10, \n",
    "#               weight=details['g_weight'], times=details['g_times'], \n",
    "#               epochs=details['g_epochs'], root_dir=details['result_root_dir'], \n",
    "#               path=details['result_path'], freq_reduce_by=details['freq_reduce_by'], \n",
    "#               freq_reduce_after=details['freq_reduce_after'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09c7ffb1-3708-469a-8f85-c29d7d420695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size:2000\n",
      "test data size:500\n",
      "X_mat shape:torch.Size([2000, 784]), y_mat shape:torch.Size([2000])\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader, fast_X_train, fast_y_train = get_random_subset(train_data_all, test_data_all, corrupt_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "037c5179-f91d-4b07-86e8-2e35386782b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239272dc6f304484945bd268af359c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttrain acc:11.25\n",
      "\ttest acc   :8.0\n",
      "\tloss       : 2.402829\n",
      "\tlr rate:[1.0]\n",
      "\ttrain acc:14.75\n",
      "\ttest acc   :6.8\n",
      "\tloss       : 2.376818\n",
      "\tlr rate:[0.000999000999000999]\n",
      "\ttrain acc:13.55\n",
      "\ttest acc   :10.8\n",
      "\tloss       : 2.002281\n",
      "\tlr rate:[0.0004997501249375312]\n",
      "\ttrain acc:12.5\n",
      "\ttest acc   :11.6\n",
      "\tloss       : 2.328550\n",
      "\tlr rate:[0.0003332222592469177]\n",
      "\ttrain acc:12.45\n",
      "\ttest acc   :12.0\n",
      "\tloss       : 2.017007\n",
      "\tlr rate:[0.00024993751562109475]\n",
      "\ttrain acc:12.05\n",
      "\ttest acc   :12.8\n",
      "\tloss       : 2.306098\n",
      "\tlr rate:[0.0001999600079984003]\n",
      "\ttrain acc:12.25\n",
      "\ttest acc   :13.0\n",
      "\tloss       : 2.023251\n",
      "\tlr rate:[0.00016663889351774705]\n",
      "\ttrain acc:12.1\n",
      "\ttest acc   :13.0\n",
      "\tloss       : 2.291227\n",
      "\tlr rate:[0.00014283673760891302]\n",
      "\ttrain acc:12.15\n",
      "\ttest acc   :13.0\n",
      "\tloss       : 2.026544\n",
      "\tlr rate:[0.00012498437695288088]\n",
      "\ttrain acc:12.15\n",
      "\ttest acc   :13.0\n",
      "\tloss       : 2.280087\n",
      "\tlr rate:[0.00011109876680368848]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m train_model \u001b[38;5;241m=\u001b[39m Train_nn(\u001b[38;5;241m784\u001b[39m, [\u001b[38;5;241m10\u001b[39m], \u001b[38;5;241m10\u001b[39m, lr\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m, dont_decay \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, l2_reg\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.000\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_grads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_hessian\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mstore_gen_err\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_pt_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq_reduce_by\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq_reduce_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfast_X_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfast_X_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfast_y_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfast_y_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36mTrain_nn.fit\u001b[0;34m(self, train_loader, test_loader, epochs, store_grads, store_hessian, store_gen_err, store_weights, store_pt_loss, store_freq, freq_reduce_by, freq_reduce_after, fast_X_train, fast_y_train)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 269\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m## Saving point loss\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model = Train_nn(784, [10], 10, lr= 1., dont_decay = False, l2_reg=0.000)\n",
    "train_model.fit(train_loader, test_loader, epochs=100, store_grads=False, store_hessian=False, store_freq=10000,  store_gen_err=False, store_pt_loss=False, store_weights=False, freq_reduce_by = 100, freq_reduce_after=10000, fast_X_train=fast_X_train, fast_y_train=fast_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aa97c2-fc0f-4813-8b26-46ebf9e61b19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cbc9075a-bcca-4a4a-be6d-01cc7875e6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size:2000\n",
      "test data size:500\n",
      "X_mat shape:torch.Size([2000, 784]), y_mat shape:torch.Size([2000])\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader, fast_X_train, fast_y_train = get_random_subset(train_data_all, test_data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3db01245-2359-4293-aad2-7a6ee832e54c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eda49ff4873479da53dd9f9898c9f0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttrain acc:1.85\n",
      "\ttest acc   :1.2\n",
      "\tloss       : 2.498376\n",
      "\tlr rate:[0.3]\n",
      "\ttrain acc:23.35\n",
      "\ttest acc   :20.6\n",
      "\tloss       : 2.318381\n",
      "\tlr rate:[0.0002997002997002997]\n",
      "\ttrain acc:25.1\n",
      "\ttest acc   :21.6\n",
      "\tloss       : 0.418449\n",
      "\tlr rate:[0.00014992503748125936]\n",
      "\ttrain acc:27.05\n",
      "\ttest acc   :24.0\n",
      "\tloss       : 2.297851\n",
      "\tlr rate:[9.99666777740753e-05]\n",
      "\ttrain acc:27.35\n",
      "\ttest acc   :25.0\n",
      "\tloss       : 0.348010\n",
      "\tlr rate:[7.498125468632842e-05]\n",
      "\ttrain acc:28.7\n",
      "\ttest acc   :25.2\n",
      "\tloss       : 2.291203\n",
      "\tlr rate:[5.998800239952009e-05]\n",
      "\ttrain acc:29.2\n",
      "\ttest acc   :25.8\n",
      "\tloss       : 0.313152\n",
      "\tlr rate:[4.9991668055324116e-05]\n",
      "\ttrain acc:29.85\n",
      "\ttest acc   :26.0\n",
      "\tloss       : 2.284672\n",
      "\tlr rate:[4.2851021282673906e-05]\n",
      "\ttrain acc:30.1\n",
      "\ttest acc   :26.8\n",
      "\tloss       : 0.290971\n",
      "\tlr rate:[3.749531308586426e-05]\n",
      "\ttrain acc:30.5\n",
      "\ttest acc   :26.8\n",
      "\tloss       : 2.279595\n",
      "\tlr rate:[3.3329630041106545e-05]\n",
      "\ttrain acc:31.2\n",
      "\ttest acc   :27.4\n",
      "\tloss       : 0.275081\n",
      "\tlr rate:[2.9997000299970004e-05]\n",
      "\ttrain acc:31.95\n",
      "\ttest acc   :28.0\n",
      "\tloss       : 2.275143\n",
      "\tlr rate:[2.727024815925825e-05]\n",
      "\ttrain acc:32.3\n",
      "\ttest acc   :28.4\n",
      "\tloss       : 0.263325\n",
      "\tlr rate:[2.499791684026331e-05]\n",
      "\ttrain acc:32.95\n",
      "\ttest acc   :29.4\n",
      "\tloss       : 2.271270\n",
      "\tlr rate:[2.307514806553342e-05]\n",
      "\ttrain acc:33.4\n",
      "\ttest acc   :29.8\n",
      "\tloss       : 0.253983\n",
      "\tlr rate:[2.142704092564817e-05]\n",
      "\ttrain acc:33.85\n",
      "\ttest acc   :29.6\n",
      "\tloss       : 2.267582\n",
      "\tlr rate:[1.9998666755549627e-05]\n",
      "\ttrain acc:34.15\n",
      "\ttest acc   :29.6\n",
      "\tloss       : 0.246157\n",
      "\tlr rate:[1.874882819823761e-05]\n",
      "\ttrain acc:34.9\n",
      "\ttest acc   :30.2\n",
      "\tloss       : 2.264297\n",
      "\tlr rate:[1.764602082230457e-05]\n",
      "\ttrain acc:35.2\n",
      "\ttest acc   :30.8\n",
      "\tloss       : 0.239523\n",
      "\tlr rate:[1.666574079217821e-05]\n",
      "\ttrain acc:35.5\n",
      "\ttest acc   :30.4\n",
      "\tloss       : 2.261290\n",
      "\tlr rate:[1.578864270301563e-05]\n",
      "\ttrain acc:35.85\n",
      "\ttest acc   :30.8\n",
      "\tloss       : 0.233768\n",
      "\tlr rate:[1.4999250037498124e-05]\n",
      "\ttrain acc:36.2\n",
      "\ttest acc   :31.4\n",
      "\tloss       : 2.258567\n",
      "\tlr rate:[1.428503404599781e-05]\n",
      "\ttrain acc:36.4\n",
      "\ttest acc   :31.8\n",
      "\tloss       : 0.228702\n",
      "\tlr rate:[1.3635743829825917e-05]\n",
      "\ttrain acc:36.8\n",
      "\ttest acc   :32.2\n",
      "\tloss       : 2.255978\n",
      "\tlr rate:[1.3042911177774879e-05]\n",
      "\ttrain acc:37.0\n",
      "\ttest acc   :32.4\n",
      "\tloss       : 0.224205\n",
      "\tlr rate:[1.2499479188367151e-05]\n",
      "\ttrain acc:37.2\n",
      "\ttest acc   :33.0\n",
      "\tloss       : 2.253542\n",
      "\tlr rate:[1.199952001919923e-05]\n",
      "\ttrain acc:37.45\n",
      "\ttest acc   :33.0\n",
      "\tloss       : 0.220179\n",
      "\tlr rate:[1.1538017768547364e-05]\n",
      "\ttrain acc:37.7\n",
      "\ttest acc   :33.4\n",
      "\tloss       : 2.251283\n",
      "\tlr rate:[1.111069960371838e-05]\n",
      "\ttrain acc:37.8\n",
      "\ttest acc   :33.8\n",
      "\tloss       : 0.216556\n",
      "\tlr rate:[1.0713903074890182e-05]\n",
      "\ttrain acc:38.15\n",
      "\ttest acc   :34.4\n",
      "\tloss       : 2.249126\n",
      "\tlr rate:[1.0344470880314471e-05]\n",
      "\ttrain acc:38.1\n",
      "\ttest acc   :34.4\n",
      "\tloss       : 0.213244\n",
      "\tlr rate:[9.999666677777408e-06]\n",
      "\ttrain acc:38.25\n",
      "\ttest acc   :34.8\n",
      "\tloss       : 2.247111\n",
      "\tlr rate:[9.677107190090643e-06]\n",
      "\ttrain acc:38.2\n",
      "\ttest acc   :35.0\n",
      "\tloss       : 0.210203\n",
      "\tlr rate:[9.374707040404987e-06]\n",
      "\ttrain acc:38.25\n",
      "\ttest acc   :35.2\n",
      "\tloss       : 2.245173\n",
      "\tlr rate:[9.090633617163116e-06]\n",
      "\ttrain acc:38.4\n",
      "\ttest acc   :35.2\n",
      "\tloss       : 0.207401\n",
      "\tlr rate:[8.823269903826358e-06]\n",
      "\ttrain acc:38.5\n",
      "\ttest acc   :35.6\n",
      "\tloss       : 2.243339\n",
      "\tlr rate:[8.571183680466272e-06]\n",
      "\ttrain acc:38.5\n",
      "\ttest acc   :36.0\n",
      "\tloss       : 0.204812\n",
      "\tlr rate:[8.333101858281713e-06]\n",
      "\ttrain acc:38.75\n",
      "\ttest acc   :36.2\n",
      "\tloss       : 2.241635\n",
      "\tlr rate:[8.107888975973621e-06]\n",
      "\ttrain acc:38.85\n",
      "\ttest acc   :36.2\n",
      "\tloss       : 0.202403\n",
      "\tlr rate:[7.894529091339701e-06]\n",
      "\ttrain acc:38.9\n",
      "\ttest acc   :36.2\n",
      "\tloss       : 2.239997\n",
      "\tlr rate:[7.692110458706186e-06]\n",
      "\ttrain acc:39.2\n",
      "\ttest acc   :36.0\n",
      "\tloss       : 0.200146\n",
      "\tlr rate:[7.499812504687382e-06]\n",
      "\ttrain acc:39.5\n",
      "\ttest acc   :36.2\n",
      "\tloss       : 2.238419\n",
      "\tlr rate:[7.316894709885124e-06]\n",
      "\ttrain acc:39.55\n",
      "\ttest acc   :36.6\n",
      "\tloss       : 0.198021\n",
      "\tlr rate:[7.142687078879074e-06]\n",
      "\ttrain acc:39.55\n",
      "\ttest acc   :36.8\n",
      "\tloss       : 2.236944\n",
      "\tlr rate:[6.976581939954884e-06]\n",
      "\ttrain acc:39.4\n",
      "\ttest acc   :37.0\n",
      "\tloss       : 0.196037\n",
      "\tlr rate:[6.81802686302584e-06]\n",
      "\ttrain acc:39.55\n",
      "\ttest acc   :37.2\n",
      "\tloss       : 2.235519\n",
      "\tlr rate:[6.666518521810627e-06]\n",
      "\ttrain acc:39.75\n",
      "\ttest acc   :37.8\n",
      "\tloss       : 0.194167\n",
      "\tlr rate:[6.521597356579204e-06]\n",
      "\ttrain acc:39.75\n",
      "\ttest acc   :37.8\n",
      "\tloss       : 2.234130\n",
      "\tlr rate:[6.382842918235782e-06]\n",
      "\ttrain acc:39.85\n",
      "\ttest acc   :37.8\n",
      "\tloss       : 0.192398\n",
      "\tlr rate:[6.249869794379284e-06]\n",
      "\ttrain acc:40.0\n",
      "\ttest acc   :37.8\n",
      "\tloss       : 2.232788\n",
      "\tlr rate:[6.122324034203383e-06]\n",
      "\ttrain acc:40.05\n",
      "\ttest acc   :37.6\n",
      "\tloss       : 0.190718\n",
      "\tlr rate:[5.999880002399952e-06]\n",
      "\ttrain acc:40.05\n",
      "\ttest acc   :37.8\n",
      "\tloss       : 2.231504\n",
      "\tlr rate:[5.88223760318425e-06]\n",
      "\ttrain acc:40.25\n",
      "\ttest acc   :37.6\n",
      "\tloss       : 0.189122\n",
      "\tlr rate:[5.769119824618758e-06]\n",
      "\ttrain acc:40.3\n",
      "\ttest acc   :37.6\n",
      "\tloss       : 2.230255\n",
      "\tlr rate:[5.660270560932812e-06]\n",
      "\ttrain acc:40.45\n",
      "\ttest acc   :37.4\n",
      "\tloss       : 0.187598\n",
      "\tlr rate:[5.555452676802281e-06]\n",
      "\ttrain acc:40.45\n",
      "\ttest acc   :37.6\n",
      "\tloss       : 2.229043\n",
      "\tlr rate:[5.454446282794858e-06]\n",
      "\ttrain acc:40.45\n",
      "\ttest acc   :37.8\n",
      "\tloss       : 0.186147\n",
      "\tlr rate:[5.3570471955857935e-06]\n",
      "\ttrain acc:40.5\n",
      "\ttest acc   :37.8\n",
      "\tloss       : 2.227860\n",
      "\tlr rate:[5.263065560253328e-06]\n",
      "\ttrain acc:40.6\n",
      "\ttest acc   :37.8\n",
      "\tloss       : 0.184761\n",
      "\tlr rate:[5.172324615092844e-06]\n",
      "\ttrain acc:40.7\n",
      "\ttest acc   :38.0\n",
      "\tloss       : 2.226709\n",
      "\tlr rate:[5.084659582040982e-06]\n",
      "\ttrain acc:40.75\n",
      "\ttest acc   :38.0\n",
      "\tloss       : 0.183432\n",
      "\tlr rate:[4.999916668055532e-06]\n",
      "\ttrain acc:40.75\n",
      "\ttest acc   :38.0\n",
      "\tloss       : 2.225587\n",
      "\tlr rate:[4.917952164718611e-06]\n",
      "\ttrain acc:40.75\n",
      "\ttest acc   :38.0\n",
      "\tloss       : 0.182158\n",
      "\tlr rate:[4.8386316349736296e-06]\n",
      "\ttrain acc:40.9\n",
      "\ttest acc   :38.0\n",
      "\tloss       : 2.224492\n",
      "\tlr rate:[4.761829177314646e-06]\n",
      "\ttrain acc:41.0\n",
      "\ttest acc   :38.0\n",
      "\tloss       : 0.180936\n",
      "\tlr rate:[4.687426758956891e-06]\n",
      "\ttrain acc:41.1\n",
      "\ttest acc   :38.2\n",
      "\tloss       : 2.223426\n",
      "\tlr rate:[4.615313610559837e-06]\n",
      "\ttrain acc:41.15\n",
      "\ttest acc   :38.4\n",
      "\tloss       : 0.179759\n",
      "\tlr rate:[4.545385675974606e-06]\n",
      "\ttrain acc:41.05\n",
      "\ttest acc   :38.4\n",
      "\tloss       : 2.222390\n",
      "\tlr rate:[4.4775451112669955e-06]\n",
      "\ttrain acc:41.0\n",
      "\ttest acc   :38.6\n",
      "\tloss       : 0.178627\n",
      "\tlr rate:[4.4116998279437065e-06]\n",
      "\ttrain acc:41.15\n",
      "\ttest acc   :38.6\n",
      "\tloss       : 2.221377\n",
      "\tlr rate:[4.34776307589745e-06]\n",
      "\ttrain acc:41.2\n",
      "\ttest acc   :38.6\n",
      "\tloss       : 0.177538\n",
      "\tlr rate:[4.285653062099113e-06]\n",
      "\ttrain acc:41.25\n",
      "\ttest acc   :38.6\n",
      "\tloss       : 2.220383\n",
      "\tlr rate:[4.225292601512655e-06]\n",
      "\ttrain acc:41.25\n",
      "\ttest acc   :38.6\n",
      "\tloss       : 0.176489\n",
      "\tlr rate:[4.16660879710004e-06]\n",
      "\ttrain acc:41.25\n",
      "\ttest acc   :38.8\n",
      "\tloss       : 2.219412\n",
      "\tlr rate:[4.1095327461267655e-06]\n",
      "\ttrain acc:41.35\n",
      "\ttest acc   :38.8\n",
      "\tloss       : 0.175478\n",
      "\tlr rate:[4.053999270280131e-06]\n",
      "\ttrain acc:41.4\n",
      "\ttest acc   :38.8\n",
      "\tloss       : 2.218470\n",
      "\tlr rate:[3.999946667377768e-06]\n",
      "\ttrain acc:41.45\n",
      "\ttest acc   :38.8\n",
      "\tloss       : 0.174503\n",
      "\tlr rate:[3.94731648267786e-06]\n",
      "\ttrain acc:41.5\n",
      "\ttest acc   :38.8\n",
      "\tloss       : 2.217547\n",
      "\tlr rate:[3.896053298009117e-06]\n",
      "\ttrain acc:41.45\n",
      "\ttest acc   :39.0\n",
      "\tloss       : 0.173558\n",
      "\tlr rate:[3.846104537121319e-06]\n",
      "\ttrain acc:41.55\n",
      "\ttest acc   :39.0\n",
      "\tloss       : 2.216641\n",
      "\tlr rate:[3.797420285819167e-06]\n",
      "\ttrain acc:41.65\n",
      "\ttest acc   :39.0\n",
      "\tloss       : 0.172650\n",
      "\tlr rate:[3.7499531255859297e-06]\n",
      "\ttrain acc:41.6\n",
      "\ttest acc   :39.2\n",
      "\tloss       : 2.215752\n",
      "\tlr rate:[3.7036579795311167e-06]\n",
      "\ttrain acc:41.75\n",
      "\ttest acc   :39.2\n",
      "\tloss       : 0.171768\n",
      "\tlr rate:[3.658491969610127e-06]\n",
      "\ttrain acc:41.75\n",
      "\ttest acc   :39.4\n",
      "\tloss       : 2.214879\n",
      "\tlr rate:[3.614414284165251e-06]\n",
      "\ttrain acc:41.9\n",
      "\ttest acc   :39.4\n",
      "\tloss       : 0.170913\n",
      "\tlr rate:[3.5713860549279176e-06]\n",
      "\ttrain acc:41.9\n",
      "\ttest acc   :39.4\n",
      "\tloss       : 2.214019\n",
      "\tlr rate:[3.5293702427030266e-06]\n",
      "\ttrain acc:41.9\n",
      "\ttest acc   :39.4\n",
      "\tloss       : 0.170085\n",
      "\tlr rate:[3.4883315310287087e-06]\n",
      "\ttrain acc:41.85\n",
      "\ttest acc   :39.4\n",
      "\tloss       : 2.213173\n",
      "\tlr rate:[3.4482362271698027e-06]\n",
      "\ttrain acc:41.95\n",
      "\ttest acc   :39.6\n",
      "\tloss       : 0.169281\n",
      "\tlr rate:[3.4090521698617062e-06]\n",
      "\ttrain acc:42.0\n",
      "\ttest acc   :39.6\n",
      "\tloss       : 2.212343\n",
      "\tlr rate:[3.370748643273671e-06]\n",
      "\ttrain acc:42.15\n",
      "\ttest acc   :39.6\n",
      "\tloss       : 0.168501\n",
      "\tlr rate:[3.3332962967078142e-06]\n",
      "\ttrain acc:42.15\n",
      "\ttest acc   :39.6\n",
      "\tloss       : 2.211525\n",
      "\tlr rate:[3.296667069592642e-06]\n",
      "\ttrain acc:42.1\n",
      "\ttest acc   :39.6\n",
      "\tloss       : 0.167745\n",
      "\tlr rate:[3.260834121368246e-06]\n",
      "\ttrain acc:42.1\n",
      "\ttest acc   :39.6\n",
      "\tloss       : 2.210721\n",
      "\tlr rate:[3.2257717658949903e-06]\n",
      "\ttrain acc:42.05\n",
      "\ttest acc   :39.6\n",
      "\tloss       : 0.167009\n",
      "\tlr rate:[3.191455410048829e-06]\n",
      "\ttrain acc:42.05\n",
      "\ttest acc   :39.6\n",
      "\tloss       : 2.209931\n",
      "\tlr rate:[3.157861496194777e-06]\n",
      "\ttrain acc:42.1\n",
      "\ttest acc   :39.6\n",
      "\tloss       : 0.166295\n",
      "\tlr rate:[3.1249674482557474e-06]\n",
      "\ttrain acc:42.1\n",
      "\ttest acc   :39.6\n",
      "\tloss       : 2.209153\n",
      "\tlr rate:[3.092751621117308e-06]\n",
      "\ttrain acc:42.1\n",
      "\ttest acc   :39.4\n",
      "\tloss       : 0.165599\n",
      "\tlr rate:[3.06119325313007e-06]\n",
      "\ttrain acc:42.1\n",
      "\ttest acc   :39.6\n",
      "\tloss       : 2.208387\n",
      "\tlr rate:[3.030272421490692e-06]\n",
      "\ttrain acc:42.0\n",
      "\ttest acc   :39.6\n",
      "\tloss       : 0.164922\n",
      "\tlr rate:[2.999970000299997e-06]\n",
      "\ttrain acc:42.05\n",
      "\ttest acc   :39.6\n",
      "\tloss       : 2.207633\n",
      "\tlr rate:[2.970267621112662e-06]\n",
      "\ttrain acc:42.05\n",
      "\ttest acc   :39.4\n",
      "\tloss       : 0.164263\n",
      "\tlr rate:[2.941147635807492e-06]\n",
      "\ttrain acc:42.05\n",
      "\ttest acc   :39.4\n",
      "\tloss       : 2.206890\n",
      "\tlr rate:[2.912593081620567e-06]\n",
      "\ttrain acc:42.05\n",
      "\ttest acc   :39.2\n",
      "\tloss       : 0.163620\n",
      "\tlr rate:[2.8845876481956904e-06]\n",
      "\ttrain acc:42.05\n",
      "\ttest acc   :39.2\n",
      "\tloss       : 2.206158\n",
      "\tlr rate:[2.8571156465176523e-06]\n",
      "\ttrain acc:42.1\n",
      "\ttest acc   :39.2\n",
      "\tloss       : 0.162992\n",
      "\tlr rate:[2.8301619796039657e-06]\n",
      "\ttrain acc:42.1\n",
      "\ttest acc   :39.2\n",
      "\tloss       : 2.205436\n",
      "\tlr rate:[2.8037121148400484e-06]\n",
      "\ttrain acc:42.05\n",
      "\ttest acc   :39.2\n",
      "\tloss       : 0.162378\n",
      "\tlr rate:[2.777752057851316e-06]\n",
      "\ttrain acc:42.05\n",
      "\ttest acc   :39.2\n",
      "\tloss       : 2.204734\n",
      "\tlr rate:[2.752268327813506e-06]\n",
      "\ttrain acc:42.05\n",
      "\ttest acc   :39.2\n",
      "\tloss       : 0.161781\n",
      "\tlr rate:[2.72724793410969e-06]\n",
      "\ttrain acc:42.1\n",
      "\ttest acc   :39.2\n",
      "\tloss       : 2.204043\n",
      "\tlr rate:[2.7026783542490607e-06]\n",
      "\ttrain acc:42.1\n",
      "\ttest acc   :39.2\n",
      "\tloss       : 0.161202\n",
      "\tlr rate:[2.6785475129686345e-06]\n",
      "\ttrain acc:42.15\n",
      "\ttest acc   :39.2\n",
      "\tloss       : 2.203363\n",
      "\tlr rate:[2.65484376244458e-06]\n",
      "\ttrain acc:42.2\n",
      "\ttest acc   :39.2\n",
      "\tloss       : 0.160633\n",
      "\tlr rate:[2.6315558635450564e-06]\n",
      "\ttrain acc:42.25\n",
      "\ttest acc   :39.4\n",
      "\tloss       : 2.202690\n",
      "\tlr rate:[2.608672968061147e-06]\n",
      "\ttrain acc:42.35\n",
      "\ttest acc   :39.4\n",
      "\tloss       : 0.160078\n",
      "\tlr rate:[2.5861846018568804e-06]\n",
      "\ttrain acc:42.45\n",
      "\ttest acc   :39.4\n",
      "\tloss       : 2.202101\n",
      "\tlr rate:[2.564080648883343e-06]\n",
      "\ttrain acc:42.5\n",
      "\ttest acc   :39.2\n",
      "\tloss       : 0.159536\n",
      "\tlr rate:[2.542351336005627e-06]\n",
      "\ttrain acc:42.55\n",
      "\ttest acc   :39.2\n",
      "\tloss       : 2.201561\n",
      "\tlr rate:[2.520987218594802e-06]\n",
      "\ttrain acc:42.6\n",
      "\ttest acc   :39.2\n",
      "\tloss       : 0.159005\n",
      "\tlr rate:[2.4999791668402764e-06]\n",
      "\ttrain acc:42.6\n",
      "\ttest acc   :39.2\n",
      "\tloss       : 2.201028\n",
      "\tlr rate:[2.4793183527408864e-06]\n",
      "\ttrain acc:42.6\n",
      "\ttest acc   :39.2\n",
      "\tloss       : 0.158489\n",
      "\tlr rate:[2.4589962377357564e-06]\n",
      "\ttrain acc:42.6\n",
      "\ttest acc   :39.2\n",
      "\tloss       : 2.200504\n",
      "\tlr rate:[2.4390045609385285e-06]\n",
      "\ttrain acc:42.6\n",
      "\ttest acc   :39.4\n",
      "\tloss       : 0.157983\n",
      "\tlr rate:[2.419335327940904e-06]\n",
      "\ttrain acc:42.65\n",
      "\ttest acc   :39.4\n",
      "\tloss       : 2.199986\n",
      "\tlr rate:[2.3999808001535985e-06]\n",
      "\ttrain acc:42.7\n",
      "\ttest acc   :39.4\n",
      "\tloss       : 0.157489\n",
      "\tlr rate:[2.3809334846548834e-06]\n",
      "\ttrain acc:42.7\n",
      "\ttest acc   :39.4\n",
      "\tloss       : 2.199475\n",
      "\tlr rate:[2.3621861245187044e-06]\n",
      "\ttrain acc:42.7\n",
      "\ttest acc   :39.4\n",
      "\tloss       : 0.157005\n",
      "\tlr rate:[2.343731689596175e-06]\n",
      "\ttrain acc:42.75\n",
      "\ttest acc   :39.4\n",
      "\tloss       : 2.198972\n",
      "\tlr rate:[2.3255633677258314e-06]\n",
      "\ttrain acc:42.75\n",
      "\ttest acc   :39.4\n",
      "\tloss       : 0.156527\n",
      "\tlr rate:[2.3076745563495667e-06]\n",
      "\ttrain acc:42.8\n",
      "\ttest acc   :39.6\n",
      "\tloss       : 2.198475\n",
      "\tlr rate:[2.2900588545125607e-06]\n",
      "\ttrain acc:42.75\n",
      "\ttest acc   :39.6\n",
      "\tloss       : 0.156059\n",
      "\tlr rate:[2.272710055226854e-06]\n",
      "\ttrain acc:42.75\n",
      "\ttest acc   :39.8\n",
      "\tloss       : 2.197984\n",
      "\tlr rate:[2.255622138179412e-06]\n",
      "\ttrain acc:42.75\n",
      "\ttest acc   :39.8\n",
      "\tloss       : 0.155599\n",
      "\tlr rate:[2.238789262766696e-06]\n",
      "\ttrain acc:42.75\n",
      "\ttest acc   :39.8\n",
      "\tloss       : 2.197500\n",
      "\tlr rate:[2.222205761438804e-06]\n",
      "\ttrain acc:42.75\n",
      "\ttest acc   :39.6\n",
      "\tloss       : 0.155142\n",
      "\tlr rate:[2.205866133337255e-06]\n",
      "\ttrain acc:42.75\n",
      "\ttest acc   :39.6\n",
      "\tloss       : 2.197021\n",
      "\tlr rate:[2.1897650382113997e-06]\n",
      "\ttrain acc:42.8\n",
      "\ttest acc   :39.6\n",
      "\tloss       : 0.154693\n",
      "\tlr rate:[2.1738972905993433e-06]\n",
      "\ttrain acc:42.8\n",
      "\ttest acc   :39.6\n",
      "\tloss       : 2.196548\n",
      "\tlr rate:[2.158257854260041e-06]\n",
      "\ttrain acc:42.85\n",
      "\ttest acc   :39.6\n",
      "\tloss       : 0.154253\n",
      "\tlr rate:[2.1428418368440224e-06]\n",
      "\ttrain acc:42.8\n",
      "\ttest acc   :39.6\n",
      "\tloss       : 2.196081\n",
      "\tlr rate:[2.127644484790888e-06]\n",
      "\ttrain acc:42.85\n",
      "\ttest acc   :39.6\n",
      "\tloss       : 0.153822\n",
      "\tlr rate:[2.1126611784424054e-06]\n",
      "\ttrain acc:43.0\n",
      "\ttest acc   :39.6\n",
      "\tloss       : 2.195621\n",
      "\tlr rate:[2.0978874273606477e-06]\n",
      "\ttrain acc:43.0\n",
      "\ttest acc   :39.6\n",
      "\tloss       : 0.153399\n",
      "\tlr rate:[2.0833188658412093e-06]\n",
      "\ttrain acc:43.0\n",
      "\ttest acc   :39.6\n",
      "\tloss       : 2.195166\n",
      "\tlr rate:[2.0689512486120785e-06]\n",
      "\ttrain acc:43.0\n",
      "\ttest acc   :39.6\n",
      "\tloss       : 0.152982\n",
      "\tlr rate:[2.0547804467092688e-06]\n",
      "\ttrain acc:43.05\n",
      "\ttest acc   :39.6\n",
      "\tloss       : 2.194717\n",
      "\tlr rate:[2.0408024435207924e-06]\n",
      "\ttrain acc:43.0\n",
      "\ttest acc   :39.6\n",
      "\tloss       : 0.152572\n",
      "\tlr rate:[2.0270133309910066e-06]\n",
      "\ttrain acc:43.0\n",
      "\ttest acc   :39.6\n",
      "\tloss       : 2.194273\n",
      "\tlr rate:[2.013409305977812e-06]\n",
      "\ttrain acc:43.0\n",
      "\ttest acc   :39.8\n",
      "\tloss       : 0.152169\n",
      "\tlr rate:[1.9999866667555547e-06]\n",
      "\ttrain acc:43.0\n",
      "\ttest acc   :39.8\n",
      "\tloss       : 2.193833\n",
      "\tlr rate:[1.9867418096568896e-06]\n",
      "\ttrain acc:43.05\n",
      "\ttest acc   :39.8\n",
      "\tloss       : 0.151773\n",
      "\tlr rate:[1.973671225847198e-06]\n",
      "\ttrain acc:43.1\n",
      "\ttest acc   :39.8\n",
      "\tloss       : 2.193400\n",
      "\tlr rate:[1.960771498225502e-06]\n",
      "\ttrain acc:43.1\n",
      "\ttest acc   :39.8\n",
      "\tloss       : 0.151382\n",
      "\tlr rate:[1.948039298446114e-06]\n",
      "\ttrain acc:43.1\n",
      "\ttest acc   :39.8\n",
      "\tloss       : 2.192970\n",
      "\tlr rate:[1.935471384055587e-06]\n",
      "\ttrain acc:43.15\n",
      "\ttest acc   :40.0\n",
      "\tloss       : 0.151000\n",
      "\tlr rate:[1.923064595739771e-06]\n",
      "\ttrain acc:43.1\n",
      "\ttest acc   :40.2\n",
      "\tloss       : 2.192545\n",
      "\tlr rate:[1.910815854676085e-06]\n",
      "\ttrain acc:43.15\n",
      "\ttest acc   :40.2\n",
      "\tloss       : 0.150624\n",
      "\tlr rate:[1.898722159986329e-06]\n",
      "\ttrain acc:43.15\n",
      "\ttest acc   :40.2\n",
      "\tloss       : 2.192125\n",
      "\tlr rate:[1.8867805862856207e-06]\n",
      "\ttrain acc:43.15\n",
      "\ttest acc   :40.0\n",
      "\tloss       : 0.150253\n",
      "\tlr rate:[1.8749882813232417e-06]\n",
      "\ttrain acc:43.2\n",
      "\ttest acc   :40.2\n",
      "\tloss       : 2.191710\n",
      "\tlr rate:[1.8633424637114054e-06]\n",
      "\ttrain acc:43.2\n",
      "\ttest acc   :40.2\n",
      "\tloss       : 0.149887\n",
      "\tlr rate:[1.8518404207381436e-06]\n",
      "\ttrain acc:43.2\n",
      "\ttest acc   :40.2\n",
      "\tloss       : 2.191298\n",
      "\tlr rate:[1.8404795062606977e-06]\n",
      "\ttrain acc:43.3\n",
      "\ttest acc   :40.2\n",
      "\tloss       : 0.149526\n",
      "\tlr rate:[1.8292571386759837e-06]\n",
      "\ttrain acc:43.45\n",
      "\ttest acc   :40.2\n",
      "\tloss       : 2.190890\n",
      "\tlr rate:[1.8181707989648546e-06]\n",
      "\ttrain acc:43.45\n",
      "\ttest acc   :40.2\n",
      "\tloss       : 0.149170\n",
      "\tlr rate:[1.8072180288070551e-06]\n",
      "\ttrain acc:43.45\n",
      "\ttest acc   :40.4\n",
      "\tloss       : 2.190486\n",
      "\tlr rate:[1.7963964287638997e-06]\n",
      "\ttrain acc:43.45\n",
      "\ttest acc   :40.4\n",
      "\tloss       : 0.148820\n",
      "\tlr rate:[1.7857036565258538e-06]\n",
      "\ttrain acc:43.45\n",
      "\ttest acc   :40.4\n",
      "\tloss       : 2.190087\n",
      "\tlr rate:[1.7751374252223359e-06]\n",
      "\ttrain acc:43.45\n",
      "\ttest acc   :40.4\n",
      "\tloss       : 0.148475\n",
      "\tlr rate:[1.764695501791166e-06]\n",
      "\ttrain acc:43.45\n",
      "\ttest acc   :40.6\n",
      "\tloss       : 2.189691\n",
      "\tlr rate:[1.7543757054052315e-06]\n",
      "\ttrain acc:43.4\n",
      "\ttest acc   :40.6\n",
      "\tloss       : 0.148135\n",
      "\tlr rate:[1.744175905954035e-06]\n",
      "\ttrain acc:43.4\n",
      "\ttest acc   :40.6\n",
      "\tloss       : 2.189300\n",
      "\tlr rate:[1.734094022577904e-06]\n",
      "\ttrain acc:43.4\n",
      "\ttest acc   :40.6\n",
      "\tloss       : 0.147802\n",
      "\tlr rate:[1.7241280222527457e-06]\n",
      "\ttrain acc:43.4\n",
      "\ttest acc   :40.6\n",
      "\tloss       : 2.188912\n",
      "\tlr rate:[1.714275918423323e-06]\n",
      "\ttrain acc:43.4\n",
      "\ttest acc   :40.6\n",
      "\tloss       : 0.147472\n",
      "\tlr rate:[1.7045357696831268e-06]\n",
      "\ttrain acc:43.4\n",
      "\ttest acc   :40.6\n",
      "\tloss       : 2.188528\n",
      "\tlr rate:[1.6949056784989917e-06]\n",
      "\ttrain acc:43.4\n",
      "\ttest acc   :40.6\n",
      "\tloss       : 0.147148\n",
      "\tlr rate:[1.6853837899787078e-06]\n",
      "\ttrain acc:43.4\n",
      "\ttest acc   :40.6\n",
      "\tloss       : 2.188148\n",
      "\tlr rate:[1.6759682906799403e-06]\n",
      "\ttrain acc:43.45\n",
      "\ttest acc   :40.6\n",
      "\tloss       : 0.146829\n",
      "\tlr rate:[1.6666574074588475e-06]\n",
      "\ttrain acc:43.4\n",
      "\ttest acc   :40.6\n",
      "\tloss       : 2.187772\n",
      "\tlr rate:[1.657449406356871e-06]\n",
      "\ttrain acc:43.4\n",
      "\ttest acc   :40.6\n",
      "\tloss       : 0.146514\n",
      "\tlr rate:[1.6483425915242223e-06]\n",
      "\ttrain acc:43.45\n",
      "\ttest acc   :40.6\n",
      "\tloss       : 2.187399\n",
      "\tlr rate:[1.6393353041786658e-06]\n",
      "\ttrain acc:43.55\n",
      "\ttest acc   :40.6\n",
      "\tloss       : 0.146204\n",
      "\tlr rate:[1.6304259215982522e-06]\n",
      "\ttrain acc:43.55\n",
      "\ttest acc   :40.6\n",
      "\tloss       : 2.187029\n",
      "\tlr rate:[1.6216128561467234e-06]\n",
      "\ttrain acc:43.55\n",
      "\ttest acc   :40.6\n",
      "\tloss       : 0.145898\n",
      "\tlr rate:[1.612894554330353e-06]\n",
      "\ttrain acc:43.55\n",
      "\ttest acc   :40.6\n",
      "\tloss       : 2.186663\n",
      "\tlr rate:[1.6042694958850489e-06]\n",
      "\ttrain acc:43.55\n",
      "\ttest acc   :40.6\n",
      "\tloss       : 0.145597\n",
      "\tlr rate:[1.5957361928925909e-06]\n",
      "\ttrain acc:43.55\n",
      "\ttest acc   :40.6\n",
      "\tloss       : 2.186300\n",
      "\tlr rate:[1.5872931889249264e-06]\n",
      "\ttrain acc:43.6\n",
      "\ttest acc   :40.6\n",
      "\tloss       : 0.145301\n",
      "\tlr rate:[1.5789390582154831e-06]\n",
      "\ttrain acc:43.6\n",
      "\ttest acc   :40.6\n",
      "\tloss       : 2.185940\n",
      "\tlr rate:[1.570672404856519e-06]\n",
      "\ttrain acc:43.6\n",
      "\ttest acc   :40.6\n",
      "\tloss       : 0.145009\n",
      "\tlr rate:[1.562491862021552e-06]\n",
      "\ttrain acc:43.6\n",
      "\ttest acc   :40.8\n",
      "\tloss       : 2.185583\n",
      "\tlr rate:[1.5543960912119626e-06]\n",
      "\ttrain acc:43.6\n",
      "\ttest acc   :41.0\n",
      "\tloss       : 0.144720\n",
      "\tlr rate:[1.5463837815268995e-06]\n",
      "\ttrain acc:43.55\n",
      "\ttest acc   :41.0\n",
      "\tloss       : 2.185229\n",
      "\tlr rate:[1.5384536489556462e-06]\n",
      "\ttrain acc:43.6\n",
      "\ttest acc   :41.0\n",
      "\tloss       : 0.144436\n",
      "\tlr rate:[1.5306044356916546e-06]\n",
      "\ttrain acc:43.6\n",
      "\ttest acc   :41.0\n",
      "\tloss       : 2.184879\n",
      "\tlr rate:[1.5228349094674645e-06]\n",
      "\ttrain acc:43.6\n",
      "\ttest acc   :41.0\n",
      "\tloss       : 0.144156\n",
      "\tlr rate:[1.5151438629097832e-06]\n",
      "\ttrain acc:43.6\n",
      "\ttest acc   :41.0\n",
      "\tloss       : 2.184531\n",
      "\tlr rate:[1.5075301129140054e-06]\n"
     ]
    }
   ],
   "source": [
    "train_model = Train_nn(784, [10], 10, lr= 0.3, dont_decay = True, l2_reg=0.000)\n",
    "train_model.fit(train_loader, test_loader, epochs=100, store_grads=False, store_hessian=False, store_freq=10000,  store_gen_err=False, store_pt_loss=False, store_weights=False, freq_reduce_by = 100, freq_reduce_after=10000, fast_X_train=fast_X_train, fast_y_train=fast_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f47667-b0ae-42dd-bc8d-4dd9e5eb4c9b",
   "metadata": {},
   "source": [
    "## for corrupted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "06f67839-8422-42f2-97bf-ab2bbe270ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size:2000\n",
      "test data size:500\n",
      "X_mat shape:torch.Size([2000, 784]), y_mat shape:torch.Size([2000])\n"
     ]
    }
   ],
   "source": [
    "train_loaderr, test_loaderr, fast_X_trainr, fast_y_trainr = get_random_subset(train_data_all, test_data_all, corrupt_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "871545c2-d77b-46d5-a9ce-d23c0a97e6e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfa2888d7a8c40ae8fded4c08ac35d99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttrain acc:8.8\n",
      "\ttest acc   :10.0\n",
      "\tloss       : 2.057150\n",
      "\tlr rate:[0.3]\n",
      "\ttrain acc:10.65\n",
      "\ttest acc   :13.2\n",
      "\tloss       : 2.526260\n",
      "\tlr rate:[0.0002997002997002997]\n",
      "\ttrain acc:10.2\n",
      "\ttest acc   :11.6\n",
      "\tloss       : 5.509175\n",
      "\tlr rate:[0.00014992503748125936]\n",
      "\ttrain acc:10.25\n",
      "\ttest acc   :11.6\n",
      "\tloss       : 2.521321\n",
      "\tlr rate:[9.99666777740753e-05]\n",
      "\ttrain acc:10.25\n",
      "\ttest acc   :11.6\n",
      "\tloss       : 5.236142\n",
      "\tlr rate:[7.498125468632842e-05]\n",
      "\ttrain acc:10.3\n",
      "\ttest acc   :11.6\n",
      "\tloss       : 2.518581\n",
      "\tlr rate:[5.998800239952009e-05]\n",
      "\ttrain acc:10.3\n",
      "\ttest acc   :11.6\n",
      "\tloss       : 5.094241\n",
      "\tlr rate:[4.9991668055324116e-05]\n",
      "\ttrain acc:10.3\n",
      "\ttest acc   :11.4\n",
      "\tloss       : 2.516590\n",
      "\tlr rate:[4.2851021282673906e-05]\n",
      "\ttrain acc:10.35\n",
      "\ttest acc   :11.6\n",
      "\tloss       : 5.005261\n",
      "\tlr rate:[3.749531308586426e-05]\n",
      "\ttrain acc:10.25\n",
      "\ttest acc   :11.4\n",
      "\tloss       : 2.515024\n",
      "\tlr rate:[3.3329630041106545e-05]\n",
      "\ttrain acc:10.3\n",
      "\ttest acc   :11.2\n",
      "\tloss       : 4.938525\n",
      "\tlr rate:[2.9997000299970004e-05]\n",
      "\ttrain acc:10.4\n",
      "\ttest acc   :11.2\n",
      "\tloss       : 2.513749\n",
      "\tlr rate:[2.727024815925825e-05]\n",
      "\ttrain acc:10.25\n",
      "\ttest acc   :11.2\n",
      "\tloss       : 4.885887\n",
      "\tlr rate:[2.499791684026331e-05]\n",
      "\ttrain acc:10.4\n",
      "\ttest acc   :11.2\n",
      "\tloss       : 2.512633\n",
      "\tlr rate:[2.307514806553342e-05]\n",
      "\ttrain acc:10.35\n",
      "\ttest acc   :11.2\n",
      "\tloss       : 4.845410\n",
      "\tlr rate:[2.142704092564817e-05]\n",
      "\ttrain acc:10.5\n",
      "\ttest acc   :11.2\n",
      "\tloss       : 2.511670\n",
      "\tlr rate:[1.9998666755549627e-05]\n",
      "\ttrain acc:10.65\n",
      "\ttest acc   :11.4\n",
      "\tloss       : 4.809369\n",
      "\tlr rate:[1.874882819823761e-05]\n",
      "\ttrain acc:10.7\n",
      "\ttest acc   :11.4\n",
      "\tloss       : 2.510818\n",
      "\tlr rate:[1.764602082230457e-05]\n",
      "\ttrain acc:10.8\n",
      "\ttest acc   :11.4\n",
      "\tloss       : 4.778718\n",
      "\tlr rate:[1.666574079217821e-05]\n",
      "\ttrain acc:10.9\n",
      "\ttest acc   :11.0\n",
      "\tloss       : 2.510086\n",
      "\tlr rate:[1.578864270301563e-05]\n",
      "\ttrain acc:10.75\n",
      "\ttest acc   :11.2\n",
      "\tloss       : 4.749840\n",
      "\tlr rate:[1.4999250037498124e-05]\n",
      "\ttrain acc:10.75\n",
      "\ttest acc   :11.0\n",
      "\tloss       : 2.509446\n",
      "\tlr rate:[1.428503404599781e-05]\n",
      "\ttrain acc:10.8\n",
      "\ttest acc   :11.2\n",
      "\tloss       : 4.724092\n",
      "\tlr rate:[1.3635743829825917e-05]\n",
      "\ttrain acc:10.8\n",
      "\ttest acc   :10.8\n",
      "\tloss       : 2.508858\n",
      "\tlr rate:[1.3042911177774879e-05]\n",
      "\ttrain acc:10.75\n",
      "\ttest acc   :10.8\n",
      "\tloss       : 4.701060\n",
      "\tlr rate:[1.2499479188367151e-05]\n",
      "\ttrain acc:10.8\n",
      "\ttest acc   :10.8\n",
      "\tloss       : 2.508303\n",
      "\tlr rate:[1.199952001919923e-05]\n",
      "\ttrain acc:10.8\n",
      "\ttest acc   :10.8\n",
      "\tloss       : 4.680769\n",
      "\tlr rate:[1.1538017768547364e-05]\n",
      "\ttrain acc:10.95\n",
      "\ttest acc   :11.0\n",
      "\tloss       : 2.507792\n",
      "\tlr rate:[1.111069960371838e-05]\n",
      "\ttrain acc:10.85\n",
      "\ttest acc   :11.0\n",
      "\tloss       : 4.662710\n",
      "\tlr rate:[1.0713903074890182e-05]\n",
      "\ttrain acc:11.05\n",
      "\ttest acc   :11.0\n",
      "\tloss       : 2.507319\n",
      "\tlr rate:[1.0344470880314471e-05]\n",
      "\ttrain acc:11.05\n",
      "\ttest acc   :11.0\n",
      "\tloss       : 4.646162\n",
      "\tlr rate:[9.999666677777408e-06]\n",
      "\ttrain acc:11.25\n",
      "\ttest acc   :10.8\n",
      "\tloss       : 2.506880\n",
      "\tlr rate:[9.677107190090643e-06]\n",
      "\ttrain acc:11.15\n",
      "\ttest acc   :10.8\n",
      "\tloss       : 4.631295\n",
      "\tlr rate:[9.374707040404987e-06]\n",
      "\ttrain acc:11.3\n",
      "\ttest acc   :10.8\n",
      "\tloss       : 2.506470\n",
      "\tlr rate:[9.090633617163116e-06]\n",
      "\ttrain acc:11.15\n",
      "\ttest acc   :10.8\n",
      "\tloss       : 4.617561\n",
      "\tlr rate:[8.823269903826358e-06]\n",
      "\ttrain acc:11.2\n",
      "\ttest acc   :10.8\n",
      "\tloss       : 2.506084\n",
      "\tlr rate:[8.571183680466272e-06]\n",
      "\ttrain acc:11.15\n",
      "\ttest acc   :10.6\n",
      "\tloss       : 4.605040\n",
      "\tlr rate:[8.333101858281713e-06]\n",
      "\ttrain acc:11.15\n",
      "\ttest acc   :10.6\n",
      "\tloss       : 2.505711\n",
      "\tlr rate:[8.107888975973621e-06]\n",
      "\ttrain acc:11.15\n",
      "\ttest acc   :10.6\n",
      "\tloss       : 4.593503\n",
      "\tlr rate:[7.894529091339701e-06]\n",
      "\ttrain acc:11.2\n",
      "\ttest acc   :10.2\n",
      "\tloss       : 2.505357\n",
      "\tlr rate:[7.692110458706186e-06]\n",
      "\ttrain acc:11.15\n",
      "\ttest acc   :10.2\n",
      "\tloss       : 4.583095\n",
      "\tlr rate:[7.499812504687382e-06]\n",
      "\ttrain acc:11.05\n",
      "\ttest acc   :10.2\n",
      "\tloss       : 2.505011\n",
      "\tlr rate:[7.316894709885124e-06]\n",
      "\ttrain acc:11.15\n",
      "\ttest acc   :10.2\n",
      "\tloss       : 4.572834\n",
      "\tlr rate:[7.142687078879074e-06]\n",
      "\ttrain acc:11.15\n",
      "\ttest acc   :10.0\n",
      "\tloss       : 2.504679\n",
      "\tlr rate:[6.976581939954884e-06]\n",
      "\ttrain acc:11.2\n",
      "\ttest acc   :10.0\n",
      "\tloss       : 4.562833\n",
      "\tlr rate:[6.81802686302584e-06]\n",
      "\ttrain acc:11.2\n",
      "\ttest acc   :10.0\n",
      "\tloss       : 2.504362\n",
      "\tlr rate:[6.666518521810627e-06]\n",
      "\ttrain acc:11.2\n",
      "\ttest acc   :9.8\n",
      "\tloss       : 4.553750\n",
      "\tlr rate:[6.521597356579204e-06]\n",
      "\ttrain acc:11.2\n",
      "\ttest acc   :9.8\n",
      "\tloss       : 2.504056\n",
      "\tlr rate:[6.382842918235782e-06]\n",
      "\ttrain acc:11.15\n",
      "\ttest acc   :9.8\n",
      "\tloss       : 4.545493\n",
      "\tlr rate:[6.249869794379284e-06]\n",
      "\ttrain acc:11.15\n",
      "\ttest acc   :9.8\n",
      "\tloss       : 2.503747\n",
      "\tlr rate:[6.122324034203383e-06]\n",
      "\ttrain acc:11.25\n",
      "\ttest acc   :9.8\n",
      "\tloss       : 4.537500\n",
      "\tlr rate:[5.999880002399952e-06]\n",
      "\ttrain acc:11.15\n",
      "\ttest acc   :9.6\n",
      "\tloss       : 2.503452\n",
      "\tlr rate:[5.88223760318425e-06]\n",
      "\ttrain acc:11.15\n",
      "\ttest acc   :9.4\n",
      "\tloss       : 4.529907\n",
      "\tlr rate:[5.769119824618758e-06]\n",
      "\ttrain acc:11.1\n",
      "\ttest acc   :9.4\n",
      "\tloss       : 2.503172\n",
      "\tlr rate:[5.660270560932812e-06]\n",
      "\ttrain acc:11.05\n",
      "\ttest acc   :9.4\n",
      "\tloss       : 4.522645\n",
      "\tlr rate:[5.555452676802281e-06]\n",
      "\ttrain acc:11.1\n",
      "\ttest acc   :9.4\n",
      "\tloss       : 2.502903\n",
      "\tlr rate:[5.454446282794858e-06]\n",
      "\ttrain acc:11.05\n",
      "\ttest acc   :9.4\n",
      "\tloss       : 4.515675\n",
      "\tlr rate:[5.3570471955857935e-06]\n",
      "\ttrain acc:11.15\n",
      "\ttest acc   :9.4\n",
      "\tloss       : 2.502641\n",
      "\tlr rate:[5.263065560253328e-06]\n",
      "\ttrain acc:11.1\n",
      "\ttest acc   :9.4\n",
      "\tloss       : 4.509014\n",
      "\tlr rate:[5.172324615092844e-06]\n",
      "\ttrain acc:11.2\n",
      "\ttest acc   :9.4\n",
      "\tloss       : 2.502392\n",
      "\tlr rate:[5.084659582040982e-06]\n",
      "\ttrain acc:11.1\n",
      "\ttest acc   :9.2\n",
      "\tloss       : 4.502796\n",
      "\tlr rate:[4.999916668055532e-06]\n",
      "\ttrain acc:11.1\n",
      "\ttest acc   :8.8\n",
      "\tloss       : 2.502151\n",
      "\tlr rate:[4.917952164718611e-06]\n",
      "\ttrain acc:11.05\n",
      "\ttest acc   :8.8\n",
      "\tloss       : 4.496869\n",
      "\tlr rate:[4.8386316349736296e-06]\n",
      "\ttrain acc:11.1\n",
      "\ttest acc   :8.6\n",
      "\tloss       : 2.501917\n",
      "\tlr rate:[4.761829177314646e-06]\n",
      "\ttrain acc:10.95\n",
      "\ttest acc   :8.6\n",
      "\tloss       : 4.491553\n",
      "\tlr rate:[4.687426758956891e-06]\n",
      "\ttrain acc:11.05\n",
      "\ttest acc   :8.4\n",
      "\tloss       : 2.501693\n",
      "\tlr rate:[4.615313610559837e-06]\n",
      "\ttrain acc:10.95\n",
      "\ttest acc   :8.4\n",
      "\tloss       : 4.486435\n",
      "\tlr rate:[4.545385675974606e-06]\n",
      "\ttrain acc:11.05\n",
      "\ttest acc   :8.4\n",
      "\tloss       : 2.501471\n",
      "\tlr rate:[4.4775451112669955e-06]\n",
      "\ttrain acc:11.0\n",
      "\ttest acc   :8.4\n",
      "\tloss       : 4.481481\n",
      "\tlr rate:[4.4116998279437065e-06]\n",
      "\ttrain acc:10.95\n",
      "\ttest acc   :8.4\n",
      "\tloss       : 2.501258\n",
      "\tlr rate:[4.34776307589745e-06]\n",
      "\ttrain acc:11.0\n",
      "\ttest acc   :8.4\n",
      "\tloss       : 4.476878\n",
      "\tlr rate:[4.285653062099113e-06]\n",
      "\ttrain acc:10.95\n",
      "\ttest acc   :8.4\n",
      "\tloss       : 2.501052\n",
      "\tlr rate:[4.225292601512655e-06]\n",
      "\ttrain acc:11.0\n",
      "\ttest acc   :8.4\n",
      "\tloss       : 4.472430\n",
      "\tlr rate:[4.16660879710004e-06]\n",
      "\ttrain acc:10.95\n",
      "\ttest acc   :8.4\n",
      "\tloss       : 2.500852\n",
      "\tlr rate:[4.1095327461267655e-06]\n",
      "\ttrain acc:10.95\n",
      "\ttest acc   :8.4\n",
      "\tloss       : 4.468125\n",
      "\tlr rate:[4.053999270280131e-06]\n",
      "\ttrain acc:10.9\n",
      "\ttest acc   :8.4\n",
      "\tloss       : 2.500656\n",
      "\tlr rate:[3.999946667377768e-06]\n",
      "\ttrain acc:10.95\n",
      "\ttest acc   :8.2\n",
      "\tloss       : 4.463969\n",
      "\tlr rate:[3.94731648267786e-06]\n",
      "\ttrain acc:10.9\n",
      "\ttest acc   :8.2\n",
      "\tloss       : 2.500466\n",
      "\tlr rate:[3.896053298009117e-06]\n",
      "\ttrain acc:10.85\n",
      "\ttest acc   :8.0\n",
      "\tloss       : 4.459963\n",
      "\tlr rate:[3.846104537121319e-06]\n",
      "\ttrain acc:10.9\n",
      "\ttest acc   :8.0\n",
      "\tloss       : 2.500282\n",
      "\tlr rate:[3.797420285819167e-06]\n",
      "\ttrain acc:10.8\n",
      "\ttest acc   :7.8\n",
      "\tloss       : 4.456073\n",
      "\tlr rate:[3.7499531255859297e-06]\n",
      "\ttrain acc:10.75\n",
      "\ttest acc   :7.8\n",
      "\tloss       : 2.500103\n",
      "\tlr rate:[3.7036579795311167e-06]\n",
      "\ttrain acc:10.8\n",
      "\ttest acc   :7.8\n",
      "\tloss       : 4.452377\n",
      "\tlr rate:[3.658491969610127e-06]\n",
      "\ttrain acc:10.8\n",
      "\ttest acc   :7.8\n",
      "\tloss       : 2.499929\n",
      "\tlr rate:[3.614414284165251e-06]\n",
      "\ttrain acc:10.75\n",
      "\ttest acc   :7.8\n",
      "\tloss       : 4.448784\n",
      "\tlr rate:[3.5713860549279176e-06]\n",
      "\ttrain acc:10.75\n",
      "\ttest acc   :7.8\n",
      "\tloss       : 2.499758\n",
      "\tlr rate:[3.5293702427030266e-06]\n",
      "\ttrain acc:10.8\n",
      "\ttest acc   :8.0\n",
      "\tloss       : 4.445289\n",
      "\tlr rate:[3.4883315310287087e-06]\n",
      "\ttrain acc:10.75\n",
      "\ttest acc   :7.8\n",
      "\tloss       : 2.499593\n",
      "\tlr rate:[3.4482362271698027e-06]\n",
      "\ttrain acc:10.7\n",
      "\ttest acc   :8.0\n",
      "\tloss       : 4.441900\n",
      "\tlr rate:[3.4090521698617062e-06]\n",
      "\ttrain acc:10.7\n",
      "\ttest acc   :8.2\n",
      "\tloss       : 2.499429\n",
      "\tlr rate:[3.370748643273671e-06]\n",
      "\ttrain acc:10.75\n",
      "\ttest acc   :8.2\n",
      "\tloss       : 4.438729\n",
      "\tlr rate:[3.3332962967078142e-06]\n",
      "\ttrain acc:10.85\n",
      "\ttest acc   :8.2\n",
      "\tloss       : 2.499269\n",
      "\tlr rate:[3.296667069592642e-06]\n",
      "\ttrain acc:10.9\n",
      "\ttest acc   :8.2\n",
      "\tloss       : 4.435659\n",
      "\tlr rate:[3.260834121368246e-06]\n",
      "\ttrain acc:10.9\n",
      "\ttest acc   :8.2\n",
      "\tloss       : 2.499110\n",
      "\tlr rate:[3.2257717658949903e-06]\n",
      "\ttrain acc:10.9\n",
      "\ttest acc   :8.2\n",
      "\tloss       : 4.432670\n",
      "\tlr rate:[3.191455410048829e-06]\n",
      "\ttrain acc:10.9\n",
      "\ttest acc   :8.2\n",
      "\tloss       : 2.498956\n",
      "\tlr rate:[3.157861496194777e-06]\n",
      "\ttrain acc:10.9\n",
      "\ttest acc   :8.2\n",
      "\tloss       : 4.429750\n",
      "\tlr rate:[3.1249674482557474e-06]\n",
      "\ttrain acc:10.9\n",
      "\ttest acc   :8.2\n",
      "\tloss       : 2.498804\n",
      "\tlr rate:[3.092751621117308e-06]\n",
      "\ttrain acc:10.85\n",
      "\ttest acc   :8.2\n",
      "\tloss       : 4.426901\n",
      "\tlr rate:[3.06119325313007e-06]\n",
      "\ttrain acc:10.9\n",
      "\ttest acc   :8.2\n",
      "\tloss       : 2.498655\n",
      "\tlr rate:[3.030272421490692e-06]\n",
      "\ttrain acc:10.9\n",
      "\ttest acc   :8.2\n",
      "\tloss       : 4.424139\n",
      "\tlr rate:[2.999970000299997e-06]\n",
      "\ttrain acc:10.95\n",
      "\ttest acc   :8.2\n",
      "\tloss       : 2.498509\n",
      "\tlr rate:[2.970267621112662e-06]\n",
      "\ttrain acc:10.95\n",
      "\ttest acc   :8.2\n",
      "\tloss       : 4.421332\n",
      "\tlr rate:[2.941147635807492e-06]\n",
      "\ttrain acc:10.9\n",
      "\ttest acc   :8.2\n",
      "\tloss       : 2.498365\n",
      "\tlr rate:[2.912593081620567e-06]\n",
      "\ttrain acc:10.9\n",
      "\ttest acc   :8.2\n",
      "\tloss       : 4.418589\n",
      "\tlr rate:[2.8845876481956904e-06]\n",
      "\ttrain acc:10.9\n",
      "\ttest acc   :8.2\n",
      "\tloss       : 2.498224\n",
      "\tlr rate:[2.8571156465176523e-06]\n",
      "\ttrain acc:10.9\n",
      "\ttest acc   :8.2\n",
      "\tloss       : 4.415904\n",
      "\tlr rate:[2.8301619796039657e-06]\n",
      "\ttrain acc:10.95\n",
      "\ttest acc   :8.2\n",
      "\tloss       : 2.498087\n",
      "\tlr rate:[2.8037121148400484e-06]\n",
      "\ttrain acc:10.9\n",
      "\ttest acc   :8.2\n",
      "\tloss       : 4.413278\n",
      "\tlr rate:[2.777752057851316e-06]\n",
      "\ttrain acc:10.9\n",
      "\ttest acc   :8.2\n",
      "\tloss       : 2.497953\n",
      "\tlr rate:[2.752268327813506e-06]\n",
      "\ttrain acc:10.85\n",
      "\ttest acc   :8.0\n",
      "\tloss       : 4.410695\n",
      "\tlr rate:[2.72724793410969e-06]\n",
      "\ttrain acc:10.85\n",
      "\ttest acc   :8.0\n",
      "\tloss       : 2.497822\n",
      "\tlr rate:[2.7026783542490607e-06]\n",
      "\ttrain acc:10.9\n",
      "\ttest acc   :8.0\n",
      "\tloss       : 4.408166\n",
      "\tlr rate:[2.6785475129686345e-06]\n",
      "\ttrain acc:10.95\n",
      "\ttest acc   :8.0\n",
      "\tloss       : 2.497692\n",
      "\tlr rate:[2.65484376244458e-06]\n",
      "\ttrain acc:11.05\n",
      "\ttest acc   :8.0\n",
      "\tloss       : 4.405689\n",
      "\tlr rate:[2.6315558635450564e-06]\n",
      "\ttrain acc:10.95\n",
      "\ttest acc   :8.0\n",
      "\tloss       : 2.497564\n",
      "\tlr rate:[2.608672968061147e-06]\n",
      "\ttrain acc:11.0\n",
      "\ttest acc   :8.0\n",
      "\tloss       : 4.403266\n",
      "\tlr rate:[2.5861846018568804e-06]\n",
      "\ttrain acc:10.95\n",
      "\ttest acc   :8.0\n",
      "\tloss       : 2.497438\n",
      "\tlr rate:[2.564080648883343e-06]\n",
      "\ttrain acc:10.95\n",
      "\ttest acc   :8.0\n",
      "\tloss       : 4.400875\n",
      "\tlr rate:[2.542351336005627e-06]\n",
      "\ttrain acc:10.9\n",
      "\ttest acc   :8.0\n",
      "\tloss       : 2.497314\n",
      "\tlr rate:[2.520987218594802e-06]\n",
      "\ttrain acc:10.95\n",
      "\ttest acc   :8.0\n",
      "\tloss       : 4.398542\n",
      "\tlr rate:[2.4999791668402764e-06]\n",
      "\ttrain acc:10.9\n",
      "\ttest acc   :8.0\n",
      "\tloss       : 2.497193\n",
      "\tlr rate:[2.4793183527408864e-06]\n",
      "\ttrain acc:10.9\n",
      "\ttest acc   :8.0\n",
      "\tloss       : 4.396249\n",
      "\tlr rate:[2.4589962377357564e-06]\n",
      "\ttrain acc:10.95\n",
      "\ttest acc   :8.0\n",
      "\tloss       : 2.497072\n",
      "\tlr rate:[2.4390045609385285e-06]\n",
      "\ttrain acc:10.95\n",
      "\ttest acc   :8.0\n",
      "\tloss       : 4.394000\n",
      "\tlr rate:[2.419335327940904e-06]\n",
      "\ttrain acc:10.95\n",
      "\ttest acc   :7.8\n",
      "\tloss       : 2.496953\n",
      "\tlr rate:[2.3999808001535985e-06]\n",
      "\ttrain acc:10.95\n",
      "\ttest acc   :8.0\n",
      "\tloss       : 4.391795\n",
      "\tlr rate:[2.3809334846548834e-06]\n",
      "\ttrain acc:10.95\n",
      "\ttest acc   :7.8\n",
      "\tloss       : 2.496837\n",
      "\tlr rate:[2.3621861245187044e-06]\n",
      "\ttrain acc:10.95\n",
      "\ttest acc   :7.8\n",
      "\tloss       : 4.389633\n",
      "\tlr rate:[2.343731689596175e-06]\n",
      "\ttrain acc:10.95\n",
      "\ttest acc   :7.8\n",
      "\tloss       : 2.496722\n",
      "\tlr rate:[2.3255633677258314e-06]\n",
      "\ttrain acc:10.9\n",
      "\ttest acc   :7.8\n",
      "\tloss       : 4.387507\n",
      "\tlr rate:[2.3076745563495667e-06]\n",
      "\ttrain acc:10.7\n",
      "\ttest acc   :7.8\n",
      "\tloss       : 2.496608\n",
      "\tlr rate:[2.2900588545125607e-06]\n",
      "\ttrain acc:10.7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [38]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m train_model \u001b[38;5;241m=\u001b[39m Train_nn(\u001b[38;5;241m784\u001b[39m, [\u001b[38;5;241m10\u001b[39m], \u001b[38;5;241m10\u001b[39m, lr\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.3\u001b[39m, dont_decay \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, l2_reg\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.000\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loaderr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loaderr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_grads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_hessian\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mstore_gen_err\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_pt_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq_reduce_by\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq_reduce_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfast_X_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfast_X_trainr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfast_y_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfast_y_trainr\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [36]\u001b[0m, in \u001b[0;36mTrain_nn.fit\u001b[0;34m(self, train_loader, test_loader, epochs, store_grads, store_hessian, store_gen_err, store_weights, store_pt_loss, store_freq, freq_reduce_by, freq_reduce_after, fast_X_train, fast_y_train)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mtrain acc:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[1;32m    355\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    356\u001b[0m         pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(X)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1359\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1359\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1362\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1325\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1325\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1326\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1327\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1163\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1161\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1167\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1168\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/multiprocessing/connection.py:262\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/multiprocessing/connection.py:429\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 429\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/multiprocessing/connection.py:936\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    933\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model = Train_nn(784, [10], 10, lr= 0.3, dont_decay = False, l2_reg=0.000)\n",
    "train_model.fit(train_loaderr, test_loaderr, epochs=100, store_grads=False, store_hessian=False, store_freq=10000,  store_gen_err=False, store_pt_loss=False, store_weights=False, freq_reduce_by = 100, freq_reduce_after=10000, fast_X_train=fast_X_trainr, fast_y_train=fast_y_trainr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7981fcbc-1043-413e-80bf-e133f788c99a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 9, 6, 1, 8, 7, 8, 8, 6])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(10, (9,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0099a645-3ecb-4551-ac2f-3e731a92a1f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 2, 5, 5, 5, 1, 6, 7, 4])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9e496f4a-75a2-472f-bfe1-f148ab2dffdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8, 9, 7,  ..., 6, 9, 4])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_corrupted = fast_y_train[np.random.permutation(len(fast_y_train))]\n",
    "y_corrupted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23c480b-e896-44ef-8413-90082cf260f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a26201-4203-4e7c-a98a-b0592cf081d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599f5e5b-8904-4575-8e4b-a1d057dea25a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
