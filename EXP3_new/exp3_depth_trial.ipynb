{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10112785-12f1-4a55-a54e-c1b4aa3dc894",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Exp3 trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30e536cf-39e8-4421-a722-548ceacc1e5e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected weight:[9, 9, 9, 9]\n",
      "train data size:2000\n",
      "test data size:256\n",
      "Time:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e290fbe1ef404e83f2e52364581c43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 390>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    386\u001b[0m         grad_list\u001b[38;5;241m.\u001b[39mappend(train_model\u001b[38;5;241m.\u001b[39mgrads_norms)\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grad_list, hess_norm_list\n\u001b[0;32m--> 390\u001b[0m grad_list, hess_norm_list \u001b[38;5;241m=\u001b[39m \u001b[43mexp_get_lp_sm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m              \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mg_weight\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mg_times\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m              \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mg_epochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresult_root_dir\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m              \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresult_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq_reduce_by\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfreq_reduce_by\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m              \u001b[49m\u001b[43mfreq_reduce_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfreq_reduce_after\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36mexp_get_lp_sm\u001b[0;34m(train_data_all, test_data_all, op_features, weight, times, epochs, root_dir, path, clear_file, freq_reduce_by, freq_reduce_after)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    377\u001b[0m train_model \u001b[38;5;241m=\u001b[39m Train_nn(\u001b[38;5;241m784\u001b[39m, weight, op_features, lr\u001b[38;5;241m=\u001b[39m details[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha_0\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 378\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_grads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_hessian\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbook_keep_freq\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mstore_gen_err\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_pt_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq_reduce_by\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfreq_reduce_by\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq_reduce_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreq_reduce_after\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(grad_file_path,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    381\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(grad) \u001b[38;5;28;01mfor\u001b[39;00m grad \u001b[38;5;129;01min\u001b[39;00m train_model\u001b[38;5;241m.\u001b[39mgrads_norms]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36mTrain_nn.fit\u001b[0;34m(self, train_loader, test_loader, epochs, store_grads, store_hessian, store_gen_err, store_weights, store_pt_loss, store_freq, freq_reduce_by, freq_reduce_after)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m store_hessian \u001b[38;5;129;01mand\u001b[39;00m (batch\u001b[38;5;241m%\u001b[39m store_freq\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;66;03m#assert False, \"Not implemented\"\u001b[39;00m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhess_norms\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_hessianv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    317\u001b[0m     store_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m store_count\u001b[38;5;241m%\u001b[39mfreq_reduce_after\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36mTrain_nn.get_hessianv2\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    246\u001b[0m     local_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(out, y)\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m local_loss\n\u001b[0;32m--> 248\u001b[0m hess_mat \u001b[38;5;241m=\u001b[39m \u001b[43mhessian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_fun_hess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# print(f'len of hess mat:{len(hess_mat)}')\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# print(f'hess_mat[0] shape:{len(hess_mat[0])}')\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# print(f'hess_mat[0][0] shape:{hess_mat[0][0].shape}')\u001b[39;00m\n\u001b[1;32m    252\u001b[0m hess_norm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/functional.py:808\u001b[0m, in \u001b[0;36mhessian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, outer_jacobian_strategy)\u001b[0m\n\u001b[1;32m    805\u001b[0m     _check_requires_grad(jac, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacobian\u001b[39m\u001b[38;5;124m\"\u001b[39m, strict\u001b[38;5;241m=\u001b[39mstrict)\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jac\n\u001b[0;32m--> 808\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mjacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjac_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvectorize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m               \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mouter_jacobian_strategy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _tuple_postprocess(res, (is_inputs_tuple, is_inputs_tuple))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/functional.py:670\u001b[0m, in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[1;32m    668\u001b[0m jac_i: Tuple[List[torch\u001b[38;5;241m.\u001b[39mTensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m([] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs)))  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(out\u001b[38;5;241m.\u001b[39mnelement()):\n\u001b[0;32m--> 670\u001b[0m     vj \u001b[38;5;241m=\u001b[39m \u001b[43m_autograd_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m el_idx, (jac_i_el, vj_el, inp_el) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(jac_i, vj, inputs)):\n\u001b[1;32m    674\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m vj_el \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/functional.py:159\u001b[0m, in \u001b[0;36m_autograd_grad\u001b[0;34m(outputs, inputs, grad_outputs, create_graph, retain_graph, is_grads_batched)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m,) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_grad_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_grads_batched\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:276\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd.functional import hessian\n",
    "import torch.utils.data as data_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from tqdm.notebook import tqdm \n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import seaborn as sns\n",
    "import os\n",
    "import copy\n",
    "from torch.nn.utils import _stateless\n",
    "\n",
    "batch_size = 1\n",
    "num_workers = 1\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# device\n",
    "\n",
    "details = {}\n",
    "details['use_db'] = 'mnist'\n",
    "details['result_root_dir']='results/t0/'\n",
    "details['result_path']='try1_t8_w9999'\n",
    "# details['ratio'] = 15\n",
    "details['book_keep_freq'] = 20\n",
    "details['g_times'] = 8\n",
    "details['g_epochs'] = 1\n",
    "details['alpha_0']= 0.001\n",
    "details['freq_reduce_by'] = 20\n",
    "details['freq_reduce_after'] = 100\n",
    "\n",
    "details['training_step_limit'] = 2000000 ## this is to train for max updates per epochs\n",
    "details['stop_hess_computation'] = 1000000 ## Stop computing hessian after calculated these many times\n",
    "\n",
    "details['g_weight'] = [9,9,9,9]\n",
    "print(f'selected weight:{details[\"g_weight\"]}')\n",
    "\n",
    "with open(details['result_root_dir']+'details_'+details['result_path']+'.txt', 'w+') as f:\n",
    "    for key, val in details.items():\n",
    "        content = key + ' : '+str(val) + '\\n'\n",
    "        f.write(content)\n",
    "        \n",
    "torch.manual_seed(3407)\n",
    "np.random.seed(3407)\n",
    "torch.cuda.manual_seed_all(3407)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "# os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":4096:2\"\n",
    "\n",
    "train_data_all = torchvision.datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "test_data_all = torchvision.datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "# print(f'train data:{train_data}')\n",
    "# print(f'test data:{test_data}')\n",
    "\n",
    "def get_random_subset(train_data_all, test_data_all):    \n",
    "    # train_indices = torch.arange(20000)\n",
    "    test_indices = torch.arange(256)\n",
    "    train_indices = torch.randint(60000-1, (2000,))\n",
    "    # print(f'train indices:{train_indices[:10]}')\n",
    "    train_data = data_utils.Subset(train_data_all, train_indices)\n",
    "    test_data = data_utils.Subset(test_data_all, test_indices)\n",
    "    # print(f'train data:{train_data}')\n",
    "    # print(f'test data:{test_data}')\n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(0)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g,\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_data,\n",
    "        batch_size=256,\n",
    "        num_workers=num_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g,\n",
    "    )\n",
    "    print(f'train data size:{len(train_loader.dataset)}')\n",
    "    print(f'test data size:{len(test_loader.dataset)}')\n",
    "    # X_mat, y_mat = torch.Tensor(len(train_loader.dataset),784), torch.Tensor(len(train_loader.dataset)).long()\n",
    "    # for i, (data, label) in enumerate(train_loader):\n",
    "    #     X_mat[i] = data.flatten()\n",
    "    #     y_mat[i] = label.flatten()\n",
    "    # print(f'X_mat shape:{X_mat.shape}, y_mat shape:{y_mat.shape}')\n",
    "    return train_loader, test_loader #, X_mat, y_mat\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_features, hidden_layers, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = len(hidden_layers) + 1\n",
    "        self.total_params_len = 0\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        prev_weight = input_features\n",
    "        \n",
    "        for i, weight in enumerate(hidden_layers):\n",
    "            self.fc_layers.append(nn.Linear(prev_weight, weight))\n",
    "            self.total_params_len += prev_weight*weight + weight\n",
    "            prev_weight = weight\n",
    "        \n",
    "        self.fc_last = nn.Linear(hidden_layers[-1], output_size)\n",
    "        self.total_params_len += hidden_layers[-1]*output_size + output_size\n",
    "        \n",
    "        ### Others required params\n",
    "        self.param_list = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        # print('x shape in forward',x.shape)\n",
    "        for fc_layer in self.fc_layers:\n",
    "            x = F.relu(fc_layer(x))\n",
    "        x = self.fc_last(x)\n",
    "        return x\n",
    "    \n",
    "    def fit(self, X, Y, X_val, Y_val, epochs, batch_size=1, **kwargs):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.SGD(self.parameters())\n",
    "        \n",
    "\n",
    "class Train_nn:\n",
    "    \n",
    "    def __init__(self, input_features, hidden_layers, output_size, lr):\n",
    "        self.model = Net(input_features, hidden_layers=hidden_layers, output_size=output_size)\n",
    "        self.model.to(device)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda= lambda it: 1/(it+1))\n",
    "        \n",
    "    def get_loss(self, X, y, params=None):\n",
    "        # if params is not None:\n",
    "        assert False, \"Model not initialized with given params\"\n",
    "        op = self.model(X)\n",
    "        loss = self.loss_fn(pred, y)\n",
    "        return loss\n",
    "    \n",
    "    def get_gradient(self):\n",
    "        params = (self.model.parameters())\n",
    "        grad_norm_sq = torch.tensor(0, dtype=float).to(device)\n",
    "        # print('grad Norm init:', grad_norm_sq)\n",
    "        for param in self.model.parameters():\n",
    "            temp = param.grad.data.pow(2).sum()\n",
    "            # print(f'param grad norm \\n\\tsum:{temp.data}')#,\\n\\tshape:{param.shape}')\n",
    "            grad_norm_sq += temp\n",
    "            \n",
    "        return grad_norm_sq.sqrt().cpu()\n",
    "    \n",
    "    def get_gradientv2(self, X, y):\n",
    "        names = list(n for n, _ in self.model.named_parameters())\n",
    "        def loss_fun_grad(*params):\n",
    "            out: torch.Tensor = _stateless.functional_call(self.model, {n: p for n, p in zip(names, params)}, X)\n",
    "            local_loss = self.loss_fn(out, y)\n",
    "            return local_loss\n",
    "        grad_mat = torch.autograd.grad(loss_fun_grad, tuple(self.model.parameters()))\n",
    "        # print(f'len of hess mat:{len(hess_mat)}')\n",
    "        # print(f'hess_mat[0] shape:{len(hess_mat[0])}')\n",
    "        # print(f'hess_mat[0][0] shape:{hess_mat[0][0].shape}')\n",
    "        grad_norm = torch.tensor(0.).to(device)\n",
    "        for i in range(len(grad_mat)):\n",
    "            for j in range(len(grad_mat[0])):\n",
    "                grad_norm+= grad_mat[i][j].pow(2).sum()\n",
    "        grad_norm = grad_norm.sqrt()\n",
    "        # print(f'v2 hess norm{hess_norm}')\n",
    "        return grad_norm.cpu()\n",
    "    \n",
    "    def try_operator_norm(self, hess_mat):\n",
    "        for i in len(hess_mat):\n",
    "            for j in len(hess_mat[0]):\n",
    "                torch.unsqueeze(hess_mat[i][i],0)\n",
    "        hess_tensor_dim = list(hess_mat[0][0].shape)\n",
    "        hess_tensor_dim += [n*2,n*2]\n",
    "        hess_mat_np = np.zeros(shape=hess_tensor_dim)\n",
    "        hess_tensor = torch.tensor(hess_mat_np)\n",
    "        torch.cat(hess_mat, out=hess_tensor)\n",
    "        \n",
    "        hess_mat.reshpe(n*2,n*2)\n",
    "        hess_norm = torch.linalg.norm(hess_mat, 2)\n",
    "        assert False, \"Not working\"\n",
    "    \n",
    "    def get_hessian(self, X, y):\n",
    "        prev_params = copy.deepcopy(list(self.model.parameters()))\n",
    "        n = self.model.layers\n",
    "        def local_model(*params):\n",
    "            # print(f'len of params:{len(params)}')\n",
    "            # print(f'shape of params[0]:{params[0].shape}')\n",
    "            # with torch.no_grad():\n",
    "            #initialize model with given params\n",
    "            i = 0\n",
    "            for i, param in enumerate(self.model.parameters()):\n",
    "                param.data = params[i]\n",
    "            pred = self.model(X)\n",
    "            loss = self.loss_fn(pred, y)\n",
    "            # print(f'loss type:{type(loss)}')\n",
    "            return loss\n",
    "        p =list(self.model.parameters())\n",
    "        hess_mat = hessian(local_model, tuple(p))\n",
    "        hess_norm = torch.tensor(0.).to(device)\n",
    "        for i in range(len(hess_mat)):\n",
    "            for j in range(len(hess_mat[0])):\n",
    "                hess_norm+= hess_mat[i][j].pow(2).sum()\n",
    "        \n",
    "        # print(f'Hess mat len:{len(hess_mat)}')\n",
    "        # print(f'Hess mat[0] len:{len(hess_mat[0])}')\n",
    "        # print(f'Hess mat[0][0] shape:{hess_mat[0][0].shape}')\n",
    "        \n",
    "        hess_norm = hess_norm.sqrt()\n",
    "        # print(f'hess norm:{hess_norm}')\n",
    "        \n",
    "        # Reinitialize the original params to model\n",
    "        for i, param in enumerate(self.model.parameters()):\n",
    "                param.data = prev_params[i]\n",
    "        \n",
    "        return hess_norm\n",
    "    \n",
    "    def get_hessianv2(self, X,y):\n",
    "        names = list(n for n, _ in self.model.named_parameters())\n",
    "        def loss_fun_hess(*params):\n",
    "            out: torch.Tensor = _stateless.functional_call(self.model, {n: p for n, p in zip(names, params)}, X)\n",
    "            local_loss = self.loss_fn(out, y)\n",
    "            return local_loss\n",
    "        hess_mat = hessian(loss_fun_hess, tuple(self.model.parameters()))\n",
    "        # print(f'len of hess mat:{len(hess_mat)}')\n",
    "        # print(f'hess_mat[0] shape:{len(hess_mat[0])}')\n",
    "        # print(f'hess_mat[0][0] shape:{hess_mat[0][0].shape}')\n",
    "        hess_norm = torch.tensor(0.).to(device)\n",
    "        for i in range(len(hess_mat)):\n",
    "            for j in range(len(hess_mat[0])):\n",
    "                hess_norm+= hess_mat[i][j].pow(2).sum()\n",
    "        hess_norm = hess_norm.sqrt()\n",
    "        # print(f'v2 hess norm{hess_norm}')\n",
    "        return hess_norm.cpu()\n",
    "        \n",
    "    def fit(self, train_loader, test_loader, epochs, store_grads=False, store_hessian=False, store_gen_err=False, store_weights=False, store_pt_loss=True, store_freq = 20, freq_reduce_by=None, freq_reduce_after=None):\n",
    "        \n",
    "        ## For Book keeping results ##\n",
    "        self.grads_norms = []\n",
    "        self.grads_normsv2 = []\n",
    "        self.param_list = []\n",
    "        self.hess_norms = []\n",
    "        self.gen_err = []\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        self.point_loss = []\n",
    "        ## Initializing values ##\n",
    "        terminate_training = False\n",
    "        store_count = 0\n",
    "        \n",
    "        for epoch in tqdm(range(epochs), total=epochs, unit=\"epoch\", disable=True):\n",
    "            if terminate_training == True:\n",
    "                break\n",
    "            for batch, (X, y) in tqdm(enumerate(train_loader), total=len(train_loader), unit='batch'):\n",
    "                if batch>details['training_step_limit']:\n",
    "                    terminate_training = True\n",
    "                    break\n",
    "                \n",
    "                X, y =X.to(device), y.to(device)\n",
    "                pred = self.model(X)\n",
    "                loss = self.loss_fn(pred, y)\n",
    "\n",
    "                # Backpropagation\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                ## Saving point loss\n",
    "                if store_pt_loss and (batch%store_freq==0):\n",
    "                    self.point_loss.append(loss.item())\n",
    "                    \n",
    "                ## Saving the weights\n",
    "                if store_weights and (batch%store_freq==0):\n",
    "                    current_params = tuple(self.model.parameters())\n",
    "                    self.param_list.append(current_params)\n",
    "                \n",
    "                ## computing and saving the gradient\n",
    "                if store_grads and (batch% store_freq == 0):\n",
    "                    # store_count += 1\n",
    "                    # # print(f'\\tstore_freq:{store_freq}, batch:{batch}')\n",
    "                    # if store_count%freq_reduce_after==0:\n",
    "                    #     store_freq += freq_reduce_by\n",
    "                    #     # print(f'store freq:{store_freq}, batch:{batch}')\n",
    "                    grad_norm_per_update = self.get_gradient()\n",
    "                    # print('\\tgrad norm:', grad_norm_per_update)\n",
    "                    self.grads_norms.append(grad_norm_per_update)\n",
    "                    # self.grads_normsv2.append(self.get_gradientv2(X,y))\n",
    "                ## computing and saving hessian\n",
    "                if store_hessian and (batch% store_freq==0):\n",
    "                    #assert False, \"Not implemented\"\n",
    "                    self.optimizer.zero_grad()\n",
    "                    self.hess_norms.append(self.get_hessianv2(X,y))\n",
    "                    store_count += 1\n",
    "                    if store_count%freq_reduce_after==0:\n",
    "                        store_freq += freq_reduce_by\n",
    "                \n",
    "                ## computing and storing the generalization error\n",
    "                if store_gen_err and (batch% store_freq == 0):\n",
    "                    assert False, \"fix reducing freq to get it working and fastX, fasty\"\n",
    "                    train_loss, test_loss, point_loss=0, 0, 0\n",
    "                    with torch.no_grad():\n",
    "                        for sub_batch, (X_local,y_local) in enumerate(train_loader):\n",
    "                            # if sub_batch> batch: # only taking the encountered points to calculate train loss\n",
    "                            #     break\n",
    "                            X_local, y_local = X_local.to(device), y_local.to(device)\n",
    "                            pred_local = self.model(X_local)\n",
    "                            train_loss += self.loss_fn(pred_local, y_local).item()\n",
    "                    train_loss = train_loss/(batch+1)\n",
    "                    with torch.no_grad():\n",
    "                        for sub_batch, (X_local,y_local) in enumerate(test_loader):\n",
    "                            X_local, y_local = X_local.to(device), y_local.to(device)\n",
    "                            pred_local = self.model(X_local)\n",
    "                            test_loss += self.loss_fn(pred_local, y_local).item()\n",
    "                    test_batch_size = len(test_loader)\n",
    "                    # print(f\"Number of batches in test:{len(test_loader)}\")\n",
    "                    test_loss = test_loss/ len(test_loader)\n",
    "                    self.train_loss.append(train_loss)\n",
    "                    self.val_loss.append(test_loss)\n",
    "                \n",
    "                if batch % 10000 == 9999:\n",
    "                    loss, current = loss.item(), batch * len(X)\n",
    "                    correct = 0\n",
    "                    test_loss = 0\n",
    "                    with torch.no_grad():\n",
    "                        for X, y in test_loader:\n",
    "                            X, y = X.to(device), y.to(device)\n",
    "                            pred = self.model(X)\n",
    "                            test_loss += self.loss_fn(pred, y).item()\n",
    "                            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "                    acc = 100*correct/len(test_loader.dataset)\n",
    "                    print(f\"\\taccuracy:{acc}\")#, at batch:{batch}\")\n",
    "                    print(f\"\\tloss: {loss:>7f}\")\n",
    "                \n",
    "                    # print(f'Learning rate:{self.scheduler.get_last_lr()}')\n",
    "                self.scheduler.step()\n",
    "            \n",
    "def exp_get_lp_sm(train_data_all, test_data_all, op_features, weight = 10, times = 8, epochs = 1, root_dir='', path=None, clear_file = True, freq_reduce_by=10, freq_reduce_after=100):\n",
    "    grad_list        = []\n",
    "    hess_norm_list   = []\n",
    "    if path is not None:\n",
    "        grad_file_path = root_dir+'grad_'+path\n",
    "        hess_file_path = root_dir+'hess_'+path\n",
    "        # gen_file_path = root_dir+'gen_'+path\n",
    "        if clear_file:\n",
    "            with open(grad_file_path, 'w+') as f:\n",
    "                f.write('')\n",
    "            with open(hess_file_path, 'w+') as f:\n",
    "                f.write('')\n",
    "    \n",
    "    train_loader, test_loader = get_random_subset(train_data_all, test_data_all)\n",
    "    for t in range(times):\n",
    "        print(f'Time:{t}')\n",
    "        train_model = Train_nn(784, weight, op_features, lr= details['alpha_0'])\n",
    "        train_model.fit(train_loader, test_loader, epochs=epochs, store_grads=True, store_hessian=True, store_freq=details['book_keep_freq'],  store_gen_err=False, store_pt_loss=False, store_weights=False, freq_reduce_by = freq_reduce_by, freq_reduce_after=freq_reduce_after, )\n",
    "        \n",
    "        with open(grad_file_path,'a+') as f:\n",
    "            f.write(' '.join([str(grad) for grad in train_model.grads_norms]) + '\\n')\n",
    "        with open(hess_file_path,'a+') as f:\n",
    "            f.write(' '.join([str(hess) for hess in train_model.hess_norms]) + '\\n')\n",
    "        \n",
    "        hess_norm_list.append(train_model.hess_norms)\n",
    "        grad_list.append(train_model.grads_norms)\n",
    "         \n",
    "    return grad_list, hess_norm_list\n",
    "\n",
    "grad_list, hess_norm_list = exp_get_lp_sm(train_data_all, test_data_all, op_features=10, \n",
    "              weight=details['g_weight'], times=details['g_times'], \n",
    "              epochs=details['g_epochs'], root_dir=details['result_root_dir'], \n",
    "              path=details['result_path'], freq_reduce_by=details['freq_reduce_by'], \n",
    "              freq_reduce_after=details['freq_reduce_after'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bddd51-74f9-4de9-ae3f-6d2a0a4ab948",
   "metadata": {},
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "705a9acc-397a-4f33-ae7e-e44770794b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "def get_exp_details(root_dir, path):\n",
    "    result_details={}\n",
    "    details_path = root_dir+ 'details_'+ path + '.txt'\n",
    "    with open(details_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        key, val = line[:-1].split(':')\n",
    "        key, val = key.strip(' '), val.strip(' ')\n",
    "        if key in ['ratio', 'alpha_0']:\n",
    "            val = float(val)\n",
    "        if key in ['Times', 'Weights', 'Epochs', 'book_keep_freq', 'g_times', 'g_epochs', 'freq_reduce_by', 'freq_reduce_after']:\n",
    "            val = int(val)\n",
    "        elif key in ['g_weight']:\n",
    "            print('weight:', val)\n",
    "            val = [int(t) for t in val[1:-1].split(', ')]\n",
    "        result_details[key] = val\n",
    "    return result_details\n",
    "\n",
    "def get_exp_results(r_det, train_size=20000):\n",
    "    root_dir = r_det['result_root_dir']\n",
    "    path = r_det['result_path']\n",
    "    grad_file_path = root_dir+'grad_'+path\n",
    "    hess_file_path = root_dir+'hess_'+path\n",
    "    gen_file_path  = root_dir+'gen_'+path\n",
    "    \n",
    "    # print(result_details)\n",
    "    grad_list    = []\n",
    "    hess_list    = []\n",
    "    gen_err_list = []\n",
    "    \n",
    "    with open(grad_file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        tensors = line[:-1].split(') ')\n",
    "        # print('tensors',tensors)\n",
    "        t_list = [float(t[7:].split(' ')[0][:-1]) for t in tensors]\n",
    "        # print('grad_list_i length:',len(t_list))\n",
    "        grad_list.append(t_list)\n",
    "    \n",
    "    with open(hess_file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        tensors = line[:-1].split(' ')\n",
    "        # print('tensors',tensors)\n",
    "        t_list = [float(t[7:-1]) for t in tensors]\n",
    "        # print('hess_list_i length:',len(t_list))\n",
    "        hess_list.append(t_list)\n",
    "    hess_list = np.array(hess_list)\n",
    "    grad_list = np.array(grad_list)\n",
    "    print('hess list shape:',hess_list.shape)\n",
    "    print('grad list shape:',grad_list.shape)\n",
    "    K_g = np.max(np.mean(np.array(grad_list), 0))\n",
    "    L_g = np.max(np.mean(np.array(hess_list), 0))\n",
    "    \n",
    "    book_keep_freq = r_det['book_keep_freq']\n",
    "    freq_reduce_after = r_det['freq_reduce_after']\n",
    "    freq_reduce_by = r_det['freq_reduce_by']\n",
    "    x_values = []\n",
    "    count_keep = 0\n",
    "    for i in range(train_size):\n",
    "        if i%book_keep_freq==0:\n",
    "            x_values.append(i+1)\n",
    "            count_keep+=1\n",
    "            if count_keep%freq_reduce_after==0:\n",
    "                book_keep_freq+=freq_reduce_by\n",
    "    \n",
    "    return grad_list, hess_list, K_g, L_g, x_values\n",
    "\n",
    "def plot_exp_data(root_dir_list, path_list, weights_list, save_fig=False, train_size=20000):\n",
    "    grad_mean_list = []\n",
    "    grad_df_list = []\n",
    "    grad_err_list  = []\n",
    "    hess_mean_list = []\n",
    "    hess_df_list = []\n",
    "    hess_err_list  = []\n",
    "    data = {}\n",
    "    x_values = []\n",
    "    for root_dir, path, weights in zip(root_dir_list, path_list, weights_list):\n",
    "\n",
    "        res_details = get_exp_details(root_dir, path)\n",
    "        grad_list, hess_list, K_g, L_g, x_values = get_exp_results(res_details, train_size)\n",
    "        grad_list_m = np.max(grad_list,1)\n",
    "        hess_list_m = np.max(hess_list,1)\n",
    "        # K_g = np.percentile(np.mean(grad_list, 0),90)\n",
    "        # L_g = np.percentile(np.mean(hess_list, 0),90)\n",
    "        \n",
    "        # K_g = np.max(np.mean(grad_list, 1))\n",
    "        # L_g = np.max(np.mean(hess_list, 1))\n",
    "        # print('grad_list shape',grad_list_m.shape)\n",
    "        # print('grad_list ',grad_list_m)\n",
    "        # print(f'K_g:{K_g}, L_g:{L_g}')\n",
    "        grad_mean, grad_err = np.mean(grad_list_m), np.std(grad_list_m)\n",
    "        hess_mean, hess_err = np.mean(hess_list_m), np.std(hess_list_m)\n",
    "        grad_mean_list.append(grad_mean)\n",
    "        grad_err_list.append(grad_err)\n",
    "        hess_mean_list.append(hess_mean)\n",
    "        hess_err_list.append(hess_err)\n",
    "        params_size = 784\n",
    "        for weight in weights:\n",
    "            params_size*= weight\n",
    "        x_values.append(str(params_size))\n",
    "        for run_id in range(len(grad_list)):\n",
    "            for epoch in range(len(grad_list[run_id])):\n",
    "                grad_df_list.append({'key':int(params_size), 'run_id': run_id, 'epoch':epoch, 'val': grad_list[run_id][epoch]})\n",
    "                                      \n",
    "        for run_id in range(len(hess_list)):\n",
    "            for epoch in range(len(hess_list[run_id])):\n",
    "                hess_df_list.append({'key':int(params_size), 'run_id': run_id, 'epoch':epoch, 'val': hess_list[run_id][epoch]})\n",
    "        \n",
    "        # plt.errorbar(x_values, np.mean(grad_list,0),np.std(grad_list,0), label='ratio:'+str(ratio))\n",
    "    df1 = pd.DataFrame(grad_df_list)\n",
    "    df2 = pd.DataFrame(hess_df_list)\n",
    "    \n",
    "    temp_df1 = df1.groupby(['key','run_id'])['val'].max().reset_index()\n",
    "    temp_df2 = df2.groupby(['key','run_id'])['val'].max().reset_index()\n",
    "    # temp_df1 = df1\n",
    "    # temp_df2 = df2\n",
    "    grad_mean_list = np.array(grad_mean_list)\n",
    "    grad_err_list = np.array(grad_err_list)\n",
    "    hess_mean_list = np.array(hess_mean_list)\n",
    "    hess_err_list = np.array(hess_err_list)\n",
    "    # x_values = [str(int(train_size*t)) for t in ratios]\n",
    "    fig, ax = plt.subplots(figsize=(12,7))\n",
    "    sns.pointplot(data = temp_df1, x='key',y='val', estimator=np.mean, ax=ax, label='Lipschitz const.', color='orange', errwidth=0)\n",
    "    ax.fill_between(x_values, grad_mean_list-grad_err_list, grad_mean_list+ grad_err_list, color='orange', alpha=.3)\n",
    "    plt.xlabel('Number of parameters', fontsize = 24)\n",
    "    plt.ylabel('Lipschitz constant', fontsize = 24)    \n",
    "    sns.set_context(\"talk\")\n",
    "    sns.set(font='sans-serif', style='whitegrid', font_scale=2, rc={\"lines.linewidth\": 2, \"lines.markersize\":10})\n",
    "    plt.title('Mnist dataset', fontsize=24)\n",
    "    if save_fig:\n",
    "        plt.savefig('e3_Lipschitz_constant_mnist.png')\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12,7))\n",
    "    sns.pointplot(data = temp_df2, x='key',y='val', ax=ax, label='Smoothness const.', color='green', errwidth=0)\n",
    "    ax.fill_between(x_values, hess_mean_list-hess_err_list, hess_mean_list+ hess_err_list, color='green', alpha=.3)\n",
    "    plt.xlabel('Number of parameters', fontsize = 24)\n",
    "    plt.ylabel('Smoothness constant', fontsize = 24)    \n",
    "    sns.set_context(\"talk\")\n",
    "    sns.set(font='sans-serif', style='whitegrid', font_scale=2, rc={\"lines.linewidth\": 2, \"lines.markersize\":10})\n",
    "    # plt.show()\n",
    "    plt.title('Mnist dataset', fontsize=24)\n",
    "    if save_fig:\n",
    "        plt.savefig('e3_Smoothness_constant_mnist.png')\n",
    "    return df1, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cec242af-e94b-4d2d-8574-1a878ba3f435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight: [9, 9, 9]\n",
      "hess list shape: (8, 100)\n",
      "grad list shape: (8, 100)\n",
      "weight: [9, 9, 9, 9]\n",
      "hess list shape: (8, 100)\n",
      "grad list shape: (8, 100)\n",
      "weight: [9, 9, 9, 9, 9]\n",
      "hess list shape: (8, 100)\n",
      "grad list shape: (8, 100)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m ratios \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m2.5\u001b[39m, \u001b[38;5;241m5.0\u001b[39m, \u001b[38;5;241m7.5\u001b[39m, \u001b[38;5;241m10.0\u001b[39m]\n\u001b[1;32m      4\u001b[0m weights \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m9\u001b[39m,\u001b[38;5;241m9\u001b[39m,\u001b[38;5;241m9\u001b[39m], [\u001b[38;5;241m9\u001b[39m,\u001b[38;5;241m9\u001b[39m,\u001b[38;5;241m9\u001b[39m,\u001b[38;5;241m9\u001b[39m], [\u001b[38;5;241m9\u001b[39m,\u001b[38;5;241m9\u001b[39m,\u001b[38;5;241m9\u001b[39m,\u001b[38;5;241m9\u001b[39m,\u001b[38;5;241m9\u001b[39m]]\n\u001b[0;32m----> 5\u001b[0m grad_df, hess_f\u001b[38;5;241m=\u001b[39m \u001b[43mplot_exp_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_fig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mplot_exp_data\u001b[0;34m(root_dir_list, path_list, weights_list, save_fig, train_size)\u001b[0m\n\u001b[1;32m    127\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m7\u001b[39m))\n\u001b[1;32m    128\u001b[0m sns\u001b[38;5;241m.\u001b[39mpointplot(data \u001b[38;5;241m=\u001b[39m temp_df1, x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m'\u001b[39m,y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m, estimator\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mmean, ax\u001b[38;5;241m=\u001b[39max, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLipschitz const.\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morange\u001b[39m\u001b[38;5;124m'\u001b[39m, errwidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 129\u001b[0m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill_between\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_mean_list\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mgrad_err_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_mean_list\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgrad_err_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43morange\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m.3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of parameters\u001b[39m\u001b[38;5;124m'\u001b[39m, fontsize \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m24\u001b[39m)\n\u001b[1;32m    131\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLipschitz constant\u001b[39m\u001b[38;5;124m'\u001b[39m, fontsize \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m24\u001b[39m)    \n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/__init__.py:1412\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1409\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1411\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1412\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1414\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1415\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1416\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/axes/_axes.py:5252\u001b[0m, in \u001b[0;36mAxes.fill_between\u001b[0;34m(self, x, y1, y2, where, interpolate, step, **kwargs)\u001b[0m\n\u001b[1;32m   5250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfill_between\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y1, y2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, interpolate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   5251\u001b[0m                  step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 5252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fill_between_x_or_y\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/axes/_axes.py:5157\u001b[0m, in \u001b[0;36mAxes._fill_between_x_or_y\u001b[0;34m(self, ind_dir, ind, dep1, dep2, where, interpolate, step, **kwargs)\u001b[0m\n\u001b[1;32m   5153\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m   5154\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_patches_for_fill\u001b[38;5;241m.\u001b[39mget_next_color()\n\u001b[1;32m   5156\u001b[0m \u001b[38;5;66;03m# Handle united data, such as dates\u001b[39;00m\n\u001b[0;32m-> 5157\u001b[0m ind, dep1, dep2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m   5158\u001b[0m     ma\u001b[38;5;241m.\u001b[39mmasked_invalid, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_unit_info(\n\u001b[1;32m   5159\u001b[0m         [(ind_dir, ind), (dep_dir, dep1), (dep_dir, dep2)], kwargs))\n\u001b[1;32m   5161\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, array \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m   5162\u001b[0m         (ind_dir, ind), (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdep_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m, dep1), (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdep_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m\"\u001b[39m, dep2)]:\n\u001b[1;32m   5163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/numpy/ma/core.py:2369\u001b[0m, in \u001b[0;36mmasked_invalid\u001b[0;34m(a, copy)\u001b[0m\n\u001b[1;32m   2367\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(a)\n\u001b[1;32m   2368\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2369\u001b[0m     condition \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39m(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misfinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2370\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m MaskedArray\n\u001b[1;32m   2371\u001b[0m result \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28mcls\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "root_dir_list = ['results/t1/','results/t1/','results/t1/','results/t1/','results/t1/']\n",
    "path_list = ['try1_t8_w999', 'try1_t8_w9999', 'try1_t8_w99999']\n",
    "ratios = [1.0, 2.5, 5.0, 7.5, 10.0]\n",
    "weights = [[9,9,9], [9,9,9,9], [9,9,9,9,9]]\n",
    "grad_df, hess_f= plot_exp_data(root_dir_list[:3], path_list[:], weights[:], save_fig=False, train_size=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90098fe8-9990-43fe-8554-fa4097d944e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9654996227799617"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.max(np.array(grad_list),1))\n",
    "# for grad_list_1 in grad_list:\n",
    "#     print(grad_list_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45999413-3f19-410e-8be4-1b8e034389b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(1.3310, dtype=torch.float64), tensor(1.6456, dtype=torch.float64)]\n",
      "[tensor(1.9846, dtype=torch.float64), tensor(1.7699, dtype=torch.float64)]\n",
      "[tensor(1.8806, dtype=torch.float64), tensor(2.1590, dtype=torch.float64)]\n",
      "[tensor(1.9023, dtype=torch.float64), tensor(2.2948, dtype=torch.float64)]\n",
      "[tensor(1.9634, dtype=torch.float64), tensor(1.7891, dtype=torch.float64)]\n",
      "[tensor(1.8968, dtype=torch.float64), tensor(2.2273, dtype=torch.float64)]\n",
      "[tensor(1.9494, dtype=torch.float64), tensor(2.4226, dtype=torch.float64)]\n",
      "[tensor(1.7676, dtype=torch.float64), tensor(1.6867, dtype=torch.float64)]\n"
     ]
    }
   ],
   "source": [
    "for grad_list_1 in grad_list:\n",
    "    print(grad_list_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ec5681e-598c-4a67-a5c3-a66a25432c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.041699999999999"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.645+1.9846+2.1590+2.2948+1.9634+2.2273+1.7676"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e028cea2-a135-4696-9150-80a6006edd3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor(26.4066), tensor(22.3224)],\n",
       " [tensor(33.6504), tensor(15.7732)],\n",
       " [tensor(31.5590), tensor(25.5066)],\n",
       " [tensor(29.2283), tensor(22.7296)],\n",
       " [tensor(36.8180), tensor(24.5703)],\n",
       " [tensor(28.2731), tensor(21.5612)],\n",
       " [tensor(34.9414), tensor(28.8288)],\n",
       " [tensor(27.7194), tensor(20.4769)]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hess_norm_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f40dc25-33ba-4321-b271-425915c2b576",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
